{"categories":[],"posts":[{"content":"Golang \u0026amp; OpenAPI Spec Fiddling with an idea of generating models from the OpenAPI spec for YAML.\nNotes  Bazel model works, but would want to formalize as a proper build system Model of pkg/apis/... is a good direction, version selecting will need to be a thing though What about YAML Support? Is swagger the expected way forward for this kind of thing? Should the generation of files be done as a bazel run or a bazel build? Generation should really commit against the repository, that way we can be more confident about the code results Works much better to have all components available  ","id":0,"section":"posts","summary":"Golang \u0026amp; OpenAPI Spec Fiddling with an idea of generating models from the OpenAPI spec for YAML.\nNotes  Bazel model works, but would want to formalize as a proper build system Model of pkg/apis/... is a good direction, version selecting will need to be a thing though What about YAML Support? Is swagger the expected way forward for this kind of thing? Should the generation of files be done as a bazel run or a bazel build?","tags":["org:jrbeverly"],"title":"swagger-golang-bazelgen-exp","uri":"/2022/05/jrbeverly-swagger-golang-bazelgen-exp/","year":"2022"},{"content":"Fiddling with gqlgen Experimenting a bit with gqlgen for generating GraphQL code from spec.\nNotes  Would prefer to move gqlgen.yml and schema.graphql into a spec/ directory (or other areas) Installation method with tools.go - not sure about this, static binary is preferrable for my usages Generated models are pretty solid, similar to swagger Resolvers is nice, but what about partial updates?  I\u0026rsquo;m not entirely sold on this pattern. Feel like I\u0026rsquo;d prefer more flexibility with how the models and various components are defined. Almost interested in the idea of instead of using annotations like json or yaml, leveraging generated code for rendering the JSON/YAML components. Then isolating the various elements of what the system is expected to do, things like:\n Data Serialization Patch \u0026amp; Diff Pattern for Models (e.g. receive patch, apply patch to model) Hexagonal architecture  Maybe this is a case where Roslyn is a better fit for specialized code generation with a hexagonal architecture in-mind? More research needed into this.\n","id":1,"section":"posts","summary":"Fiddling with gqlgen Experimenting a bit with gqlgen for generating GraphQL code from spec.\nNotes  Would prefer to move gqlgen.yml and schema.graphql into a spec/ directory (or other areas) Installation method with tools.go - not sure about this, static binary is preferrable for my usages Generated models are pretty solid, similar to swagger Resolvers is nice, but what about partial updates?  I\u0026rsquo;m not entirely sold on this pattern. Feel like I\u0026rsquo;d prefer more flexibility with how the models and various components are defined.","tags":["org:jrbeverly"],"title":"graphql-golang-note-check","uri":"/2022/05/jrbeverly-graphql-golang-note-check/","year":"2022"},{"content":"State Machine for Confirmation Dialog Running through the workshop example of Build A Confirmation Modal in React with State Machines\nNotes  In principal, like the idea of representation this kinds of logic \u0026ldquo;Flows\u0026rdquo; Usage of strings for state is less than ideal, almost would want it to be objects Pattern of constructing a \u0026ldquo;flow\u0026rdquo; then making use of the \u0026ldquo;flow\u0026rdquo; Potential opportunities with systems like codegen Possible ideas for test evaluation with the states Not sure about this library, needs opportunities for isolation Feels like it doesn\u0026rsquo;t fit with the View -\u0026gt; ViewModel -\u0026gt; Model concept I was thinking of for State Machines The \u0026lsquo;dispatcher\u0026rsquo; works similar to what a wrapper over \u0026ldquo;Modal\u0026rdquo; would have, what benefits does it bring? Consider looking into eventing systems instead, they seem to better encapsulate this idea without strictness of the \u0026ldquo;machine structure\u0026rdquo; Maybe this could work better when combined with message passing + code generation?  ","id":2,"section":"posts","summary":"State Machine for Confirmation Dialog Running through the workshop example of Build A Confirmation Modal in React with State Machines\nNotes  In principal, like the idea of representation this kinds of logic \u0026ldquo;Flows\u0026rdquo; Usage of strings for state is less than ideal, almost would want it to be objects Pattern of constructing a \u0026ldquo;flow\u0026rdquo; then making use of the \u0026ldquo;flow\u0026rdquo; Potential opportunities with systems like codegen Possible ideas for test evaluation with the states Not sure about this library, needs opportunities for isolation Feels like it doesn\u0026rsquo;t fit with the View -\u0026gt; ViewModel -\u0026gt; Model concept I was thinking of for State Machines The \u0026lsquo;dispatcher\u0026rsquo; works similar to what a wrapper over \u0026ldquo;Modal\u0026rdquo; would have, what benefits does it bring?","tags":["org:jrbeverly"],"title":"react-xstate-machines","uri":"/2022/05/jrbeverly-react-xstate-machines/","year":"2022"},{"content":"Experimenting with Pulumi Experimenting with the pulumi examples from https://github.com/pulumi/examples, and the options to have\nNotes  Enabling Bazel wtih this model encountered some pain points Pulumi is capable of performing operations like Docker Build / Referencing lambda binaries The change source pattern (git.dirty, git.author, etc) for volatile status is a nice pattern Building of artifacts should not be the responsibility of the deployment model Pulumi takes over the context element of deployments, and use that for deployments  Pulumi has some nice elements too it with respect to be able to use code to create the deployment, as well as a good web/console interface. However I\u0026rsquo;m not sold on the idea of these kind of \u0026ldquo;dynamic\u0026rdquo; systems, and instead prefer the style of an application model. This way a series of concerns can be baked into \u0026ldquo;Configuration\u0026rdquo; that is then acted upon, similar to Terraform.\nIdea on Notes Basics of this components are:\n Volatile Status - Properties about the change that aren\u0026rsquo;t really related to the software artifacts - Git SVC, Git Commit, Manifest Version, GitHub PR URL, etc Artifact Manifest - Software artifacts created as part of the build process Application Manifest(s) - What is the calver/semver of the application system Deployment Artifacts - Software artifacts created as part of the build process for deploying the service Application Model - Model of the application, and the various ways it can be assembled or connected to components (Graph relations with optional components and multiple vertex sets)  Ideally something like this:\nvolatile_status.json:\n{ \u0026#34;exec\u0026#34;: \u0026#34;cli\u0026#34;, \u0026#34;git\u0026#34;: { \u0026#34;author\u0026#34;: \u0026#34;Jonathan Beverly\u0026#34;, \u0026#34;author.email\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;committer\u0026#34;: \u0026#34;Jonathan Beverly\u0026#34;, \u0026#34;committer.email\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;dirty\u0026#34;: true, \u0026#34;head\u0026#34;: \u0026#34;ec3d5a5a63251dd884634e25b1f98c9be6b4b912\u0026#34;, \u0026#34;headName\u0026#34;: \u0026#34;refs/heads/main\u0026#34;, }, \u0026#34;vcs\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;github.com\u0026#34;, \u0026#34;owner\u0026#34;: \u0026#34;jrbeverly\u0026#34;, \u0026#34;repo\u0026#34;: \u0026#34;exp-pulumi-lambda\u0026#34; } } artifact.manifest.yaml:\napiVersion: openapplicationmodel/v1alphav1 kind: Manifest metadata: name: pulumi-lambda labels: app: pulumi service: xyz property: xyss spec: artifacts: - id: 946debe2-e213-11ec-8fea-0242ac120002 - id: e213ebe2-946d-8fea-11ec-200020242ac1 lambda.appmanifest.yaml:\napiVersion: openapplicationmodel/v1alphav1 kind: AppManifest metadata: name: pulumi-lambda labels: app: pulumi service: xyz property: xyss spec: id: 946debe2-e213-11ec-8fea-0242ac120002 filename: lambda.tar sha256: ba7816bf8f01cfea414140de5dae2223b00361a396177a9cb410ff61f20015ad annotations: app: lambda Then these artifact components will need some sort of spec for defining how to combine these various components. Additionally this spec should be oriented as something that tools can generate, which would allow for dynamic deployment processes by simply generating the spec, then publishing it to a service for performing the deployment. Similar to how pulumi is doing the deployment, but instead requiring it to resolve all properties \u0026amp; actions it would perform (e.g. terraform plan), then require it to execute the plan.\n","id":3,"section":"posts","summary":"Experimenting with Pulumi Experimenting with the pulumi examples from https://github.com/pulumi/examples, and the options to have\nNotes  Enabling Bazel wtih this model encountered some pain points Pulumi is capable of performing operations like Docker Build / Referencing lambda binaries The change source pattern (git.dirty, git.author, etc) for volatile status is a nice pattern Building of artifacts should not be the responsibility of the deployment model Pulumi takes over the context element of deployments, and use that for deployments  Pulumi has some nice elements too it with respect to be able to use code to create the deployment, as well as a good web/console interface.","tags":["org:jrbeverly"],"title":"exp-pulumi-lambda","uri":"/2022/05/jrbeverly-exp-pulumi-lambda/","year":"2022"},{"content":"Golang Gin \u0026amp; Gitpod Fiddling with the Dev experience of Golang gin within Gitpod\nBased on gitpod-io/go-gin-app\nNotes  Prefer the pattern of combining this with cobra Server rendered HTML is pretty straightforward Returning simple JSON pretty straightforward Can be combined with GRPC + Proto What about swagger for generating the modules? Considerations about how to organize the elements (cli, server, routes, models, database, etc) What about logging for this? Gitpod has occassionally issues with the import of gin References for things like templates/ at the top level is nice Log while running is excellent  Overall positive.\n","id":4,"section":"posts","summary":"Golang Gin \u0026amp; Gitpod Fiddling with the Dev experience of Golang gin within Gitpod\nBased on gitpod-io/go-gin-app\nNotes  Prefer the pattern of combining this with cobra Server rendered HTML is pretty straightforward Returning simple JSON pretty straightforward Can be combined with GRPC + Proto What about swagger for generating the modules? Considerations about how to organize the elements (cli, server, routes, models, database, etc) What about logging for this? Gitpod has occassionally issues with the import of gin References for things like templates/ at the top level is nice Log while running is excellent  Overall positive.","tags":["org:jrbeverly"],"title":"golang-gin-gitpod","uri":"/2022/05/jrbeverly-golang-gin-gitpod/","year":"2022"},{"content":"MirageJS Tutorial Running through the (MirageJS Tutorial)[https://github.com/miragejs/tutorial] case\nNotes  If generated from a schema, the system is essentially  Backend (proper) Backend Mock (in-memory) Backend JS Client (proper) Backend JS Client Mock (in-memory - integrated with MirageJS)   How could this be integrated with code generation from a specification? Does this assist with local development? Test scenarios in an pseudo-E2E case?  ","id":5,"section":"posts","summary":"MirageJS Tutorial Running through the (MirageJS Tutorial)[https://github.com/miragejs/tutorial] case\nNotes  If generated from a schema, the system is essentially  Backend (proper) Backend Mock (in-memory) Backend JS Client (proper) Backend JS Client Mock (in-memory - integrated with MirageJS)   How could this be integrated with code generation from a specification? Does this assist with local development? Test scenarios in an pseudo-E2E case?  ","tags":["org:jrbeverly"],"title":"reminders-miragejs","uri":"/2022/05/jrbeverly-reminders-miragejs/","year":"2022"},{"content":"MAnim Experimentation with Video Generation Experimenting with the MAnim library for generating video animations.\nNotes  Solid library that works really well at what it does Scenes feel very \u0026ldquo;on-track\u0026rdquo; in that it is a sequence of steps one followed by the next Python allows for re-use to create common elements like an isometric file Works great for scenes where the emphasis is on the core \u0026ldquo;shapes\u0026rdquo; and animation  Don\u0026rsquo;t think this fits with the use-case I\u0026rsquo;m after. Would make more sense to explore as a unity \u0026ldquo;game\u0026rdquo; that is really just an interactive cutscene. As that would allow for elements of the scene to still remain dynamic (animated) while the core \u0026ldquo;state\u0026rdquo; of the animation remains at a fixed point in time.\n","id":6,"section":"posts","summary":"MAnim Experimentation with Video Generation Experimenting with the MAnim library for generating video animations.\nNotes  Solid library that works really well at what it does Scenes feel very \u0026ldquo;on-track\u0026rdquo; in that it is a sequence of steps one followed by the next Python allows for re-use to create common elements like an isometric file Works great for scenes where the emphasis is on the core \u0026ldquo;shapes\u0026rdquo; and animation  Don\u0026rsquo;t think this fits with the use-case I\u0026rsquo;m after.","tags":["org:jrbeverly"],"title":"manim-exp-video-generation","uri":"/2022/05/jrbeverly-manim-exp-video-generation/","year":"2022"},{"content":"React Webpack with Rust WebAssembly Fiddling around with an opinionated example for Webpack builds with WebAssembly.\nSource code for fractal is based on https://dev.to/brightdevs/using-webassembly-with-react-1led, and the repository templated by https://github.com/Fallenstedt/wasm-react-webpack-template.\nNotes  WASM Build works pretty well with rust Rust is a solid option for getting webassembly integrated Golang was considered, but previous experiments weren\u0026rsquo;t as desired Makefile as an entrypoint is preferrable than using yarn ... Embedding the generated packages within the front (e.g. under pkg/) allows for local refs When does the build perform certain targets? E.g. resolution of paths vs copying of static files? Searching for options of individual target execution didn\u0026rsquo;t yield much (using wrong search terms?) Webpacks plugin model isn\u0026rsquo;t really the desired execution model, prefer graph-driven like buck/bazel Webpack plugin dumping the files just by directory path isn\u0026rsquo;t ideal, would prefer to ref (e.g. //pkg/fractal) Loading WebAssembly requires a bit of pre-amble, could probably be made into a \u0026lt;project\u0026gt;-commons for UIs responsible for loading it Usages of them are pretty nice, opportunities for generated API/Schemas/State Machines/State Models  I\u0026rsquo;m not sure on this one. I get the benefits that come with the webpack ecosystem, but had multiple issues with the way its goes about it in comparison to patterns like Bazel/Buck. I like this pattern more for combining various languages to create the final result, such as having a rust library for certain functions or components. Splitting it away from the frontend makes it so they can be designed in more of a \u0026ldquo;unit-test\u0026rdquo; way. Possible options might be to encode a state machine within one of these libraries, then have the web interface act on this, like a View =\u0026gt; ViewModel =\u0026gt; {Model} where the ViewModel performs queries into the state machine {Model}.\n","id":7,"section":"posts","summary":"React Webpack with Rust WebAssembly Fiddling around with an opinionated example for Webpack builds with WebAssembly.\nSource code for fractal is based on https://dev.to/brightdevs/using-webassembly-with-react-1led, and the repository templated by https://github.com/Fallenstedt/wasm-react-webpack-template.\nNotes  WASM Build works pretty well with rust Rust is a solid option for getting webassembly integrated Golang was considered, but previous experiments weren\u0026rsquo;t as desired Makefile as an entrypoint is preferrable than using yarn ... Embedding the generated packages within the front (e.","tags":["org:jrbeverly"],"title":"react-wasm-babel","uri":"/2022/05/jrbeverly-react-wasm-babel/","year":"2022"},{"content":"Vulnerability Disclosure Policy from Dioterms Exploring leveraging dioterms and policymaker for creating vulnerability disclore policies for a website.\nNotes  DNS is related for the deployment of the website (_security) Entry within the /.well-known/ root of the domain (example.com/.well-known/security.txt) Security entry for the domain (example.com/security) If the application is located within example.com/app/... (e.g. index.html), then the top level domain elements can be \u0026ldquo;procedural\u0026rdquo; Construct the webpage into a bundle (website.wbn), publish it to the \u0026ldquo;deployer\u0026rdquo;, which can then handle the top level elements References can still exist within the app (/security, /.well-known/...), known to the website manifest Website manifest allow it to enforce expectations about required top-level components Distributable/Sharable webpages can combine/merge these components (e.g. website.wbn, website.manifest, website.policy) with organization (or overwrite)  Acknowledgements:\n https://jacobian.org/2021/jul/8/appsec-pagnis/ https://github.com/disclose/dioterms https://policymaker.disclose.io/policymaker  ","id":8,"section":"posts","summary":"Vulnerability Disclosure Policy from Dioterms Exploring leveraging dioterms and policymaker for creating vulnerability disclore policies for a website.\nNotes  DNS is related for the deployment of the website (_security) Entry within the /.well-known/ root of the domain (example.com/.well-known/security.txt) Security entry for the domain (example.com/security) If the application is located within example.com/app/... (e.g. index.html), then the top level domain elements can be \u0026ldquo;procedural\u0026rdquo; Construct the webpage into a bundle (website.wbn), publish it to the \u0026ldquo;deployer\u0026rdquo;, which can then handle the top level elements References can still exist within the app (/security, /.","tags":["org:jrbeverly"],"title":"vuln-disclosure-policy","uri":"/2022/05/jrbeverly-vuln-disclosure-policy/","year":"2022"},{"content":"Experimenting with ConfTest, Terraform \u0026amp; Bazel Experimenting with using Bazel to handle the build \u0026amp; execution of Terraform files, while providing means of writing tests against the terraform with conftest. Fiddling with the idea of having local tests against the configuration, as well as tests against the terraform plan.\nThe intention is that Bazel would be responsible for constructing Terraform deployable tarballs, which contains all resolved modules \u0026amp; providers. These would be executed to perform apply, plan and other commands.\nNotes  Terraform init should be treated as a repository rule, based off the .terraform.lock.hcl Modules can exist in any directory, and be substituted into other packages as its using the pkg_tar under the hood Using a provider to keep track of \u0026ldquo;runpaths\u0026rdquo; for any of the commands, making it so these properties can be shared across rules Substitutions in command arguments through {variable} and kwargs works great Standard pattern for \u0026ldquo;chaining\u0026rdquo; commands is needed, so that running one will run the others Should it always be the case that Terraform does a plan then apply? How should chained commands perform some degree of caching so they aren\u0026rsquo;t re-running every single time? Terraform modules need to exist in multiple forms: local modules \u0026amp; externally sourced modules (e.g. how to integrate with http_archive - or similar) Using the pattern of custom rules under bazel/rules/rules_xyz is really nice, the deps.bzl can then just load them Rego/OPA are missing vscode extensions for use in gitpod (does vscode have them in devcontainers?) Adding the opa toolchain along with the testing process went really well Long term rules_toolchains should be external, and rely on a pre-installed tool like toolchains or termtools, that can download these How do these rules integrate with different kinds of rules? Is the API solid enough that something like rules_terraform_ext could be made to provide common helpers? Should the terraform_workspace really exist? Or should it just be terraform_command with something like rules_terraform_ext filling in the ease-of-use? Makefile with bazel can be great for the make help addition  ","id":9,"section":"posts","summary":"Experimenting with ConfTest, Terraform \u0026amp; Bazel Experimenting with using Bazel to handle the build \u0026amp; execution of Terraform files, while providing means of writing tests against the terraform with conftest. Fiddling with the idea of having local tests against the configuration, as well as tests against the terraform plan.\nThe intention is that Bazel would be responsible for constructing Terraform deployable tarballs, which contains all resolved modules \u0026amp; providers. These would be executed to perform apply, plan and other commands.","tags":["org:jrbeverly"],"title":"bazel-terraform-conftest-experiments","uri":"/2022/05/jrbeverly-bazel-terraform-conftest-experiments/","year":"2022"},{"content":"asdf   Manage multiple runtime versions with a single CLI tool, extendable via plugins - docs at asdf-vm.com\nasdf is a CLI tool that can manage multiple language runtime versions on a per-project basis. It is like gvm, nvm, rbenv \u0026amp; pyenv (and more) all in one! Simply install your language\u0026rsquo;s plugin!\nWhy use asdf?  single CLI for multiple languages consistent commands to manage all your languages single global config keeping defaults in one place single .tool-versions config file per project support for existing config files .node-version, .nvmrc, .ruby-version for easy migration automatically switches runtime versions as you traverse your directories simple plugin system to add support for your language of choice shell completion available for common shells (Bash, Zsh, Fish, Elvish)  Documentation Please head over to the documentation site for more information!\n Getting Started All Commands All Plugins Create a Plugin with our asdf-plugin-template asdf GitHub Actions  Contributing See CONTRIBUTING.md in the repo or the Contributing section on the docs site.\nCommunity \u0026amp; Questions  FAQ GitHub Issues: report a bug or raise a feature request to the asdf core team StackOverflow Tag: see existing Q\u0026amp;A for asdf. Some of the core team watch this tag in addition to our helpful community  Ballad of asdf  Once upon a time there was a programming languageThere were many versions of itSo people wrote a version manager for itTo switch between versions for projectsDifferent, old, new.\n  Then there came more programming languagesSo there came more version managersAnd many commands for them\n  I installed a lot of themI learnt a lot of commands\n  Then I said, just one more version managerWhich I will write instead\n  So, there came another version managerasdf version manager - https://github.com/asdf-vm/asdf\n  A version manager so extendablefor which anyone can create a pluginTo support their favourite languageNo more installing more version managersOr learning more commands\n  ","id":10,"section":"posts","summary":"asdf   Manage multiple runtime versions with a single CLI tool, extendable via plugins - docs at asdf-vm.com\nasdf is a CLI tool that can manage multiple language runtime versions on a per-project basis. It is like gvm, nvm, rbenv \u0026amp; pyenv (and more) all in one! Simply install your language\u0026rsquo;s plugin!\nWhy use asdf?  single CLI for multiple languages consistent commands to manage all your languages single global config keeping defaults in one place single .","tags":["org:jrbeverly"],"title":"asdf","uri":"/2022/05/jrbeverly-asdf/","year":"2022"},{"content":"Julia Jupyter Notebook Experiments with working with Julia \u0026amp; Jupyter Notebooks.\nNotes  Installation of Jupyter, Conda + Julia (+ packages) onto a gitpod image Makefile for common actions working with the notebooks Validating some basic cases working with Julia for generating plots \u0026amp; other bits for the visualization  ","id":11,"section":"posts","summary":"Julia Jupyter Notebook Experiments with working with Julia \u0026amp; Jupyter Notebooks.\nNotes  Installation of Jupyter, Conda + Julia (+ packages) onto a gitpod image Makefile for common actions working with the notebooks Validating some basic cases working with Julia for generating plots \u0026amp; other bits for the visualization  ","tags":["org:jrbeverly"],"title":"julia-with-jupyter-notebook","uri":"/2022/04/jrbeverly-julia-with-jupyter-notebook/","year":"2022"},{"content":"GitPod Jupyter Notebook Validating working with Jupyter notebooks in a GitPod environments\nNotes  Support for preview or open in browser mode Initial provisioning of the image takes a bit, pre-baked likely would help in this area Jupyter lab is another option Makefile is a good option for acting as an entrypoint for common commands Outstanding questions still exist for multi-notebook repositories like the learning repo  ","id":12,"section":"posts","summary":"GitPod Jupyter Notebook Validating working with Jupyter notebooks in a GitPod environments\nNotes  Support for preview or open in browser mode Initial provisioning of the image takes a bit, pre-baked likely would help in this area Jupyter lab is another option Makefile is a good option for acting as an entrypoint for common commands Outstanding questions still exist for multi-notebook repositories like the learning repo  ","tags":["org:jrbeverly"],"title":"gitpod-jupyter-notebook","uri":"/2022/03/jrbeverly-gitpod-jupyter-notebook/","year":"2022"},{"content":"Pushgateway Compose Setup Simple code setup for spinning up Pushgateway, Prometheus \u0026amp; Grafana for validating lifecycle pushgateway metrrics.\nNotes  Metrics published to pushgateway are collected by Prometheus Prometheus is enabled in Grafana for queries Grafana datasources \u0026amp; dashboards are configured from the provisioning directory The script publish.sh can be used to publish the metrics into the system Dashboards in grafana/dashboards can be configured with other tools for construction Dashboards could be standardized, then shared into other sources Tools can be configured for publishing in these scenarios  ","id":13,"section":"posts","summary":"Pushgateway Compose Setup Simple code setup for spinning up Pushgateway, Prometheus \u0026amp; Grafana for validating lifecycle pushgateway metrrics.\nNotes  Metrics published to pushgateway are collected by Prometheus Prometheus is enabled in Grafana for queries Grafana datasources \u0026amp; dashboards are configured from the provisioning directory The script publish.sh can be used to publish the metrics into the system Dashboards in grafana/dashboards can be configured with other tools for construction Dashboards could be standardized, then shared into other sources Tools can be configured for publishing in these scenarios  ","tags":["org:jrbeverly"],"title":"pushgateway-compose-setup","uri":"/2022/03/jrbeverly-pushgateway-compose-setup/","year":"2022"},{"content":"AWS Organization Structure Experiments - Mirrored Organizations Experiments with AWS Organization structure and potential SCP policies.\nNotes  The entire organization unit hierarchy shouldn\u0026rsquo;t be a single entity for mirroring. Makes it difficult to evaluate in \u0026ldquo;isolation\u0026rdquo; Entire organization mirrors can work with the SCPs, but internal permissions (e.g. S3 Bucket) still might have issues Organizations should include a uniqueness component to allow for constructing a new version (\u0026amp; prototyping) SCPs seem like they would benefit in cases where there is a sort of \u0026ldquo;State machine\u0026rdquo; in the SCP State machine examples are \u0026ldquo;During provisioning of account, need to create IAM Users, but from then on no users should be created\u0026rdquo; Account boundaries for services as a way of strictly locking things makes it easier to have DenyKMS and other such policies Region denies only apply after provisioning, as we need to purge the \u0026ldquo;default VPCs\u0026rdquo; created when an AWS Account is created (+ any other \u0026ldquo;default\u0026rdquo; resources) AWS Password Policy / AWS IAM Account Name / Etc are all good examples of something that should only need to be provisioned \u0026ldquo;once\u0026rdquo; SCPs give a potential idea for the concept of \u0026ldquo;Immutable AWS Account Infrastructure\u0026rdquo;, that require you to create a new AWS Account (+ migrate resources) rather than edit them Sandbox/Staging organizations can contain the developer workloads that are for sandbox/development Developer workloads should be contained within accoounts that can be created/decommissioned on a release schedule (see ubuntu - Bionic Beaver, Focal Fossa, Xenial Xerus)  More investigation is needed into this idea, as the exact \u0026ldquo;concern\u0026rdquo; that this kind of structure \u0026amp; SCP policy layout will handle is kind of vague. Although grouping AWS Accounts and associating tags with them can be useful for things like data residency / storage compliance, its not immediately clear how this design maps to the \u0026ldquo;problem\u0026rdquo; itself.\nSCPs seem like they would be a good guardrail, but have concerns that it would encounter issues in cases with the rule being enforced at all times, vs a more state machine concept (e.g. [Provisioning (allowed) =\u0026gt; Running (not allowed)])\n","id":14,"section":"posts","summary":"AWS Organization Structure Experiments - Mirrored Organizations Experiments with AWS Organization structure and potential SCP policies.\nNotes  The entire organization unit hierarchy shouldn\u0026rsquo;t be a single entity for mirroring. Makes it difficult to evaluate in \u0026ldquo;isolation\u0026rdquo; Entire organization mirrors can work with the SCPs, but internal permissions (e.g. S3 Bucket) still might have issues Organizations should include a uniqueness component to allow for constructing a new version (\u0026amp; prototyping) SCPs seem like they would benefit in cases where there is a sort of \u0026ldquo;State machine\u0026rdquo; in the SCP State machine examples are \u0026ldquo;During provisioning of account, need to create IAM Users, but from then on no users should be created\u0026rdquo; Account boundaries for services as a way of strictly locking things makes it easier to have DenyKMS and other such policies Region denies only apply after provisioning, as we need to purge the \u0026ldquo;default VPCs\u0026rdquo; created when an AWS Account is created (+ any other \u0026ldquo;default\u0026rdquo; resources) AWS Password Policy / AWS IAM Account Name / Etc are all good examples of something that should only need to be provisioned \u0026ldquo;once\u0026rdquo; SCPs give a potential idea for the concept of \u0026ldquo;Immutable AWS Account Infrastructure\u0026rdquo;, that require you to create a new AWS Account (+ migrate resources) rather than edit them Sandbox/Staging organizations can contain the developer workloads that are for sandbox/development Developer workloads should be contained within accoounts that can be created/decommissioned on a release schedule (see ubuntu - Bionic Beaver, Focal Fossa, Xenial Xerus)  More investigation is needed into this idea, as the exact \u0026ldquo;concern\u0026rdquo; that this kind of structure \u0026amp; SCP policy layout will handle is kind of vague.","tags":["org:jrbeverly"],"title":"aws-exp-organizations-policy","uri":"/2022/03/jrbeverly-aws-exp-organizations-policy/","year":"2022"},{"content":"Release Mirror for GitHub Releases Lightweight experiment for mirroring GitHub releases into a file store system like Minio or AWS S3.\nGetting Started packages/ Contains the tool definition files for each of the mirror repositories (tool.ini), and the computed checksums for the downloaded files. In an actual use case it would be better to store the checksums externally from this repository, allowing this one to act purely as an executor, and the other repository acting as a \u0026ldquo;record\u0026rdquo; of the known-good commit SHAs (as well as .sign/.ack files to chain \u0026ldquo;trust\u0026rdquo;)\ncmd/ Contains the scripts that are responsible for performing the lookup \u0026amp; download of toolchains from each of the potential sources. Only source for now is GitHub.com.\nNotes  Could other tools like asdf be leveraged instead to download the core binaries, then checksum/bundle that way? What about concerns running any old git repos? Do we wish to mirror the GitHub releases, or the binaries themselves? Or is it multiple things we wish to run? Are there alternative options for having this kind of mirror? Could this benefit from some \u0026ldquo;import into content addressable storage\u0026rdquo; model Commiting against the same repo isn\u0026rsquo;t great, as it can muddle the commit history of changes to the executor Likely want the ability to work off not just the \u0026ldquo;latest\u0026rdquo;, but any release created for these Revocation of builds with known CVEs should be its own separate thing (not integrated into this) gh (GitHub CLI) is pretty effective for downloading these kinds of binaries  ","id":15,"section":"posts","summary":"Release Mirror for GitHub Releases Lightweight experiment for mirroring GitHub releases into a file store system like Minio or AWS S3.\nGetting Started packages/ Contains the tool definition files for each of the mirror repositories (tool.ini), and the computed checksums for the downloaded files. In an actual use case it would be better to store the checksums externally from this repository, allowing this one to act purely as an executor, and the other repository acting as a \u0026ldquo;record\u0026rdquo; of the known-good commit SHAs (as well as .","tags":["org:jrbeverly"],"title":"github-pullthrough-mirror","uri":"/2022/03/jrbeverly-github-pullthrough-mirror/","year":"2022"},{"content":"GitHub Configuration in Code Fiddling with the configuration options available for GitHub, while encoding the properties in the .github directory.\nGetting Started This repository\nThe concept is to create an almost \u0026ldquo;self-contained\u0026rdquo; repository, that includes within hidden directories like .github/.aws/.azure/.gcp/etc that represent almost \u0026ldquo;interfaces\u0026rdquo; between the repository and external services that would act on it. This way rather than having the repository rely on assumptions about how it is configured, it is instead providing all the baseline elements for any supporting infrastructure (e.g. operators) to provide these integrations themselves.\nExamples of these include:\n GitHub Labels GitHub Secrets from External Stores (e.g. source from aws) Dependabot CodeQL Repository Settings License Data  To get the full list of all \u0026ldquo;initial\u0026rdquo; baseline constraints for this, I explored the use of https://github.com/ossf/scorecard to identify potential concerns that would exist in the repository.\nNotes A collection of \u0026ldquo;best practices\u0026rdquo; were collected at: https://bestpractices.coreinfrastructure.org/en/criteria. These can be summarized roughly as the following, with the full list contained under docs/CRITERIA.md:\n Use fuzzing (fuzz testing) for programs (see https://github.com/google/oss-fuzz) Published process for reporting vulnerabilities in the repository Provide a working build system that can automatically rebuild the software from source code General policy that tests will be added to an automated test suite (new functionality) Apply at least one static code analysis tool (beyond compiler warnings and safe language modes) Provide should contain licensing configuration Provide documentation in the form of API Reference, Examples \u0026amp; \u0026ldquo;basics\u0026rdquo; Project should support some means of organizing for discussion To enable collaborative review, the project\u0026rsquo;s source repository MUST include interim versions for review between releases; it MUST NOT include only final releases. Impose a versioning system on the release of artifacts (do not pin latest) Provide release notes with changes and so on  Some of these practices are setting a baseline, which I think can be useful, but don\u0026rsquo;t exactly map perfectly to how I\u0026rsquo;d think about this. However the overall idea of a \u0026ldquo;Standard\u0026rdquo; with clear set of criteria (programmatic if possible) sounds like a positive option for these kinds of repositories.\nOverall think this is a worthwhile idea to continue with, having repositories be \u0026ldquo;self-contained\u0026rdquo;. I think some of the components could be split out of .github into other standards, then leveraging automation to populate the approach components in the .github/ directory. Examples could include things like:\n Having the \u0026ldquo;artifacts\u0026rdquo; published by a repository defined in repository Having the \u0026ldquo;secrets\u0026rdquo; needed for the build process encoded in repository Using Content/Unique Addressable Storage for dependencies/tools to avoid specifying where artifacts must be sourced (so they can pull from authoritative or mirror) CODEOWNERS leveraging an RBAC model for the repository, rather than inheriting the implementation of the service (e.g. how GitHub teams are \u0026ldquo;done\u0026rdquo;) IDE Related components working to externalize the dependency installation into repository pre-baked images  ","id":16,"section":"posts","summary":"GitHub Configuration in Code Fiddling with the configuration options available for GitHub, while encoding the properties in the .github directory.\nGetting Started This repository\nThe concept is to create an almost \u0026ldquo;self-contained\u0026rdquo; repository, that includes within hidden directories like .github/.aws/.azure/.gcp/etc that represent almost \u0026ldquo;interfaces\u0026rdquo; between the repository and external services that would act on it. This way rather than having the repository rely on assumptions about how it is configured, it is instead providing all the baseline elements for any supporting infrastructure (e.","tags":["org:jrbeverly"],"title":"github-config-in-code","uri":"/2022/03/jrbeverly-github-config-in-code/","year":"2022"},{"content":"GitHub App for Generated Commits Running through a bunch of things to be done with this\nNotes  Is it possible to push empty commits, which would need to be handled Commits should be crafted first, then attempts being made to apply the change Would need to have better visibility into the \u0026ldquo;crafted\u0026rdquo; files that will be committed Does this really require a GitHub Application? Feels like this could be decoupled Need to ensure that requests with the API go through the proper retry processes Need to ensure that the configuration is split from auth, so its easier to rotate the auth without fiddling with the config Should the app really be responsible for the work-component of crafting the change? Should all changes be made as a pull request + request to merge from the GitHub App? This would avoid conflicts as much, and let GitHub be responsible for changes?  Does this kind of thing really need a GitHub Application? Feels like this is something that can be split away from GitHub as a kind of entity that is responsible for the following actions:\n Pull the original source from the git repositories Apply the processes on the source files (regular text processing) Apply the change, and publish using standard git procedures  This wouldn\u0026rsquo;t require any additional overhead complexity with communicating with the GitHub repository. Or could this be something that is abstracted into a common application that is responsible for something like\n Receive a set of \u0026ldquo;Changes\u0026rdquo; as proposed within git Perform the change on behave of this system  This would allow commits to be \u0026ldquo;queued\u0026rdquo; and reduce a lot of the headache with dealing with this processes within the tool, and instead allow other tools to just communicate with the \u0026ldquo;Git Operations\u0026rdquo; API, which is much more fault tolerant.\n","id":17,"section":"posts","summary":"GitHub App for Generated Commits Running through a bunch of things to be done with this\nNotes  Is it possible to push empty commits, which would need to be handled Commits should be crafted first, then attempts being made to apply the change Would need to have better visibility into the \u0026ldquo;crafted\u0026rdquo; files that will be committed Does this really require a GitHub Application? Feels like this could be decoupled Need to ensure that requests with the API go through the proper retry processes Need to ensure that the configuration is split from auth, so its easier to rotate the auth without fiddling with the config Should the app really be responsible for the work-component of crafting the change?","tags":["org:jrbeverly"],"title":"github-app-for-code-change","uri":"/2022/02/jrbeverly-github-app-for-code-change/","year":"2022"},{"content":"GPG Artifact Sign Experiment Minor experiment with a shell script for signing artifacts that would be generated from a build process.\nNotes  Build tooling can support multiple checksum algorithms (sha256/sha1/md5/sha512/etc) Docker Container Trust (DCT) didn\u0026rsquo;t fit with the usecase/portability desired GPG is the standard way for doing this (can this be packaged into something more portable?) Design should aim to be agnostic of GitHub Releases or any other platform Build tooling likely doesn\u0026rsquo;t need to understand the concept of \u0026ldquo;signing\u0026rdquo; (or should it?) If build tools understand signing, does that mean there need to be \u0026lsquo;Developer Keys\u0026rsquo;?  Is this really just as just a matter of familiarity?   The CI/Build process is responsible for signing files to assert that \u0026ldquo;it\u0026rdquo; was responsible for the build (e.g. not just dev artifacts published to S3)  ","id":18,"section":"posts","summary":"GPG Artifact Sign Experiment Minor experiment with a shell script for signing artifacts that would be generated from a build process.\nNotes  Build tooling can support multiple checksum algorithms (sha256/sha1/md5/sha512/etc) Docker Container Trust (DCT) didn\u0026rsquo;t fit with the usecase/portability desired GPG is the standard way for doing this (can this be packaged into something more portable?) Design should aim to be agnostic of GitHub Releases or any other platform Build tooling likely doesn\u0026rsquo;t need to understand the concept of \u0026ldquo;signing\u0026rdquo; (or should it?","tags":["org:jrbeverly"],"title":"gpg-artifact-sign-exp","uri":"/2022/01/jrbeverly-gpg-artifact-sign-exp/","year":"2022"},{"content":"Rusy Bevy Baseline Fiddling with one of the Bevy examples for provisioning a window with Bevy.\nNotes  Initialize setup required installation of libraries like: libasound2-dev libwebkit2gtk-4.0 libudev-dev mingw-w64 Cross-compilation works, but associated GitHub Issues seemed conflicting Earlier version of bevy had issue with missing audio driver (devcontainer) failing the build  Don\u0026rsquo;t think will continue with this for now, maybe investigate later.\nComponents for the chess board sourced from https://caballerocoll.com/blog/bevy-chess-tutorial/\n","id":19,"section":"posts","summary":"Rusy Bevy Baseline Fiddling with one of the Bevy examples for provisioning a window with Bevy.\nNotes  Initialize setup required installation of libraries like: libasound2-dev libwebkit2gtk-4.0 libudev-dev mingw-w64 Cross-compilation works, but associated GitHub Issues seemed conflicting Earlier version of bevy had issue with missing audio driver (devcontainer) failing the build  Don\u0026rsquo;t think will continue with this for now, maybe investigate later.\nComponents for the chess board sourced from https://caballerocoll.","tags":["org:jrbeverly"],"title":"bevy-rustlang-example-window","uri":"/2022/01/jrbeverly-bevy-rustlang-example-window/","year":"2022"},{"content":"Experiment - WebAssembly Golang + Bazel Experimenting with some issues encountering with WebAssembly, Golang \u0026amp; Bazel\nNotes  Confirmed issue with syscall/js in the basecase with using goos and goarch (toolchains passed as orgs better option?) Using a genrule sufficient to workaround the case Base case with a simple calculator, using just base HTML js.Value conversions, framework wrapper to exist to handle the interop? Directory layout of cmd/ and app feels a bit off, but does help keep the bits separate  ","id":20,"section":"posts","summary":"Experiment - WebAssembly Golang + Bazel Experimenting with some issues encountering with WebAssembly, Golang \u0026amp; Bazel\nNotes  Confirmed issue with syscall/js in the basecase with using goos and goarch (toolchains passed as orgs better option?) Using a genrule sufficient to workaround the case Base case with a simple calculator, using just base HTML js.Value conversions, framework wrapper to exist to handle the interop? Directory layout of cmd/ and app feels a bit off, but does help keep the bits separate  ","tags":["org:jrbeverly"],"title":"exp-webassembly-golang-bazel","uri":"/2022/01/jrbeverly-exp-webassembly-golang-bazel/","year":"2022"},{"content":"Packer Overwrite MOTD Overwriting the MOTD of pre-baked AMIs using Packer\nNotes  Message files are located in /etc/update-motd.d/ Existing ones can be purged and replaced with a fixed one Likely want to keep the status components (source from existing, overwrite) Scripts need to be executable, and include shebang Current packer requires AWS for EC2 create/snapshot, can we avoid?  This works well for setting up a baseline for the AMIs. Is there a way we can perform this Packer construction without needing to be connected to AWS? As at the moment this more resembles \u0026ldquo;Assembly\u0026rdquo; than \u0026ldquo;Build\u0026rdquo;. Having a daemon (virtualbox/hyper-v) is less optimal, but if it can be solely run on any machine with the installed tools, then that would be preferable.\nPotential option is to use Virtualbox (or similar) to create an OVF file, upload to S3 and then import as an AMI using that model. That would require the build machines to have VirtualBox, but that can be worked around. This could make it so that a single server image is compatible with multiple clouds/execution environments.\nWhat about Cloud-specific optimizations or toolchains? E.g. AWS has the CLI, Session Manager and such? Could just be a post-build, like a platform/arch specific compilation for systems.\n","id":21,"section":"posts","summary":"Packer Overwrite MOTD Overwriting the MOTD of pre-baked AMIs using Packer\nNotes  Message files are located in /etc/update-motd.d/ Existing ones can be purged and replaced with a fixed one Likely want to keep the status components (source from existing, overwrite) Scripts need to be executable, and include shebang Current packer requires AWS for EC2 create/snapshot, can we avoid?  This works well for setting up a baseline for the AMIs.","tags":["org:jrbeverly"],"title":"packer-overwrite-motd","uri":"/2022/01/jrbeverly-packer-overwrite-motd/","year":"2022"},{"content":"CobraCMD with GenMarkdownTree Experiment with the GenMarkdownTree method available with cobrago.\nActions The command line utility can be executed by running:\nbazel run //cmd/cobradocs version The markdown tree for the docs can be generated by running:\nbazel run //docs/cobradocs -- --dir $PWD Notes  Markdown or YAML generated from the docs works fairly nice Better fit would be YAML, then providing it to a markdown templator (or alternative tools) Feels like having an OpenAPI spec for the CLI would make this process easier (build docs from the OpenAPI spec) Need to have a reference to the command object for the CLI Files are written into a directory, or dumped to a buffer \u0026amp; stdout Can be good for getting some docs out, but is this the ideal direction?  ","id":22,"section":"posts","summary":"CobraCMD with GenMarkdownTree Experiment with the GenMarkdownTree method available with cobrago.\nActions The command line utility can be executed by running:\nbazel run //cmd/cobradocs version The markdown tree for the docs can be generated by running:\nbazel run //docs/cobradocs -- --dir $PWD Notes  Markdown or YAML generated from the docs works fairly nice Better fit would be YAML, then providing it to a markdown templator (or alternative tools) Feels like having an OpenAPI spec for the CLI would make this process easier (build docs from the OpenAPI spec) Need to have a reference to the command object for the CLI Files are written into a directory, or dumped to a buffer \u0026amp; stdout Can be good for getting some docs out, but is this the ideal direction?","tags":["org:jrbeverly"],"title":"cobra-cmd-with-docs","uri":"/2022/01/jrbeverly-cobra-cmd-with-docs/","year":"2022"},{"content":"XTerm for Terminal as Browser Experimenting with the idea of a minimum environment for running terminal applications in browser. In essence, allowing a user to navigate to example.com/terminal to view a terminal version of the sites API. With the appropriate token \u0026amp; other bits provided from the browser session tokens.\nNotes  WebAssembly for Golang can be used in combination with this Browser token can be used to authenticate with the service, allowing for commands to exec against it Using something like the cobra yaml export (or deriving actions from OpenAPI-like spec), the JS interface can be generated Validation would be performed within the cmd, although a common \u0026lsquo;OpenAPI-like\u0026rsquo; spec would help reduce the complexity on this front (have better interacted with UIs) Leveraging other wrapper frameworks for xTerm.js would probably suit better for any syntax highlighting that would be desired / color schemas / etc Integration with local machine state for browsers would need to be something handled by the tool itself Better option might just be a browser \u0026ldquo;terminal\u0026rdquo; that understands an OpenAPI-like spec \u0026amp; appropriate JS/WASM bindings  ","id":23,"section":"posts","summary":"XTerm for Terminal as Browser Experimenting with the idea of a minimum environment for running terminal applications in browser. In essence, allowing a user to navigate to example.com/terminal to view a terminal version of the sites API. With the appropriate token \u0026amp; other bits provided from the browser session tokens.\nNotes  WebAssembly for Golang can be used in combination with this Browser token can be used to authenticate with the service, allowing for commands to exec against it Using something like the cobra yaml export (or deriving actions from OpenAPI-like spec), the JS interface can be generated Validation would be performed within the cmd, although a common \u0026lsquo;OpenAPI-like\u0026rsquo; spec would help reduce the complexity on this front (have better interacted with UIs) Leveraging other wrapper frameworks for xTerm.","tags":["org:jrbeverly"],"title":"xterm-for-cmd-as-site","uri":"/2022/01/jrbeverly-xterm-for-cmd-as-site/","year":"2022"},{"content":"AWS AssumeRole with Certificate for CI Exploring the concept of using AWS IoT Certificates for authenticating with AWS.\nThis came up while working with minio, which supports authentication with certificates:\n MinIO provides a custom STS API that allows authentication with client X.509 / TLS certificates.\n Getting Started The commands boostrap.sh and cert.sh are provided for working with the IoT devices. Bootstrap is responsible for provisioning the certificate, IoT resources, and the permissionless IAM role. Assuming that AWS credentials are configured, a test can be started by running the following:\nbash scripts/bootstrap.sh github-my-repository-name This will create the following resources:\n An IAM Role with a trust policy for IoT credentials An IoT Thing \u0026amp; ThingType An IoT Certificate An IoT Role Alias, Policy \u0026amp; associations with the ceritifcate \u0026amp; thing  The output of this command will be located in the output directory under the name provided above (e.g. github-my-repository-name). This will contain the certificate, private/public keys and auth.yaml. The auth.yaml file is intended to work similar to a kubeconfig file, allowing the command get-aws-credentials.bash to retrieve credentials from AWS over HTTPS.\nThis can be done by then running:\nbash scripts/get-aws-credentials.bash github-my-repository-name Notes This has some potential. Rather than leveraging Terraform, I wanted to use an API oriented approach to this, as that is likely how the certificates + associated components would be provisioned. They would be created by an operator-like service that identifies repositories that are requesting authentication to the artifact publishing services. The credentials would be populated into the Continuous Integration/Delivery platform (like GitHub Actions), with rotation and revocation happening base on how the operator is interacted with.\nIt may be possible with this model to support using the \u0026ldquo;generic\u0026rdquo; S3 API for publishing, rather than relying on the AWS CLI. This wouldn\u0026rsquo;t decoupled the CI/CD system from AWS credentials \u0026amp; publishing, but it does create a case where we are no longer relying on AWS_* environment variables, but instead on things like X.509 certificates, and standard-based publishers (e.g. minio - mc)\n","id":24,"section":"posts","summary":"AWS AssumeRole with Certificate for CI Exploring the concept of using AWS IoT Certificates for authenticating with AWS.\nThis came up while working with minio, which supports authentication with certificates:\n MinIO provides a custom STS API that allows authentication with client X.509 / TLS certificates.\n Getting Started The commands boostrap.sh and cert.sh are provided for working with the IoT devices. Bootstrap is responsible for provisioning the certificate, IoT resources, and the permissionless IAM role.","tags":["org:jrbeverly"],"title":"aws-assumerole-with-cert","uri":"/2022/01/jrbeverly-aws-assumerole-with-cert/","year":"2022"},{"content":"AWSS3 \u0026amp; AWS Cognito Fiddling with AWS S3 Websites leveraging AWS Cognito for authentication\nTerraform is based on the tutorial https://transcend.io/blog/restrict-access-to-internal-websites-with-beyondcorp/ and the public repository: https://github.com/transcend-io/beyondcorp-cloudfront\nNotes  This approach isn\u0026rsquo;t really something I think is great The lack of ease configuration for Lambdas, and the need to either embed configuration in the lambda zip, or through AWS SSM is not ideal Either a single monolith terraform module, or split between multiple. Adds a lot of overhead which is less than ideal AWS S3 just doesn\u0026rsquo;t feel like a good source for storing the S3 artifacts - requires bucket policy + \u0026ldquo;public\u0026rdquo; - Don\u0026rsquo;t like Seems more preferable to have something like K8s + Nginx pod - Management of secrets/dns/etc can be externalized then Docker images gives the opportunity to sign with something like cosign, ability to \u0026ldquo;extend\u0026rdquo; with URL rewrites/etc through nginx itself Authentication can be fronted as part of existing K8s services used by backends Entire system can be validated with a local installation of K8s Alternative caching layers seem more ideal (e.g. Cloudflare pages / etc), thing that can go infront of the primary service Could a k8s-ingress served entity just synchronize with other CDNs? Cloudfront \u0026ldquo;deploying\u0026rdquo; wait times are kind of in the way for rapid changes (exists in k8s with pods, but cloudfront just feels slower?)  Overall not really liking this approach, as it doesn\u0026rsquo;t simplify the system as much as I was hoping it would.\n","id":25,"section":"posts","summary":"AWSS3 \u0026amp; AWS Cognito Fiddling with AWS S3 Websites leveraging AWS Cognito for authentication\nTerraform is based on the tutorial https://transcend.io/blog/restrict-access-to-internal-websites-with-beyondcorp/ and the public repository: https://github.com/transcend-io/beyondcorp-cloudfront\nNotes  This approach isn\u0026rsquo;t really something I think is great The lack of ease configuration for Lambdas, and the need to either embed configuration in the lambda zip, or through AWS SSM is not ideal Either a single monolith terraform module, or split between multiple.","tags":["org:jrbeverly"],"title":"cloudfront-cognito-private-auth","uri":"/2022/01/jrbeverly-cloudfront-cognito-private-auth/","year":"2022"},{"content":"Bazel Container Image Rules    Bazel CI         Generated API documentation is in the docs folder, or you can browse it online at https://docs.aspect.dev/rules_docker\nBasic Rules  container_image (example) container_bundle (example) container_import container_load container_pull (example) container_push (example)  These rules used to be docker_build, docker_push, etc. and the aliases for these (mostly) legacy names still exist largely for backwards-compatibility. We also have early-stage oci_image, oci_push, etc. aliases for folks that enjoy the consistency of a consistent rule prefix. The only place the format-specific names currently do any more than alias things is in foo_push, where they also specify the appropriate format as which to publish the image.\nOverview This repository contains a set of rules for pulling down base images, augmenting them with build artifacts and assets, and publishing those images. These rules do not require / use Docker for pulling, building, or pushing images. This means:\n They can be used to develop Docker containers on OSX without boot2docker or docker-machine installed. Note use of these rules on Windows is currently not supported. They do not require root access on your workstation.  Also, unlike traditional container builds (e.g. Dockerfile), the Docker images produced by container_image are deterministic / reproducible.\nTo get started with building Docker images, check out the examples that build the same images using both rules_docker and a Dockerfile.\nNOTE: container_push and container_pull make use of google/go-containerregistry for registry interactions.\nLanguage Rules  py_image (signature) py3_image (signature) nodejs_image (usage) java_image (signature) war_image (signature) scala_image (signature) groovy_image (signature) cc_image (signature) go_image (signature) rust_image (signature) d_image (signature)  It is notable that: cc_image, go_image, rust_image, and d_image also allow you to specify an external binary target.\nDocker Rules This repo now includes rules that provide additional functionality to install packages and run commands inside docker containers. These rules, however, require a docker binary is present and properly configured. These rules include:\n Package manager rules: rules to install apt-get packages. Docker run rules: rules to run commands inside docker containers.  Overview In addition to low-level rules for building containers, this repository provides a set of higher-level rules for containerizing applications. The idea behind these rules is to make containerizing an application built via a lang_binary rule as simple as changing it to lang_image.\nBy default these higher level rules make use of the distroless language runtimes, but these can be overridden via the base=\u0026quot;...\u0026quot; attribute (e.g. with a container_pull or container_image target).\nNote also that these rules do not expose any docker related attributes. If you need to add a custom env or symlink to a lang_image, you must use container_image targets for this purpose. Specifically, you can use as base for your lang_image target a container_image target that adds e.g., custom env or symlink. Please see go_image (custom base)for an example.\nSetup Add the following to your WORKSPACE file to add the external repositories:\nload(\u0026#34;@bazel_tools//tools/build_defs/repo:http.bzl\u0026#34;, \u0026#34;http_archive\u0026#34;) http_archive( # Get copy paste instructions for the http_archive attributes from the # release notes at https://github.com/bazelbuild/rules_docker/releases ) # OPTIONAL: Call this to override the default docker toolchain configuration. # This call should be placed BEFORE the call to \u0026#34;container_repositories\u0026#34; below # to actually override the default toolchain configuration. # Note this is only required if you actually want to call # docker_toolchain_configure with a custom attr; please read the toolchains # docs in /toolchains/docker/ before blindly adding this to your WORKSPACE. # BEGIN OPTIONAL segment: load(\u0026#34;@io_bazel_rules_docker//toolchains/docker:toolchain.bzl\u0026#34;, docker_toolchain_configure=\u0026#34;toolchain_configure\u0026#34; ) docker_toolchain_configure( name = \u0026#34;docker_config\u0026#34;, # OPTIONAL: Bazel target for the build_tar tool, must be compatible with build_tar.py build_tar_target=\u0026#34;\u0026lt;enter absolute path (i.e., must start with repo name @...//:...) to an executable build_tar target\u0026gt;\u0026#34;, # OPTIONAL: Path to a directory which has a custom docker client config.json. # See https://docs.docker.com/engine/reference/commandline/cli/#configuration-files # for more details. client_config=\u0026#34;\u0026lt;enter absolute path to your docker config directory here\u0026gt;\u0026#34;, # OPTIONAL: Path to the docker binary. # Should be set explicitly for remote execution. docker_path=\u0026#34;\u0026lt;enter absolute path to the docker binary (in the remote exec env) here\u0026gt;\u0026#34;, # OPTIONAL: Path to the gzip binary. gzip_path=\u0026#34;\u0026lt;enter absolute path to the gzip binary (in the remote exec env) here\u0026gt;\u0026#34;, # OPTIONAL: Bazel target for the gzip tool. gzip_target=\u0026#34;\u0026lt;enter absolute path (i.e., must start with repo name @...//:...) to an executable gzip target\u0026gt;\u0026#34;, # OPTIONAL: Path to the xz binary. # Should be set explicitly for remote execution. xz_path=\u0026#34;\u0026lt;enter absolute path to the xz binary (in the remote exec env) here\u0026gt;\u0026#34;, # OPTIONAL: Bazel target for the xz tool. # Either xz_path or xz_target should be set explicitly for remote execution. xz_target=\u0026#34;\u0026lt;enter absolute path (i.e., must start with repo name @...//:...) to an executable xz target\u0026gt;\u0026#34;, # OPTIONAL: List of additional flags to pass to the docker command. docker_flags = [ \u0026#34;--tls\u0026#34;, \u0026#34;--log-level=info\u0026#34;, ], ) # End of OPTIONAL segment. load( \u0026#34;@io_bazel_rules_docker//repositories:repositories.bzl\u0026#34;, container_repositories = \u0026#34;repositories\u0026#34;, ) container_repositories() load(\u0026#34;@io_bazel_rules_docker//repositories:deps.bzl\u0026#34;, container_deps = \u0026#34;deps\u0026#34;) container_deps() load( \u0026#34;@io_bazel_rules_docker//container:container.bzl\u0026#34;, \u0026#34;container_pull\u0026#34;, ) container_pull( name = \u0026#34;java_base\u0026#34;, registry = \u0026#34;gcr.io\u0026#34;, repository = \u0026#34;distroless/java\u0026#34;, # \u0026#39;tag\u0026#39; is also supported, but digest is encouraged for reproducibility. digest = \u0026#34;sha256:deadbeef\u0026#34;, ) Known Issues  Bazel does not deal well with diamond dependencies.  If the repositories that are imported by container_repositories() have already been imported (at a different version) by other rules you called in your WORKSPACE, which are placed above the call to container_repositories(), arbitrary errors might occur. If you get errors related to external repositories, you will likely not be able to use container_repositories() and will have to import directly in your WORKSPACE all the required dependencies (see the most up to date impl of container_repositories() for details).\n ImportError: No module named moves.urllib.parse  This is an example of an error due to a diamond dependency. If you get this error, make sure to import rules_docker before other libraries, so that six can be patched properly.\nSee https://github.com/bazelbuild/rules_docker/issues/1022 for more details.\n Ensure your project has a BUILD or BUILD.bazel file at the top level. This can be a blank file if necessary. Otherwise you might see an error that looks like:  Unable to load package for //:WORKSPACE: BUILD file not found in any of the following directories. Using with Docker locally. Suppose you have a container_image target //my/image:helloworld:\ncontainer_image( name = \u0026#34;helloworld\u0026#34;, ... ) You can load this into your local Docker client by running: bazel run my/image:helloworld.\nFor the lang_image targets, this will also run the container using docker run to maximize compatibility with lang_binary rules.\nArguments to this command are forwarded to docker, meaning the command\nbazel run my/image:helloworld -- -p 8080:80 -- arg0 performs the following steps:\n load the my/image:helloworld target into your local Docker client start a container using this image where arg0 is passed to the image entrypoint port forward 8080 on the host to port 80 on the container, as per docker run documentation  You can suppress this behavior by passing the single flag: bazel run :foo -- --norun\nAlternatively, you can build a docker load compatible bundle with: bazel build my/image:helloworld.tar. This will produce the file: bazel-bin/my/image/helloworld.tar, which you can load into your local Docker client by running: docker load -i bazel-bin/my/image/helloworld.tar. Building this target can be expensive for large images.\nThese work with both container_image, container_bundle, and the lang_image rules. For everything except container_bundle, the image name will be bazel/my/image:helloworld. For container_bundle, it will apply the tags you have specified.\nAuthentication You can use these rules to access private images using standard Docker authentication methods. e.g. to utilize the Google Container Registry. See here for authentication methods.\nSee also:\n Amazon ECR Docker Credential Helper Azure Docker Credential Helper  Once you\u0026rsquo;ve setup your docker client configuration, see here for an example of how to use container_pull with custom docker authentication credentials and here for an example of how to use container_push with custom docker authentication credentials.\nVarying image names A common request from folks using container_push, container_bundle, or container_image is to be able to vary the tag that is pushed or embedded. There are two options at present for doing this.\nStamping The first option is to use stamping. Stamping is enabled when bazel is run with --stamp. This enables replacements in stamp-aware attributes. A python format placeholder (e.g. {BUILD_USER}) is replaced by the value of the corresponding workspace-status variable.\n# A common pattern when users want to avoid trampling # on each other\u0026#39;s images during development. container_push( name = \u0026#34;publish\u0026#34;, format = \u0026#34;Docker\u0026#34;, # Any of these components may have variables. registry = \u0026#34;gcr.io\u0026#34;, repository = \u0026#34;my-project/my-image\u0026#34;, # This will be replaced with the current user when built with --stamp tag = \u0026#34;{BUILD_USER}\u0026#34;, )  Rules that are sensitive to stamping can also be forced to stamp or non-stamp mode irrespective of the --stamp flag to Bazel. Use the build_context_data rule to make a target that provides StampSettingInfo, and pass this to the build_context_data attribute.\n The next natural question is: \u0026ldquo;Well what variables can I use?\u0026rdquo; This option consumes the workspace-status variables Bazel defines in bazel-out/stable-status.txt and bazel-out/volatile-status.txt.\n Note that changes to the stable-status file cause a rebuild of the action, while volatile-status does not.\n You can add more stamp variables via --workspace_status_command, see the bazel docs. A common example is to provide the current git SHA, with --workspace_status_command=\u0026quot;echo STABLE_GIT_SHA $(git rev-parse HEAD)\u0026quot;\nThat flag is typically passed in the .bazelrc file, see for example .bazelrc in kubernetes.\nMake variables The second option is to employ Makefile-style variables:\ncontainer_bundle( name = \u0026#34;bundle\u0026#34;, images = { \u0026#34;gcr.io/$(project)/frontend:latest\u0026#34;: \u0026#34;//frontend:image\u0026#34;, \u0026#34;gcr.io/$(project)/backend:latest\u0026#34;: \u0026#34;//backend:image\u0026#34;, } ) These variables are specified on the CLI using:\nbazel build --define project=blah //path/to:bundle Debugging lang_image rules By default the lang_image rules use the distroless base runtime images, which are optimized to be the minimal set of things your application needs at runtime. That can make debugging these containers difficult because they lack even a basic shell for exploring the filesystem.\nTo address this, we publish variants of the distroless runtime images tagged :debug, which are the exact-same images, but with additions such as busybox to make debugging easier.\nFor example (in this repo):\n$ bazel run -c dbg testdata:go_image ... INFO: Build completed successfully, 5 total actions INFO: Running command line: bazel-bin/testdata/go_image Loaded image ID: sha256:9c5c2167a1db080a64b5b401b43b3c5cdabb265b26cf7a60aabe04a20da79e24 Tagging 9c5c2167a1db080a64b5b401b43b3c5cdabb265b26cf7a60aabe04a20da79e24 as bazel/testdata:go_image Hello, world! $ docker run -ti --rm --entrypoint=sh bazel/testdata:go_image -c \u0026#34;echo Hello, busybox.\u0026#34; Hello, busybox. Examples container_image container_image( name = \u0026#34;app\u0026#34;, # References container_pull from WORKSPACE (above) base = \u0026#34;@java_base//image\u0026#34;, files = [\u0026#34;//java/com/example/app:Hello_deploy.jar\u0026#34;], cmd = [\u0026#34;Hello_deploy.jar\u0026#34;] ) Hint: if you want to put files in specific directories inside the image use pkg_tar ruleto create the desired directory structure and pass that to container_image via tars attribute. Note you might need to set strip_prefix = \u0026quot;.\u0026quot; or strip_prefix = \u0026quot;{some directory}\u0026quot; in your rule for the files to not be flattened. See Bazel upstream issue 2176and rules_docker issue 317for more details.\ncc_image To use cc_image, add the following to WORKSPACE:\nload( \u0026#34;@io_bazel_rules_docker//repositories:repositories.bzl\u0026#34;, container_repositories = \u0026#34;repositories\u0026#34;, ) container_repositories() load( \u0026#34;@io_bazel_rules_docker//cc:image.bzl\u0026#34;, _cc_image_repos = \u0026#34;repositories\u0026#34;, ) _cc_image_repos() Then in your BUILD file, simply rewrite cc_binary to cc_image with the following import:\nload(\u0026#34;@io_bazel_rules_docker//cc:image.bzl\u0026#34;, \u0026#34;cc_image\u0026#34;) cc_image( name = \u0026#34;cc_image\u0026#34;, srcs = [\u0026#34;cc_image.cc\u0026#34;], deps = [\u0026#34;:cc_image_library\u0026#34;], ) cc_image (external binary) To use cc_image (or go_image, d_image, rust_image) with an external cc_binary (or the like) target, then your BUILD file should instead look like:\nload(\u0026#34;@io_bazel_rules_docker//cc:image.bzl\u0026#34;, \u0026#34;cc_image\u0026#34;) cc_binary( name = \u0026#34;cc_binary\u0026#34;, srcs = [\u0026#34;cc_binary.cc\u0026#34;], deps = [\u0026#34;:cc_library\u0026#34;], ) cc_image( name = \u0026#34;cc_image\u0026#34;, binary = \u0026#34;:cc_binary\u0026#34;, ) If you need to modify somehow the container produced by cc_image (e.g., env, symlink), see note above in Language Rules Overviewabout how to do this and see go_image (custom base)example below.\npy_image To use py_image, add the following to WORKSPACE:\nload( \u0026#34;@io_bazel_rules_docker//repositories:repositories.bzl\u0026#34;, container_repositories = \u0026#34;repositories\u0026#34;, ) container_repositories() load( \u0026#34;@io_bazel_rules_docker//python:image.bzl\u0026#34;, _py_image_repos = \u0026#34;repositories\u0026#34;, ) _py_image_repos() Then in your BUILD file, simply rewrite py_binary to py_image with the following import:\nload(\u0026#34;@io_bazel_rules_docker//python:image.bzl\u0026#34;, \u0026#34;py_image\u0026#34;) py_image( name = \u0026#34;py_image\u0026#34;, srcs = [\u0026#34;py_image.py\u0026#34;], deps = [\u0026#34;:py_image_library\u0026#34;], main = \u0026#34;py_image.py\u0026#34;, ) If you need to modify somehow the container produced by py_image (e.g., env, symlink), see note above in Language Rules Overviewabout how to do this and see go_image (custom base)example below.\nIf you are using py_image with a custom base that has python tools installed in a location different to the default base, please see Python tools.\npy_image (fine layering) For Python and Java\u0026rsquo;s lang_image rules, you can factor dependencies that don\u0026rsquo;t change into their own layers by overriding the layers=[] attribute. Consider this sample from the rules_k8s repository:\npy_image( name = \u0026#34;server\u0026#34;, srcs = [\u0026#34;server.py\u0026#34;], # \u0026#34;layers\u0026#34; is just like \u0026#34;deps\u0026#34;, but it also moves the dependencies each into # their own layer, which can dramatically improve developer cycle time. For # example here, the grpcio layer is ~40MB, but the rest of the app is only # ~400KB. By partitioning things this way, the large grpcio layer remains # unchanging and we can reduce the amount of image data we repush by ~99%! layers = [ requirement(\u0026#34;grpcio\u0026#34;), \u0026#34;//examples/hellogrpc/proto:py\u0026#34;, ], main = \u0026#34;server.py\u0026#34;, ) You can also implement more complex fine layering strategies by using the py_layer rule and its filter attribute. For example:\n# Suppose that we are synthesizing an image that depends on a complex set # of libraries that we want to break into layers. LIBS = [ \u0026#34;//pkg/complex_library\u0026#34;, # ... ] # First, we extract all transitive dependencies of LIBS that are under //pkg/common. py_layer( name = \u0026#34;common_deps\u0026#34;, deps = LIBS, filter = \u0026#34;//pkg/common\u0026#34;, ) # Then, we further extract all external dependencies of the deps under //pkg/common. py_layer( name = \u0026#34;common_external_deps\u0026#34;, deps = [\u0026#34;:common_deps\u0026#34;], filter = \u0026#34;@\u0026#34;, ) # We also extract all external dependencies of LIBS, which is a superset of # \u0026#34;:common_external_deps\u0026#34;. py_layer( name = \u0026#34;external_deps\u0026#34;, deps = LIBS, filter = \u0026#34;@\u0026#34;, ) # Finally, we create the image, stacking the above filtered layers on top of one # another in the \u0026#34;layers\u0026#34; attribute. The layers are applied in order, and any # dependencies already added to the image will not be added again. Therefore, # \u0026#34;:external_deps\u0026#34; will only add the external dependencies not present in # \u0026#34;:common_external_deps\u0026#34;. py_image( name = \u0026#34;image\u0026#34;, deps = LIBS, layers = [ \u0026#34;:common_external_deps\u0026#34;, \u0026#34;:common_deps\u0026#34;, \u0026#34;:external_deps\u0026#34;, ], # ... ) py3_image To use a Python 3 runtime instead of the default of Python 2, use py3_image, instead of py_image. The other semantics are identical.\nIf you need to modify somehow the container produced by py3_image (e.g., env, symlink), see note above in Language Rules Overviewabout how to do this and see go_image (custom base)example below.\nIf you are using py3_image with a custom base that has python tools installed in a location different to the default base, please see Python tools.\nnodejs_image It is notable that unlike the other image rules, nodejs_image is not currently using the gcr.io/distroless/nodejs image for a handful of reasons. This is a switch we plan to make, when we can manage it. We are currently utilizing the gcr.io/google-appengine/debian9 image as our base.\nTo use nodejs_image, add the following to WORKSPACE:\nload(\u0026#34;@bazel_tools//tools/build_defs/repo:http.bzl\u0026#34;, \u0026#34;http_archive\u0026#34;) http_archive( name = \u0026#34;build_bazel_rules_nodejs\u0026#34;, # Replace with a real SHA256 checksum sha256 = \u0026#34;{SHA256}\u0026#34; # Replace with a real release version urls = [\u0026#34;https://github.com/bazelbuild/rules_nodejs/releases/download/{VERSION}/rules_nodejs-{VERSION}.tar.gz\u0026#34;], ) load(\u0026#34;@build_bazel_rules_nodejs//:index.bzl\u0026#34;, \u0026#34;npm_install\u0026#34;) # Install your declared Node.js dependencies npm_install( name = \u0026#34;npm\u0026#34;, package_json = \u0026#34;//:package.json\u0026#34;, yarn_lock = \u0026#34;//:yarn.lock\u0026#34;, ) load( \u0026#34;@io_bazel_rules_docker//repositories:repositories.bzl\u0026#34;, container_repositories = \u0026#34;repositories\u0026#34;, ) container_repositories() load( \u0026#34;@io_bazel_rules_docker//nodejs:image.bzl\u0026#34;, _nodejs_image_repos = \u0026#34;repositories\u0026#34;, ) _nodejs_image_repos() Note: See note about diamond dependencies in setupif you run into issues related to external repos after adding these lines to your WORKSPACE.\nThen in your BUILD file, simply rewrite nodejs_binary to nodejs_image with the following import:\nload(\u0026#34;@io_bazel_rules_docker//nodejs:image.bzl\u0026#34;, \u0026#34;nodejs_image\u0026#34;) nodejs_image( name = \u0026#34;nodejs_image\u0026#34;, entry_point = \u0026#34;@your_workspace//path/to:file.js\u0026#34;, # npm deps will be put into their own layer data = [\u0026#34;:file.js\u0026#34;, \u0026#34;@npm//some-npm-dep\u0026#34;], ... ) nodejs_image also supports the launcher and launcher_args attributes which are passed to container_image and used to prefix the image\u0026rsquo;s entry_point.\nIf you need to modify somehow the container produced by nodejs_image (e.g., env, symlink), see note above in Language Rules Overviewabout how to do this and see go_image (custom base)example below.\ngo_image To use go_image, add the following to WORKSPACE:\nload(\u0026#34;@bazel_tools//tools/build_defs/repo:http.bzl\u0026#34;, \u0026#34;http_archive\u0026#34;) load( \u0026#34;@io_bazel_rules_docker//repositories:repositories.bzl\u0026#34;, container_repositories = \u0026#34;repositories\u0026#34;, ) container_repositories() load( \u0026#34;@io_bazel_rules_docker//go:image.bzl\u0026#34;, _go_image_repos = \u0026#34;repositories\u0026#34;, ) _go_image_repos() Note: See note about diamond dependencies in setupif you run into issues related to external repos after adding these lines to your WORKSPACE.\nThen in your BUILD file, simply rewrite go_binary to go_image with the following import:\nload(\u0026#34;@io_bazel_rules_docker//go:image.bzl\u0026#34;, \u0026#34;go_image\u0026#34;) go_image( name = \u0026#34;go_image\u0026#34;, srcs = [\u0026#34;main.go\u0026#34;], importpath = \u0026#34;github.com/your/path/here\u0026#34;, ) Notice that it is important to explicitly build this target with the --platforms=@io_bazel_rules_go//go/toolchain:linux_amd64 flag as the binary should be built for Linux since it will run in a Linux container.\nIf you need to modify somehow the container produced by go_image (e.g., env, symlink), see note above in Language Rules Overviewabout how to do this and see example below.\ngo_image (custom base) To use a custom base image, with any of the lang_image rules, you can override the default base=\u0026quot;...\u0026quot; attribute. Consider this modified sample from the distroless repository:\nload(\u0026#34;@rules_pkg//:pkg.bzl\u0026#34;, \u0026#34;pkg_tar\u0026#34;) # Create a passwd file with a root and nonroot user and uid. passwd_entry( username = \u0026#34;root\u0026#34;, uid = 0, gid = 0, name = \u0026#34;root_user\u0026#34;, ) passwd_entry( username = \u0026#34;nonroot\u0026#34;, info = \u0026#34;nonroot\u0026#34;, uid = 1002, name = \u0026#34;nonroot_user\u0026#34;, ) passwd_file( name = \u0026#34;passwd\u0026#34;, entries = [ \u0026#34;:root_user\u0026#34;, \u0026#34;:nonroot_user\u0026#34;, ], ) # Create a tar file containing the created passwd file pkg_tar( name = \u0026#34;passwd_tar\u0026#34;, srcs = [\u0026#34;:passwd\u0026#34;], mode = \u0026#34;0o644\u0026#34;, package_dir = \u0026#34;etc\u0026#34;, ) # Include it in our base image as a tar. container_image( name = \u0026#34;passwd_image\u0026#34;, base = \u0026#34;@go_image_base//image\u0026#34;, tars = [\u0026#34;:passwd_tar\u0026#34;], user = \u0026#34;nonroot\u0026#34;, ) # Simple go program to print out the username and uid. go_image( name = \u0026#34;user\u0026#34;, srcs = [\u0026#34;user.go\u0026#34;], # Override the base image. base = \u0026#34;:passwd_image\u0026#34;, ) java_image To use java_image, add the following to WORKSPACE:\nload( \u0026#34;@io_bazel_rules_docker//repositories:repositories.bzl\u0026#34;, container_repositories = \u0026#34;repositories\u0026#34;, ) container_repositories() load( \u0026#34;@io_bazel_rules_docker//java:image.bzl\u0026#34;, _java_image_repos = \u0026#34;repositories\u0026#34;, ) _java_image_repos() Then in your BUILD file, simply rewrite java_binary to java_image with the following import:\nload(\u0026#34;@io_bazel_rules_docker//java:image.bzl\u0026#34;, \u0026#34;java_image\u0026#34;) java_image( name = \u0026#34;java_image\u0026#34;, srcs = [\u0026#34;Binary.java\u0026#34;], # Put these runfiles into their own layer. layers = [\u0026#34;:java_image_library\u0026#34;], main_class = \u0026#34;examples.images.Binary\u0026#34;, ) If you need to modify somehow the container produced by java_image (e.g., env, symlink), see note above in Language Rules Overviewabout how to do this and see go_image (custom base)example.\nwar_image To use war_image, add the following to WORKSPACE:\nload( \u0026#34;@io_bazel_rules_docker//repositories:repositories.bzl\u0026#34;, container_repositories = \u0026#34;repositories\u0026#34;, ) container_repositories() load( \u0026#34;@io_bazel_rules_docker//java:image.bzl\u0026#34;, _java_image_repos = \u0026#34;repositories\u0026#34;, ) _java_image_repos() Note: See note about diamond dependencies in setupif you run into issues related to external repos after adding these lines to your WORKSPACE.\nThen in your BUILD file, simply rewrite java_war to war_image with the following import:\nload(\u0026#34;@io_bazel_rules_docker//java:image.bzl\u0026#34;, \u0026#34;war_image\u0026#34;) war_image( name = \u0026#34;war_image\u0026#34;, srcs = [\u0026#34;Servlet.java\u0026#34;], # Put these JARs into their own layers. layers = [ \u0026#34;:java_image_library\u0026#34;, \u0026#34;@javax_servlet_api//jar:jar\u0026#34;, ], ) The produced image uses Jetty 9.x to serve the web application. Servlets included in the web application need to follow the API specification 3.0. For best compatibility, use a Servlet dependency provided by the Jetty project.\nA Servlet implementation needs to declare the @WebServlet annotation to be auto-discovered. The use of a web.xml to declare the Servlet URL mapping is not supported.\nIf you need to modify somehow the container produced by war_image (e.g., env, symlink), see note above in Language Rules Overviewabout how to do this and see go_image (custom base)example.\nscala_image To use scala_image, add the following to WORKSPACE:\nload(\u0026#34;@bazel_tools//tools/build_defs/repo:http.bzl\u0026#34;, \u0026#34;http_archive\u0026#34;) # You *must* import the Scala rules before setting up the scala_image rules. http_archive( name = \u0026#34;io_bazel_rules_scala\u0026#34;, # Replace with a real SHA256 checksum sha256 = \u0026#34;{SHA256}\u0026#34; # Replace with a real commit SHA strip_prefix = \u0026#34;rules_scala-{HEAD}\u0026#34;, urls = [\u0026#34;https://github.com/bazelbuild/rules_scala/archive/{HEAD}.tar.gz\u0026#34;], ) load(\u0026#34;@io_bazel_rules_scala//scala:scala.bzl\u0026#34;, \u0026#34;scala_repositories\u0026#34;) scala_repositories() load( \u0026#34;@io_bazel_rules_docker//repositories:repositories.bzl\u0026#34;, container_repositories = \u0026#34;repositories\u0026#34;, ) container_repositories() load( \u0026#34;@io_bazel_rules_docker//scala:image.bzl\u0026#34;, _scala_image_repos = \u0026#34;repositories\u0026#34;, ) _scala_image_repos() Note: See note about diamond dependencies in setupif you run into issues related to external repos after adding these lines to your WORKSPACE.\nThen in your BUILD file, simply rewrite scala_binary to scala_image with the following import:\nload(\u0026#34;@io_bazel_rules_docker//scala:image.bzl\u0026#34;, \u0026#34;scala_image\u0026#34;) scala_image( name = \u0026#34;scala_image\u0026#34;, srcs = [\u0026#34;Binary.scala\u0026#34;], main_class = \u0026#34;examples.images.Binary\u0026#34;, ) If you need to modify somehow the container produced by scala_image (e.g., env, symlink), see note above in Language Rules Overviewabout how to do this and see go_image (custom base)example.\ngroovy_image To use groovy_image, add the following to WORKSPACE:\nload(\u0026#34;@bazel_tools//tools/build_defs/repo:http.bzl\u0026#34;, \u0026#34;http_archive\u0026#34;) # You *must* import the Groovy rules before setting up the groovy_image rules. http_archive( name = \u0026#34;io_bazel_rules_groovy\u0026#34;, # Replace with a real SHA256 checksum sha256 = \u0026#34;{SHA256}\u0026#34; # Replace with a real commit SHA strip_prefix = \u0026#34;rules_groovy-{HEAD}\u0026#34;, urls = [\u0026#34;https://github.com/bazelbuild/rules_groovy/archive/{HEAD}.tar.gz\u0026#34;], ) load(\u0026#34;@io_bazel_rules_groovy//groovy:groovy.bzl\u0026#34;, \u0026#34;groovy_repositories\u0026#34;) groovy_repositories() load( \u0026#34;@io_bazel_rules_docker//repositories:repositories.bzl\u0026#34;, container_repositories = \u0026#34;repositories\u0026#34;, ) container_repositories() load( \u0026#34;@io_bazel_rules_docker//groovy:image.bzl\u0026#34;, _groovy_image_repos = \u0026#34;repositories\u0026#34;, ) _groovy_image_repos() Note: See note about diamond dependencies in setupif you run into issues related to external repos after adding these lines to your WORKSPACE.\nThen in your BUILD file, simply rewrite groovy_binary to groovy_image with the following import:\nload(\u0026#34;@io_bazel_rules_docker//groovy:image.bzl\u0026#34;, \u0026#34;groovy_image\u0026#34;) groovy_image( name = \u0026#34;groovy_image\u0026#34;, srcs = [\u0026#34;Binary.groovy\u0026#34;], main_class = \u0026#34;examples.images.Binary\u0026#34;, ) If you need to modify somehow the container produced by groovy_image (e.g., env, symlink), see note above in Language Rules Overviewabout how to do this and see go_image (custom base)example.\nrust_image To use rust_image, add the following to WORKSPACE:\nload(\u0026#34;@bazel_tools//tools/build_defs/repo:http.bzl\u0026#34;, \u0026#34;http_archive\u0026#34;) # You *must* import the Rust rules before setting up the rust_image rules. http_archive( name = \u0026#34;rules_rust\u0026#34;, # Replace with a real SHA256 checksum sha256 = \u0026#34;{SHA256}\u0026#34; # Replace with a real commit SHA strip_prefix = \u0026#34;rules_rust-{HEAD}\u0026#34;, urls = [\u0026#34;https://github.com/bazelbuild/rules_rust/archive/{HEAD}.tar.gz\u0026#34;], ) load(\u0026#34;@rules_rust//rust:repositories.bzl\u0026#34;, \u0026#34;rust_repositories\u0026#34;) rust_repositories() load( \u0026#34;@io_bazel_rules_docker//repositories:repositories.bzl\u0026#34;, container_repositories = \u0026#34;repositories\u0026#34;, ) container_repositories() load( \u0026#34;@io_bazel_rules_docker//rust:image.bzl\u0026#34;, _rust_image_repos = \u0026#34;repositories\u0026#34;, ) _rust_image_repos() Note: See note about diamond dependencies in setupif you run into issues related to external repos after adding these lines to your WORKSPACE.\nThen in your BUILD file, simply rewrite rust_binary to rust_image with the following import:\nload(\u0026#34;@io_bazel_rules_docker//rust:image.bzl\u0026#34;, \u0026#34;rust_image\u0026#34;) rust_image( name = \u0026#34;rust_image\u0026#34;, srcs = [\u0026#34;main.rs\u0026#34;], ) If you need to modify somehow the container produced by rust_image (e.g., env, symlink), see note above in Language Rules Overviewabout how to do this and see go_image (custom base)example.\nd_image To use d_image, add the following to WORKSPACE:\nload(\u0026#34;@bazel_tools//tools/build_defs/repo:http.bzl\u0026#34;, \u0026#34;http_archive\u0026#34;) # You *must* import the D rules before setting up the d_image rules. http_archive( name = \u0026#34;io_bazel_rules_d\u0026#34;, # Replace with a real SHA256 checksum sha256 = \u0026#34;{SHA256}\u0026#34; # Replace with a real commit SHA strip_prefix = \u0026#34;rules_d-{HEAD}\u0026#34;, urls = [\u0026#34;https://github.com/bazelbuild/rules_d/archive/{HEAD}.tar.gz\u0026#34;], ) load(\u0026#34;@io_bazel_rules_d//d:d.bzl\u0026#34;, \u0026#34;d_repositories\u0026#34;) d_repositories() load( \u0026#34;@io_bazel_rules_docker//repositories:repositories.bzl\u0026#34;, container_repositories = \u0026#34;repositories\u0026#34;, ) container_repositories() load( \u0026#34;@io_bazel_rules_docker//d:image.bzl\u0026#34;, _d_image_repos = \u0026#34;repositories\u0026#34;, ) _d_image_repos() Note: See note about diamond dependencies in setupif you run into issues related to external repos after adding these lines to your WORKSPACE.\nThen in your BUILD file, simply rewrite d_binary to d_image with the following import:\nload(\u0026#34;@io_bazel_rules_docker//d:image.bzl\u0026#34;, \u0026#34;d_image\u0026#34;) d_image( name = \u0026#34;d_image\u0026#34;, srcs = [\u0026#34;main.d\u0026#34;], ) If you need to modify somehow the container produced by d_image (e.g., env, symlink), see note above in Language Rules Overviewabout how to do this and see go_image (custom base)example.\n NOTE: all application image rules support the args string_list attribute. If specified, they will be appended directly after the container ENTRYPOINT binary name.\n container_bundle container_bundle( name = \u0026#34;bundle\u0026#34;, images = { # A set of images to bundle up into a single tarball. \u0026#34;gcr.io/foo/bar:bazz\u0026#34;: \u0026#34;:app\u0026#34;, \u0026#34;gcr.io/foo/bar:blah\u0026#34;: \u0026#34;//my:sidecar\u0026#34;, \u0026#34;gcr.io/foo/bar:booo\u0026#34;: \u0026#34;@your//random:image\u0026#34;, } ) container_pull In WORKSPACE:\ncontainer_pull( name = \u0026#34;base\u0026#34;, registry = \u0026#34;gcr.io\u0026#34;, repository = \u0026#34;my-project/my-base\u0026#34;, # \u0026#39;tag\u0026#39; is also supported, but digest is encouraged for reproducibility. digest = \u0026#34;sha256:deadbeef\u0026#34;, ) This can then be referenced in BUILD files as @base//image.\nTo get the correct digest one can run docker manifest inspect gcr.io/my-project/my-base:tag once experimental docker cli features are enabled.\nSee here for an example of how to use container_pull with custom docker authentication credentials.\ncontainer_push This target pushes on bazel run :push_foo:\ncontainer_push( name = \u0026#34;push_foo\u0026#34;, image = \u0026#34;:foo\u0026#34;, format = \u0026#34;Docker\u0026#34;, registry = \u0026#34;gcr.io\u0026#34;, repository = \u0026#34;my-project/my-image\u0026#34;, tag = \u0026#34;dev\u0026#34;, ) We also support the docker_push (from docker/docker.bzl) and oci_push (from oci/oci.bzl) aliases, which bake in the format = \u0026quot;...\u0026quot; attribute.\nSee here for an example of how to use container_push with custom docker authentication credentials.\ncontainer_push (Custom client configuration) If you wish to use container_push using custom docker authentication credentials, in WORKSPACE:\n# Download the rules_docker repository http_archive( name = \u0026#34;io_bazel_rules_docker\u0026#34;, ... ) # Load the macro that allows you to customize the docker toolchain configuration. load(\u0026#34;@io_bazel_rules_docker//toolchains/docker:toolchain.bzl\u0026#34;, docker_toolchain_configure=\u0026#34;toolchain_configure\u0026#34; ) docker_toolchain_configure( name = \u0026#34;docker_config\u0026#34;, # Replace this with an absolute path to a directory which has a custom docker # client config.json. Note relative paths are not supported. # Docker allows you to specify custom authentication credentials # in the client configuration JSON file. # See https://docs.docker.com/engine/reference/commandline/cli/#configuration-files # for more details. client_config=\u0026#34;/path/to/docker/client/config-dir\u0026#34;, ) In BUILD file:\nload(\u0026#34;@io_bazel_rules_docker//container:container.bzl\u0026#34;, \u0026#34;container_push\u0026#34;) container_push( name = \u0026#34;push_foo\u0026#34;, image = \u0026#34;:foo\u0026#34;, format = \u0026#34;Docker\u0026#34;, registry = \u0026#34;gcr.io\u0026#34;, repository = \u0026#34;my-project/my-image\u0026#34;, tag = \u0026#34;dev\u0026#34;, ) container_pull (DockerHub) In WORKSPACE:\ncontainer_pull( name = \u0026#34;official_ubuntu\u0026#34;, registry = \u0026#34;index.docker.io\u0026#34;, repository = \u0026#34;library/ubuntu\u0026#34;, tag = \u0026#34;14.04\u0026#34;, ) This can then be referenced in BUILD files as @official_ubuntu//image.\ncontainer_pull (Quay.io) In WORKSPACE:\ncontainer_pull( name = \u0026#34;etcd\u0026#34;, registry = \u0026#34;quay.io\u0026#34;, repository = \u0026#34;coreos/etcd\u0026#34;, tag = \u0026#34;latest\u0026#34;, ) This can then be referenced in BUILD files as @etcd//image.\ncontainer_pull (Bintray.io) In WORKSPACE:\ncontainer_pull( name = \u0026#34;artifactory\u0026#34;, registry = \u0026#34;docker.bintray.io\u0026#34;, repository = \u0026#34;jfrog/artifactory-pro\u0026#34;, ) This can then be referenced in BUILD files as @artifactory//image.\ncontainer_pull (Gitlab) In WORKSPACE:\ncontainer_pull( name = \u0026#34;gitlab\u0026#34;, registry = \u0026#34;registry.gitlab.com\u0026#34;, repository = \u0026#34;username/project/image\u0026#34;, tag = \u0026#34;tag\u0026#34;, ) This can then be referenced in BUILD files as @gitlab//image.\ncontainer_pull (Custom client configuration) If you specified a docker client directory using the client_config attribute to the docker toolchain configuration described here, you can use a container_pull that uses the authentication credentials from the specified docker client directory as follows:\nIn WORKSPACE:\nload(\u0026#34;@io_bazel_rules_docker//toolchains/docker:toolchain.bzl\u0026#34;, docker_toolchain_configure=\u0026#34;toolchain_configure\u0026#34; ) # Configure the docker toolchain. docker_toolchain_configure( name = \u0026#34;docker_config\u0026#34;, # Path to the directory which has a custom docker client config.json with # authentication credentials for registry.gitlab.com (used in this example). client_config=\u0026#34;/path/to/docker/client/config\u0026#34;, ) # Load the custom version of container_pull created by the docker toolchain # configuration. load(\u0026#34;@docker_config//:pull.bzl\u0026#34;, authenticated_container_pull=\u0026#34;container_pull\u0026#34;) authenticated_container_pull( name = \u0026#34;gitlab\u0026#34;, registry = \u0026#34;registry.gitlab.com\u0026#34;, repository = \u0026#34;username/project/image\u0026#34;, tag = \u0026#34;tag\u0026#34;, ) This can then be referenced in BUILD files as @gitlab//image.\nNOTE: This should only be used if a custom client_config was set. If you want to use the DOCKER_CONFIG env variable or the default home directory use the standard container_pull rule.\nNOTE: This will only work on systems with Python \u0026gt;2.7.6\nPython tools Starting with Bazel 0.25.0 it\u0026rsquo;s possible to configure python toolchains for rules_docker.\nTo use these features you need to enable the flags in the .bazelrc file at the root of this project.\nUse of these features require a python toolchain to be registered. //py_images/image.bzl:deps and //py3_images/image.bzl:deps register a default python toolchain (//toolchains:container_py_toolchain) that defines the path to python tools inside the default container used for these rules.\nKnown issues If you are using a custom base for py_image or py3_image builds that has python tools installed in a different location to those defined in //toolchains:container_py_toolchain, you will need to create a toolchain that points to these paths and register it before the call to py*_images/image.bzl:deps in your WORKSPACE.\nUse of python toolchain features, currently, only supports picking one version of python for execution of host tools. rules_docker heavily depends on execution of python host tools that are only compatible with python 2. Flags in the recommended .bazelrc file force all host tools to use python 2. If your project requires using host tools that are only compatible with python 3 you will not be able to use these features at the moment. We expect this issue to be resolved before use of python toolchain features becomes the default.\nUpdating the distroless base images. The digest references to the distroless base images must be updated over time to pick up bug fixes and security patches. To facilitate this, the files containing the digest references are generated by tools/update_deps.py. To update all of the dependencies, please run (from the root of the repository):\n./update_deps.sh Image references should not be updated individually because these images have shared layers and letting them diverge could result in sub-optimal push and pull performance.\ncontainer_pull MOVED: See docs/container.md\ncontainer_push MOVED: See docs/container.md\ncontainer_layer MOVED: See docs/container.md\ncontainer_image MOVED: See docs/container.md\ncontainer_bundle MOVED: See docs/container.md\ncontainer_import MOVED: See docs/container.md\ncontainer_load MOVED: See docs/container.md\nAdopters Here\u0026rsquo;s a (non-exhaustive) list of companies that use rules_docker in production. Don\u0026rsquo;t see yours? You can add it in a PR!\n Amaiz Aura Devices Button Domino Data Lab Canva Etsy Evertz Jetstack Kubernetes Container Image Promoter Nordstrom Prow Tink Wix  ","id":26,"section":"posts","summary":"Bazel Container Image Rules    Bazel CI         Generated API documentation is in the docs folder, or you can browse it online at https://docs.aspect.dev/rules_docker\nBasic Rules  container_image (example) container_bundle (example) container_import container_load container_pull (example) container_push (example)  These rules used to be docker_build, docker_push, etc. and the aliases for these (mostly) legacy names still exist largely for backwards-compatibility. We also have early-stage oci_image, oci_push, etc.","tags":["org:jrbeverly"],"title":"rules_docker","uri":"/2022/01/jrbeverly-rules_docker/","year":"2022"},{"content":"EntityModel Dapper Experiment Case Exported case of experimenting with using Postgres Functions, Dapper \u0026amp; Entity.Model.\nNotes Experimenting a bit more with the entity model of splitting out the \u0026ldquo;database\u0026rdquo; components (e.g. ID) from the model objects, with an aim towards designing a strict interface for working with the database, that could be generated from a baseline specification. This would allow for things like ReadOnly entities, better field filtering \u0026amp; potential for \u0026ldquo;programmatic\u0026rdquo; improvements to how the database handles searches.\nContinuing with the direction of leveraging functions/stored procedures for databases. This simplifies the internals of the API, letting the clients be only consumers, and having external sources be responsible for the provisioning of the database schema.\nPotential areas of interest:\n Constructing a \u0026ldquo;Database\u0026rdquo; type schema bundle, that can be provisioned on-demand by services (allows DB provisioning to be external \u0026ldquo;infrastructure\u0026rdquo;) \u0026ldquo;Models\u0026rdquo; can generated the DB \u0026amp; interface components for services, as well as CLIs for easy interaction Schema bundles can be combined for \u0026ldquo;migration\u0026rdquo; updates \u0026amp; verification policies (tests / \u0026lsquo;DB Policy\u0026rsquo; - similar to a \u0026ldquo;IAM Policy\u0026rdquo;) Common classes for interacting with the database, with tools to construct the CommandDefinitions to send messages to database How to handle the idea of \u0026ldquo;chaining\u0026rdquo;, e.g. X =\u0026gt; Data, then use that data to get Y.  Options for tools that work well in devcontainers for performing postman-like queries against a server for rapid development.\n","id":27,"section":"posts","summary":"EntityModel Dapper Experiment Case Exported case of experimenting with using Postgres Functions, Dapper \u0026amp; Entity.Model.\nNotes Experimenting a bit more with the entity model of splitting out the \u0026ldquo;database\u0026rdquo; components (e.g. ID) from the model objects, with an aim towards designing a strict interface for working with the database, that could be generated from a baseline specification. This would allow for things like ReadOnly entities, better field filtering \u0026amp; potential for \u0026ldquo;programmatic\u0026rdquo; improvements to how the database handles searches.","tags":["org:jrbeverly"],"title":"dapper-with-entity-model","uri":"/2021/10/jrbeverly-dapper-with-entity-model/","year":"2021"},{"content":"Cuelang with SchemaGen Experimenting with using Cuelang for the purposes of representing a schema, then generating associated files from the original source of truth\nNotes  Schema validation is nice, the base case is straightforward Combining data with this allows for connecting enum/datasets What about stubbing of datasets (e.g. restrict this to \u0026lsquo;Dataset\u0026rsquo; that isn\u0026rsquo;t locally defined?) Text templating isn\u0026rsquo;t really what I want to do with this kind of tool Seems like I need to write more \u0026ldquo;representation/composition\u0026rdquo; than I originally hoped Doesn\u0026rsquo;t seem to support the kind of \u0026ldquo;intentions\u0026rdquo; workflow I was hoping for  This isn\u0026rsquo;t handling the case I\u0026rsquo;m interested in with schema validation \u0026amp; generation. Although it provides a lot of the base essentials out of the box, I\u0026rsquo;m looking more for a cross between schema+terraform. A system that allows the tracking of version drift, and reconcilation as part of its design.\nClarifying my exact intended use case is probably a better direction than fiddlign with this more.\n","id":28,"section":"posts","summary":"Cuelang with SchemaGen Experimenting with using Cuelang for the purposes of representing a schema, then generating associated files from the original source of truth\nNotes  Schema validation is nice, the base case is straightforward Combining data with this allows for connecting enum/datasets What about stubbing of datasets (e.g. restrict this to \u0026lsquo;Dataset\u0026rsquo; that isn\u0026rsquo;t locally defined?) Text templating isn\u0026rsquo;t really what I want to do with this kind of tool Seems like I need to write more \u0026ldquo;representation/composition\u0026rdquo; than I originally hoped Doesn\u0026rsquo;t seem to support the kind of \u0026ldquo;intentions\u0026rdquo; workflow I was hoping for  This isn\u0026rsquo;t handling the case I\u0026rsquo;m interested in with schema validation \u0026amp; generation.","tags":["org:jrbeverly"],"title":"cue-for-schema-gen","uri":"/2021/10/jrbeverly-cue-for-schema-gen/","year":"2021"},{"content":"Bazel Bash Packaged Experimenting with using Bazel \u0026amp; Bats in container images for writing up tests for shell scripts.\nNotes Sometimes while developing things it can be useful to have a small shell script that performs a simple action or operation, that in the short-term makes sense as a shell script. In this case, it can be useful to add some simple tests to ensure that:\n The script is runnable Simple transformations work as expected Any environment variable or file references work as intended  This isn\u0026rsquo;t intended for cases where a shell script is increasing in complexity, but rather where it acts as \u0026lsquo;Glue\u0026rsquo; responsible for filling in gaps that might exist in a system.\nSome minor notes about this process are included below:\n Using docker allows avoiding requiring the expected toolchains to be installed locally (or defined in bazel) The Bats image is used instead of installed bats onto an image. Installing bats onto an existing image might be a better option Container tests can check other properties of the executable shell scripts (as it should be packed the same as it would be for other targets)  ","id":29,"section":"posts","summary":"Bazel Bash Packaged Experimenting with using Bazel \u0026amp; Bats in container images for writing up tests for shell scripts.\nNotes Sometimes while developing things it can be useful to have a small shell script that performs a simple action or operation, that in the short-term makes sense as a shell script. In this case, it can be useful to add some simple tests to ensure that:\n The script is runnable Simple transformations work as expected Any environment variable or file references work as intended  This isn\u0026rsquo;t intended for cases where a shell script is increasing in complexity, but rather where it acts as \u0026lsquo;Glue\u0026rsquo; responsible for filling in gaps that might exist in a system.","tags":["org:jrbeverly"],"title":"bazel-bash-packaged","uri":"/2021/10/jrbeverly-bazel-bash-packaged/","year":"2021"},{"content":"JCompiler Summary A Joos programming language compiler, written in Java.\nGetting Started The project is currently not maintained or kept in runnable order. You may be able to open the project in Eclipse, but at this time the code is only here as readonly.\nNotes The application was written as part of the UWaterloo CS444 Compiler Course.\nGroup Members:\n seanmk sxyuan  Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Stephen Copinger from the Noun Project.\n","id":30,"section":"posts","summary":"JCompiler Summary A Joos programming language compiler, written in Java.\nGetting Started The project is currently not maintained or kept in runnable order. You may be able to open the project in Eclipse, but at this time the code is only here as readonly.\nNotes The application was written as part of the UWaterloo CS444 Compiler Course.\nGroup Members:\n seanmk sxyuan  Acknowledgements The project icon is retrieved from the Noun Project.","tags":["org:jrbeverly"],"title":"jcompiler","uri":"/2021/10/jrbeverly-jcompiler/","year":"2021"},{"content":"bazel-toolchain-from-s3 Experimenting with setting up Bazek toolchains, when the tools are mirrored into an AWS S3 bucket.\nThis builds off previous work done in jrbeverly/bazel-external-toolchain-rule for creating toolchains from files.\nNotes  Implementation of s3_archive uses that same model as http_archive The repository_rule does not use toolchains like rules, meaning bootstrap rules must be downloaded by repository_ctx Repository rules can convert labels to paths using the repository_ctx.path method  To get a fully managed system that doesn\u0026rsquo;t require tools to be installed on the local system, would likely require a bootstrap rule using a tool that is publicly available. This way it could be downloaded using the download_and_extract rule of Bazel, then combined with the other rules for perform the act of downloading binaries. The process as follows:\n Download \u0026amp; configure the cloudio tool for downloading binaries from a cloud source (AWS S3/Azure RM/GCP/etc) Make this tool available to the other repository rules (aspect? label? ?) Repository rule uses this binary to run the download commands  This may have an additional benefit in that the cloudio tool could behave in a content-addressable manner, removing the need for URLs (primary/mirrors/etc) for tools. Instead leaving it up to the cloudio to download the toolchain from the appropriately known registries.\nDesign Jots  Using something like an awsrc file (aim to be similar to netrc) to determine which AWS Profile authentication to use for buckets (is this worthwhile? - Maybe - Our default AWS_PROFILE may not always have artifact access - think isolated AWS accounts like \u0026lsquo;malware\u0026rsquo;) Requires a sha256sum checker, which is only really available on linux (MacOS can install, but default is shasum ), and Windows is PowerShell. AWS CLI is working on parts of the environment like AWS_PROFILE (which may run tools through credential_process)  ","id":31,"section":"posts","summary":"bazel-toolchain-from-s3 Experimenting with setting up Bazek toolchains, when the tools are mirrored into an AWS S3 bucket.\nThis builds off previous work done in jrbeverly/bazel-external-toolchain-rule for creating toolchains from files.\nNotes  Implementation of s3_archive uses that same model as http_archive The repository_rule does not use toolchains like rules, meaning bootstrap rules must be downloaded by repository_ctx Repository rules can convert labels to paths using the repository_ctx.path method  To get a fully managed system that doesn\u0026rsquo;t require tools to be installed on the local system, would likely require a bootstrap rule using a tool that is publicly available.","tags":["org:jrbeverly"],"title":"bazel-toolchain-from-s3","uri":"/2021/09/jrbeverly-bazel-toolchain-from-s3/","year":"2021"},{"content":"bazel-external-toolchain-rules Experimenting with setting up Bazel toolchains using an externally managed .toolchain file, that is responsible for defining properties such as:\n System compatibility Integrity Checks Tool retrieval locations  Notes This idea came out of the idea of having Bazel rules that were not aware of how toolchains were defined (or what systems they are compatible with), and instead being entirely based on the lock file (.toolchain) available in the caller environment. This means that the bazel rules can in essence just be extensions of the command line specification.\nRepositories would have a collection of .toolchain files, that could be used by other services (e.g. pre-baked GitPod/Codespace/DevContainers, local installs, etc). When used by Bazel, these files would be read by Bazel and converted into the appropriate rules for downloading \u0026amp; setting up the toolchain.\nThe basic idea scratched out:\nload(\u0026#34;//bazel/macros:load_all.bzl\u0026#34;, \u0026#34;register_external_toolchains\u0026#34;) register_external_toolchains( name = \u0026#34;external_toolchains\u0026#34;, toolchains = { \u0026#34;//bazel/toolchains:helm.toolchain\u0026#34;: \u0026#34;bazel_toolchain_helm\u0026#34;, \u0026#34;//bazel/toolchains:yq.toolchain\u0026#34;: \u0026#34;bazel_toolchain_yq\u0026#34;, }, ) Local or imported rules can be specified in the string field (bazel_toolchain_helm), making it simple to map the toolchain definition to the toolchain use in Bazel.\nName=yq Version=4.13.2 [linux_amd64] OS=linux CPU=x86_64 Executable=yq_linux_amd64 URL=https://github.com/mikefarah/yq/releases/download/v4.13.2/yq_linux_amd64.tar.gz Sha256Sum=b462478cfee8fb02b1b6bbee87b2b1d2f0ef4f0b693a95c04308006f04cc525e [darwin_amd64] OS=osx CPU=x86_64 Executable=yq_darwin_amd64 URL=https://github.com/mikefarah/yq/releases/download/v4.13.2/yq_darwin_amd64.tar.gz Sha256Sum=cf0cbf49a423d515d69879c08af4bab64af09f29b949545aa8e3d771a94a3db7 [windows_amd64] OS=windows CPU=x86_64 Executable=yq_windows_amd64.exe URL=https://github.com/mikefarah/yq/releases/download/v4.13.2/yq_windows_amd64.zip Sha256Sum=0b08de85f4de4b55e98fcd69c707bea52f91478603cc4221c024ddd80cfd6141 Custom Tool Stores These can then be combined with custom storages for tools, rather than just relying on the http_archive. An example would be an s3_archive rule that can retrieve these tools from an AWS S3 Bucket responsible for storing these binaries.\nThe aim of this would be to enable the above rules (register_external_toolchain(s)) to support tools or systems that augment the processing of the read toolchain file. If done right, it would allow something like mirroring all toolchain dependencies to an internal store with minimal manual intervention:\n Run a command to download all the toolchains to a custom local directory (toolchains download --directory \u0026lt;xyz\u0026gt;) Upload this directory to a minio or hosted file store (aws s3 sync --recursive \u0026lt;xyz\u0026gt;/ s3://...) Map the toolchains to the now vendored path (toolchains vendor s3 s3://...) In bazel, modify the register_external_toolchains to add an adaptor/affix/aspect/etc that supports downloading from S3.  The above is the scenario this idea was explored, but the hope would be letting the .toolchain (\u0026amp; potentially supporting files) act as the source of truth about toolchains, and letting Bazel interpret it.\nDesign Jots  Instead of URLs for downloading the toolchains, what if instead they were content-addressable? Rather than using bazel to perform the analysis, would it be possible to have a tool generate the entire compatibility-layer? Thus allowing us to only define the necessary \u0026ldquo;version\u0026rdquo; components in a bazel compatible way? Would a system like gazelle make more sense? Having a tool read the .toolchain file, then generate all the repository_rule bindings (\u0026amp; use netrc instead of custom rules?)  ","id":32,"section":"posts","summary":"bazel-external-toolchain-rules Experimenting with setting up Bazel toolchains using an externally managed .toolchain file, that is responsible for defining properties such as:\n System compatibility Integrity Checks Tool retrieval locations  Notes This idea came out of the idea of having Bazel rules that were not aware of how toolchains were defined (or what systems they are compatible with), and instead being entirely based on the lock file (.toolchain) available in the caller environment.","tags":["org:jrbeverly"],"title":"bazel-external-toolchain-rules","uri":"/2021/09/jrbeverly-bazel-external-toolchain-rules/","year":"2021"},{"content":"Bazel \u0026amp; Jsonnet Templates Generating files from base configuration files using Jsonnet.\nNotes Exploring the idea of leveraging jsonnet with Bazel to create a series of templates sourced from configuration files. The basic principle of this is how to built in-repository the idea of   Configuration Files { template([inputs]) =\u0026gt; rendered }.\nThe usage of libsonnet lets the lib/ directory contain all of the models, utilities and other means of representing the data structures. Having a structure form to the data is in comparison to other templating systems like gomplate that typically just work off the idea of a text template. Having the structure means that we can leverage transforms to manipulate the data is a logical way, then render it as the expected output text.\nSupport for \u0026ldquo;aspect\u0026rdquo; style transformations of the data is fairly useful as well, as it gives the opportunity to apply things like tags to every resource.\nAlthough this makes the generation of templated data better, it feels like just a more involved templating process, that as the system becomes sufficiently complex, it will eventually be seen as better to transition to a language/DSL better suited to the specialzied task (e.g. terraform, skylark, hcl)\n","id":33,"section":"posts","summary":"Bazel \u0026amp; Jsonnet Templates Generating files from base configuration files using Jsonnet.\nNotes Exploring the idea of leveraging jsonnet with Bazel to create a series of templates sourced from configuration files. The basic principle of this is how to built in-repository the idea of   Configuration Files { template([inputs]) =\u0026gt; rendered }.\nThe usage of libsonnet lets the lib/ directory contain all of the models, utilities and other means of representing the data structures.","tags":["org:jrbeverly"],"title":"bazel-jsonnett-templates","uri":"/2021/09/jrbeverly-bazel-jsonnett-templates/","year":"2021"},{"content":"internal-reserved-license-repo Experimenting with laying out the licensing stamp for a closed/internal source repository\nNotes Experimenting with the idea of what license annotations would look like on an internal repository that is not intended for public distribution. This can seem odd, as the source code files of the project should not be distributed, so the only individuals viewing the license should be those with pre-approved access (i.e. contributors/employees).\nThe intention of this is to explore ideas around having tools \u0026amp; systems be aware of the licensing \u0026ldquo;intentions\u0026rdquo; of the source code. By that I mean, if an automated tool attempted to set a GitHub Repository to \u0026lsquo;Public\u0026rsquo; or migrate it out to the organizations OSS/Public GitHub Organization, the license.spdx file would be noted as UNLICENSED (or the equivalent term) and refuse the action on the grounds that is not permit. Or just not show it as meeting the minimum requirement to be flagged as \u0026lsquo;Open Source\u0026rsquo;.\nI don\u0026rsquo;t think designing everything to have consider what a license.spdx file would be the direction, but instead curious how an \u0026lsquo;affix\u0026rsquo; or \u0026lsquo;aspect\u0026rsquo; oriented style could work with this. This requires further investigation about intentions around annotating resources with external non-technical considerations like licensing/distribution.\n","id":34,"section":"posts","summary":"internal-reserved-license-repo Experimenting with laying out the licensing stamp for a closed/internal source repository\nNotes Experimenting with the idea of what license annotations would look like on an internal repository that is not intended for public distribution. This can seem odd, as the source code files of the project should not be distributed, so the only individuals viewing the license should be those with pre-approved access (i.e. contributors/employees).\nThe intention of this is to explore ideas around having tools \u0026amp; systems be aware of the licensing \u0026ldquo;intentions\u0026rdquo; of the source code.","tags":["org:jrbeverly"],"title":"internal-reserved-license-repo","uri":"/2021/09/jrbeverly-internal-reserved-license-repo/","year":"2021"},{"content":"github-actions-dbx-upload Publishing to Dropbox programmatically from GitHub Actions with the intentions to mirror the model of AWS S3 publishing.\nNotes Exploring how one might leverage Dropbox as an artifact storage source, with an authentication model that is similar to AWS. Setting up the initial Dropbox Application to get the tokens for publishing was a bit more involved than useful, and I suspect the process of rotating these tokens might be a pain too.\nWhen the system is setting up and going, it works fairly well. The user interface \u0026amp; sharing features of Dropbox fit well with an artifact storage system. The fixed billing plans (\u0026amp; reasonable free limit) make it an appealing option for use in comparison to an IaaS like AWS/GCloud/Azure.\nThe authentication \u0026amp; publishing process for this is similar to other options (like AWS S3), it would be possible to use it in a way that creates a common interface that allows easy switching between the two as artifact publishing sources like so:\n- name: Configure Dropbox Credentials uses: ./.github/actions/configure-dropbox-credentials with: dropbox-access-key-id: ${{ secrets.DROPBOX_ACCESS_KEY_ID }} dropbox-secret-access-key: ${{ secrets.DROPBOX_SECRET_ACCESS_KEY }} dropbox-session-token: ${{ secrets.DROPBOX_SESSION_TOKEN }}  - name: Upload to artifacts uses: ./.github/actions/upload-artifacts-to-dropbox with: path: packages/ folder: artifacts/blockycraft/${GITHUB_SHA} vs\n- name: Configure AWS Credentials uses: aws-actions/configure-aws-credentials@v1 with: aws-access-key-id: ${{ secrets.DROPBOX_ACCESS_KEY_ID }} aws-access-key-id: ${{ secrets.DROPBOX_SECRET_ACCESS_KEY }} aws-session-token: ${{ secrets.DROPBOX_SESSION_TOKEN }}  - name: Upload to artifacts uses: aws-actions/upload-artifacts-to-s3 # Doesn\u0026#39;t exist, but using as stub with: path: packages/ folder: artifacts/blockycraft/${GITHUB_SHA} ","id":35,"section":"posts","summary":"github-actions-dbx-upload Publishing to Dropbox programmatically from GitHub Actions with the intentions to mirror the model of AWS S3 publishing.\nNotes Exploring how one might leverage Dropbox as an artifact storage source, with an authentication model that is similar to AWS. Setting up the initial Dropbox Application to get the tokens for publishing was a bit more involved than useful, and I suspect the process of rotating these tokens might be a pain too.","tags":["org:jrbeverly"],"title":"github-actions-dbx-upload","uri":"/2021/08/jrbeverly-github-actions-dbx-upload/","year":"2021"},{"content":"Terraform AWS CodePipline Terraform Executor Terraform executor leveraging the CodePipeline functionality in AWS, for a fully serverless model of executing terraform in AWS.\nGetting Started The primary environment is configured in env/, with a single main file. This provisions the CodePipeline, CodeBuild components, as well as some stub S3 buckets that represent both incoming sources, artifacts. The artifacts bucket is treated as a stand-in for the Terraform state environment variable, but the pipeline itself does not attempt to configure the backend for S3.\nNotes This likely is only really suited to simple pipelines, as it seems clunky to work with. Although the AWS built-in option can be beneficial, this doesn\u0026rsquo;t really make Terraform an appealing option, and instead makes more sense to leverage something like CloudFormation. Cases where this might need more strict access controls, or the ability to have appropriate guardrails seems like it wouldn\u0026rsquo;t work out long term.\nNotes:\n Simple triggers work off the arrival of notifications in Cloudwatch for S3 publishes Can have a single file act as the sort of \u0026ldquo;GitOps\u0026rdquo; authoritative state for the infrastructure Multi-component terraform deploys (e.g. terragrunt) are possilbe, but feel like they wouldn\u0026rsquo;t fit well with the system Terraform for configuring the build pipeline doesn\u0026rsquo;t feel ideal, as opposed to a more \u0026ldquo;configuration\u0026rdquo; based management option like GitHub Actions / Gitlab CI S3 Storage (\u0026amp; dynamodb locking table) should be provisioned externally from the primary provision terraform  Not really sure about this one, as I feel CloudFormation is a better option for that \u0026ldquo;AWS built-in\u0026rdquo; approach, or using an external service responsible for the terraform executor.\n","id":36,"section":"posts","summary":"Terraform AWS CodePipline Terraform Executor Terraform executor leveraging the CodePipeline functionality in AWS, for a fully serverless model of executing terraform in AWS.\nGetting Started The primary environment is configured in env/, with a single main file. This provisions the CodePipeline, CodeBuild components, as well as some stub S3 buckets that represent both incoming sources, artifacts. The artifacts bucket is treated as a stand-in for the Terraform state environment variable, but the pipeline itself does not attempt to configure the backend for S3.","tags":["org:jrbeverly"],"title":"terraform-aws-codepipeline-terraform-runner","uri":"/2021/08/jrbeverly-terraform-aws-codepipeline-terraform-runner/","year":"2021"},{"content":"Bazel Golang Inline Analyzer Experimenting with having analyzers locally defined to a repository, rather than externally defined.\nNotes  Requires using go_tool_library instead of go_library due to a dependency change issue (must also use go_tool_library of deps) Baked natively into nogo, so it can be pretty straightforward to test Names of types aren\u0026rsquo;t as simple as package.Type, but instead include other components (using HasSuffix) (What options are there?) Change in \u0026lsquo;internal/cobrago/storage.go@ListFilesInStorage\u0026rsquo; can be removed as a test case for the errors The tools/ directory probably isn\u0026rsquo;t the best path. Want something that we can spin-out/externalize as these evolve with the code  Overall this is a pretty good way to start prototyping mechanisms for code analysis that is right next to the code, which can then later be spun out into their own generic analyzers. As it runs automatically with nogo, there is a natural way of enabling an analyzer in a tiered manner (warning =\u0026gt; error).\nWhat would be necessary to get these setup in as lightweight as possible way to ensure very simple constraints? E.g. Don't use 'XYZ' type while in 'ABC' module. The main goal of having these would be essentially creating \u0026lsquo;Tests\u0026rsquo; for the code to ensure that code is being built in a manner idiomatic to the codebase.\n","id":37,"section":"posts","summary":"Bazel Golang Inline Analyzer Experimenting with having analyzers locally defined to a repository, rather than externally defined.\nNotes  Requires using go_tool_library instead of go_library due to a dependency change issue (must also use go_tool_library of deps) Baked natively into nogo, so it can be pretty straightforward to test Names of types aren\u0026rsquo;t as simple as package.Type, but instead include other components (using HasSuffix) (What options are there?) Change in \u0026lsquo;internal/cobrago/storage.","tags":["org:jrbeverly"],"title":"golang-analyzer-inline-bazel","uri":"/2021/08/jrbeverly-golang-analyzer-inline-bazel/","year":"2021"},{"content":"GitHub App in Golang Prototyping GitHub App written in Golang with the AWS \u0026amp; GitHub integrations split away, to try and encode the core \u0026lsquo;concepts\u0026rsquo; solely into the lib/ component\nNotes  Development (\u0026amp; Testing) should support a non-smee way of development How would integration/infrastructure/e2e tests work with this kind of system? Would it make more sense to have an OpenAPI system, with the GitHub integration performing the interface? Is this similar to Hubot, in that having an interface layer/service makes more sense, as it avoids the requirement for direct interface? (This can be within the system itself)  ","id":38,"section":"posts","summary":"GitHub App in Golang Prototyping GitHub App written in Golang with the AWS \u0026amp; GitHub integrations split away, to try and encode the core \u0026lsquo;concepts\u0026rsquo; solely into the lib/ component\nNotes  Development (\u0026amp; Testing) should support a non-smee way of development How would integration/infrastructure/e2e tests work with this kind of system? Would it make more sense to have an OpenAPI system, with the GitHub integration performing the interface? Is this similar to Hubot, in that having an interface layer/service makes more sense, as it avoids the requirement for direct interface?","tags":["org:jrbeverly"],"title":"github-app-golang","uri":"/2021/07/jrbeverly-github-app-golang/","year":"2021"},{"content":"K3s In HomeLab Proof of Concept Determining how viable it would to be switch from using docker-compose to using K3s to run my internal homelab environment.\nGetting Started The installation process assumes that you have a freshly imaged ubuntu machine, connected to the internal network, and with a minimum of password-based SSH access.\nInstalling dependencies The tools FluxCD \u0026amp; K3sup can be installed on the host machine using the script setup.sh (scripts/setup.bash). This can be done like so:\nbash scripts/setup.bash When this is completed, you should have K3sup and flux installed in your environment.\nSetting up SSH keys for K3sup To setup SSH keys for K3sup, you can run the script ssh-for-fresh.sh (scripts/ssh-for-fresh.sh), that will create an SSH key pair, copy it to the machine, and adjust the SSH configurations (both on host+machine) to expect SSH keys for connections.\nYou only need this for the install of k3sup, and can allow password-based login again after install.\nbash scripts/ssh-for-fresh.sh \u0026lt;ip\u0026gt; \u0026lt;user\u0026gt; Installing K3s on the machine You can perform the installation for the machine by running the script k3s-install.sh (scripts/k3s-install.sh). You can do that like so:\nbash scripts/k3s-install.sh \u0026lt;ip\u0026gt; \u0026lt;user\u0026gt; Installing FluxCD on the cluster You can perform the installation for the cluster by running the script fluxcd-install.sh (scripts/fluxcd-install.sh). You can do that like so:\nbash scripts/k3s-install.sh \u0026lt;name of cluster\u0026gt; \u0026lt;github username\u0026gt; \u0026lt;github repository name (to be created)\u0026gt; This will create the github repository with the basic fluxcd configuration setup.\nSetting up the configuration Follow the steps in the config/ directory to get your config+secrets configured in the cluster.\nCopying in the Kustomizations Copy in all of the Kustomization yaml files located in this directory under clusters/homelab/. These setup the cluster with a series of namespaces/CRD/networking/certs. These are all configured with the jrbeverly.dev domain for prototyping.\nFor the domain resolution to work, you\u0026rsquo;ll need to manually create the domains as they exist in the YAML in Cloudflare.\nNotes  GitPod installation did not work as desired Setting up services like require cross-talk in terms of filesystems would likely be high overhead Layout of directories could use some improvement (same with naming) Secrets management could be simplified, but what is the best way to handle this? GitHub private repositories are supported by leveraging FluxCD Bootstrap instead of manually setting it up Using kubectl get secrets you can find the Opaque secret created for the Deploy Key with FluxCD Bootstrap (\u0026amp; rotate/change it) Initial setup for this requires a GitHub Personal Access Token (PAT) to be created, but can be removed when Deploy Keys are configured If the deploy keys disappear from GitHub, you can retrieve them from the secret in K3s (kubectl get secrets), then upload to GitHub.  ","id":39,"section":"posts","summary":"K3s In HomeLab Proof of Concept Determining how viable it would to be switch from using docker-compose to using K3s to run my internal homelab environment.\nGetting Started The installation process assumes that you have a freshly imaged ubuntu machine, connected to the internal network, and with a minimum of password-based SSH access.\nInstalling dependencies The tools FluxCD \u0026amp; K3sup can be installed on the host machine using the script setup.","tags":["org:jrbeverly"],"title":"k3s-at-home-poc","uri":"/2021/07/jrbeverly-k3s-at-home-poc/","year":"2021"},{"content":"GitPod Golang CLI Leveraging GitPod for prototyping out a golang cli that interfaces with AWS.\nNotes  The .gitpod.yml file must exist in the root directory Dockerfile(s) for the environment can be specified in its own directory (.gitpod) Commands can be run on start-up, ensuring that the build is working as expected GitHub Permissions required to make changes to GitHub Actions workflows GitHub Permissions required for a series of commit/pull request based actions Workspaces can be provisioned/stopped/cleaned up on-demand Docker works on the provisioned nodes Extensions \u0026amp; Other components are defined by the .gitpod.yml file  In comparison, the code-server approach is to bake all the dependencies into the same image that is running code-server itself. This means that extensions/settings/etc can be baked onto the image that are common across projects. If working in Docker, it requires a bit more process to work with other docker processes (running on same docker network, matching file system paths, etc).\nGitPod lacks the Progress Web App (PWA) experience that code-server has, which is unfortunate as its very helpful to treat it as if it were its own application.\n","id":40,"section":"posts","summary":"GitPod Golang CLI Leveraging GitPod for prototyping out a golang cli that interfaces with AWS.\nNotes  The .gitpod.yml file must exist in the root directory Dockerfile(s) for the environment can be specified in its own directory (.gitpod) Commands can be run on start-up, ensuring that the build is working as expected GitHub Permissions required to make changes to GitHub Actions workflows GitHub Permissions required for a series of commit/pull request based actions Workspaces can be provisioned/stopped/cleaned up on-demand Docker works on the provisioned nodes Extensions \u0026amp; Other components are defined by the .","tags":["org:jrbeverly"],"title":"gitpod-cobra-golang","uri":"/2021/07/jrbeverly-gitpod-cobra-golang/","year":"2021"},{"content":"Rust Language Checks Experimenting with aspects of Rustlang for working with database, and immutable data structures.\nNotes  How might the Entity.Model apply to this? Where we split the id from the data struct? Immutable hashmap appears to need additional crates (cursory look) The /src \u0026amp; /lib standard directories is a nice component Using .env to reference the database URL Requires some additional components for PSQL \u0026amp; Diesel  ","id":41,"section":"posts","summary":"Rust Language Checks Experimenting with aspects of Rustlang for working with database, and immutable data structures.\nNotes  How might the Entity.Model apply to this? Where we split the id from the data struct? Immutable hashmap appears to need additional crates (cursory look) The /src \u0026amp; /lib standard directories is a nice component Using .env to reference the database URL Requires some additional components for PSQL \u0026amp; Diesel  ","tags":["org:jrbeverly"],"title":"rust-lang-checks","uri":"/2021/07/jrbeverly-rust-lang-checks/","year":"2021"},{"content":"Packer Bakery with AWS Native Creating pre-baked AMIs using Packer within AWS Native resources (Codepipeline / CodeBuild).\nNotes  Tuning minimum permissions can be a bit difficult with the CodePipeline/CodeBuild error messages Artifacts bucket should store only temporary/cache files, and should be destroyable Logs from CodePipeline \u0026amp; CodeBuild can be restricted to a specific log group Unique \u0026lsquo;key\u0026rsquo; identifier allow single-use of module within a common key-scope Events on-complete require additional resources/overhead Image is amazon pre-built, installing Packer on-the-fly  Although nice to leverage IAM solely for this, the benefits don\u0026rsquo;t really outway the issues with leveraging CodePipeline for this kind of build. Splitting this CI/artifact process is less than ideal, but granting credentials to external providers that can spin up any EC2 \u0026amp; run arbitrary scripts has its concern points.\n","id":42,"section":"posts","summary":"Packer Bakery with AWS Native Creating pre-baked AMIs using Packer within AWS Native resources (Codepipeline / CodeBuild).\nNotes  Tuning minimum permissions can be a bit difficult with the CodePipeline/CodeBuild error messages Artifacts bucket should store only temporary/cache files, and should be destroyable Logs from CodePipeline \u0026amp; CodeBuild can be restricted to a specific log group Unique \u0026lsquo;key\u0026rsquo; identifier allow single-use of module within a common key-scope Events on-complete require additional resources/overhead Image is amazon pre-built, installing Packer on-the-fly  Although nice to leverage IAM solely for this, the benefits don\u0026rsquo;t really outway the issues with leveraging CodePipeline for this kind of build.","tags":["org:jrbeverly"],"title":"packer-bake-with-aws-native","uri":"/2021/05/jrbeverly-packer-bake-with-aws-native/","year":"2021"},{"content":"bazel-and-aws-cdk Prototyping ideas with using Bazel and AWS Cloud Development Kit to create cloudformation templates\n","id":43,"section":"posts","summary":"bazel-and-aws-cdk Prototyping ideas with using Bazel and AWS Cloud Development Kit to create cloudformation templates","tags":["org:jrbeverly"],"title":"bazel-and-aws-cdk","uri":"/2021/04/jrbeverly-bazel-and-aws-cdk/","year":"2021"},{"content":"Repository Templating \u0026amp; File Automation Experimenting with a model of building a lightweight cron+bash system for performing templating\u0026amp;file modification to multiple repositories.\nIdea The basics of this repository was the idea of leveraging the GitHub CLI (gh) to automate the creation of pull requests that were intended to facilitate common chore work in repositories. This would reduce the need to handle things like setting up license files, formatting, metadata, GitHub Actions, etc.\nRather than continuously scanning all of the repositories for anything that might not be compliant with this, this instead works based off a work-item model, where an entry in created in the database (which is just a directory tasks in this repository). On a schedule items would be pulled from the work queue, executing the process until at least one \u0026ldquo;change\u0026rdquo; was made. If everything was up to spec, then all of the available tasks would be removed from the directory.\nThe task executions are staggered to avoid triggering a mass change in all repositories, as to avoid having to cleanup a mess of invalid changes / PRs that need to be declined.\nAn example of a work item would be:\n{ \u0026#34;branch\u0026#34;: \u0026#34;ensure-workflows-are-linted\u0026#34;, \u0026#34;script\u0026#34;: \u0026#34;actions/workflow-lint.sh\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;docs(build): Workflow linting process\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;Body of the pull request\u0026#34;, \u0026#34;labels\u0026#34;: \u0026#34;documentation,chore\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;jrbeverly/repository-template-file-invoke-prototype\u0026#34; } Notes  This rough model works suprisingly well for getting some \u0026lsquo;base\u0026rsquo; elements going Merge queue, or some form of automated approvals makes sense for some of these changes (as they are additive only changes) GitHub Apps (or some managed \u0026ldquo;service-cron\u0026rdquo;) would be a better approach rather this rough \u0026ldquo;bash\u0026rdquo; process Often times the templates \u0026amp; modifications that wish to be applied to repositories would be better as a \u0026lsquo;fit-for-purpose\u0026rsquo; code/service Validating the scripts against multiple repositories is a bit of a pain Using Git as a \u0026ldquo;database\u0026rdquo; for a work queue that only pulls a single digit number of times per day works suprisingly well Staggering the changes works great to avoid a sudden huge set of changes all at once This kind of system should be split out from the CI processes, as coupling it introduces some pain points with regards to the \u0026lsquo;CRONJOB\u0026rsquo; nature of this  ","id":44,"section":"posts","summary":"Repository Templating \u0026amp; File Automation Experimenting with a model of building a lightweight cron+bash system for performing templating\u0026amp;file modification to multiple repositories.\nIdea The basics of this repository was the idea of leveraging the GitHub CLI (gh) to automate the creation of pull requests that were intended to facilitate common chore work in repositories. This would reduce the need to handle things like setting up license files, formatting, metadata, GitHub Actions, etc.","tags":["org:jrbeverly"],"title":"repository-template-file-invoke-prototype","uri":"/2021/03/jrbeverly-repository-template-file-invoke-prototype/","year":"2021"},{"content":"dotfiles A set of vim, zsh, git, and configuration files.\n","id":45,"section":"posts","summary":"dotfiles A set of vim, zsh, git, and configuration files.","tags":["org:jrbeverly"],"title":"dotfiles","uri":"/2021/03/jrbeverly-dotfiles/","year":"2021"},{"content":"Machine Learning Lab A repository for aggregating my machine learning exercises, practices and learning labs. The projects included in this repository are based on the coursework for Udacity\u0026rsquo;s Deep Learning Nanodegree Foundations. These are primarily from working on the Machine Learning Nanodegree offered by Udacity.\nThe project files are built using Jupyter Book into a web-accessible form.\nGetting Started The conda environment for working with all of the Jupyter Notebooks is provided as environment.yml. This environment is used for all testing by Github Actions and can be setup by:\nThe environment can be setup by running the following:\nconda env create -f environment.yml conda activate ml-learning-lab The Jupyter book can interacted with using the following command:\njb build notebooks/ ","id":46,"section":"posts","summary":"Machine Learning Lab A repository for aggregating my machine learning exercises, practices and learning labs. The projects included in this repository are based on the coursework for Udacity\u0026rsquo;s Deep Learning Nanodegree Foundations. These are primarily from working on the Machine Learning Nanodegree offered by Udacity.\nThe project files are built using Jupyter Book into a web-accessible form.\nGetting Started The conda environment for working with all of the Jupyter Notebooks is provided as environment.","tags":["org:jrbeverly"],"title":"ml-learning-lab","uri":"/2021/03/jrbeverly-ml-learning-lab/","year":"2021"},{"content":"Codespace Prebuilt, development environment in the browser  powered by VS Code.\nThis image acts as a catch-all image for doing full-stack development in a polyglot type environment. The running container makes use of the host docker service to allow for docker builds.\n","id":47,"section":"posts","summary":"Codespace Prebuilt, development environment in the browser  powered by VS Code.\nThis image acts as a catch-all image for doing full-stack development in a polyglot type environment. The running container makes use of the host docker service to allow for docker builds.","tags":["org:jrbeverly"],"title":"codespace","uri":"/2021/03/jrbeverly-codespace/","year":"2021"},{"content":"HomeLab - Internal Tooling Ansible playbooks for configuring services running within my internal home cloud.\nGetting Started DevContainers The DevContainer environment can be started by opening the repository in VSCode and installing the \u0026lsquo;Remote - Containers\u0026rsquo; extension. When started, the prompt will build and image and configure the container.\nThe deployments to any of the environment can be triggered by running any of the helper scripts available in /opt/bin. To deploy the codelab environment, you can run codelab.\nLocal Environment The local environment can be setup by running the command:\nsource .devcontainer/local/setup.bash Deployments for the environments can then be triggered by running the scripts available local/. This requires docker on the machine to run.\nConfiguration MediaLab Images The medialab environment has the requirement of specifying all server images as images.json, as most of the projects do not have official images, and this lets the images change in a jq-friendly manner.\nThe sample of these look like so:\n{ \u0026#34;emby\u0026#34;: \u0026#34;ghcr.io/linuxserver/...\u0026#34;, \u0026#34;indexer\u0026#34;: \u0026#34;ghcr.io/linuxserver/...\u0026#34;, \u0026#34;downloader\u0026#34;: \u0026#34;ghcr.io/linuxserver/...\u0026#34;, \u0026#34;movie_manager\u0026#34;: \u0026#34;ghcr.io/linuxserver/...\u0026#34;, \u0026#34;tv_manager\u0026#34;: \u0026#34;ghcr.io/linuxserver/...\u0026#34; } DNS Secrets The secrets for use in DNS and VPN connectivity are specified as environment variables, and the environment file defining them is available in LastPass under HomeLab. These secrets are necessary for all web-enabled machines.\nThe sample of these look like so:\nexport NAMECHEAP_TOKEN=\u0026#39;xyz\u0026#39; export OPENVPN_USERNAME=\u0026#39;pXX123\u0026#39; export OPENVPN_PASSWORD=\u0026#39;abc\u0026#39; Structure All docker related outputs are dumped to the /srv directory. THis acts as the root for the docker containers running on any of the servers. The top level contains the docker-compose YAML files that define what is running on the machine. This is split into different kinds of components (nginx, server, utility) to make it easier to share concepts between machines.\nThe intended directory structure is like so:\n\u0026gt; srv/ \u0026gt; etc/ : secrets, environment variables and configuration data \u0026gt; data/ : data shared between the deployed server \u0026gt; tmp/ : temporary data (or data that can be destroyed + recreated) The code uses copy-over-ref for the YAML configuration to allow each machine to be slightly unique in its configuration if the need presents itself.\nNotes I\u0026rsquo;ve been running variations of my home network for a while now, with varying degrees of complexity. Some times running a full suite of software development tooling, or just running a couple of simple websites for hosting home media/photos. The biggest issue I\u0026rsquo;ve had is with configuration getting out of date.\nEven when tracking the docker-compose files in GitHub, they tended to get out-of-sync as the way of making changes was to scp+docker-compose up or git pull+docker-compose up. Eventually they would get in a bad state, remain in that state for a while due to time constraints, and just be a pain.\nSwitching to ansible for this, along with this design structure is intended to remove this annoying burden, by making ansible the primary way to make changes. Any changes that exist within /srv that is infrastructure related that is not managed by ansible should be blown away.\nThis hopefully will make issues related to the docker images less of a problem in the long term for running this infrastructure.\n","id":48,"section":"posts","summary":"HomeLab - Internal Tooling Ansible playbooks for configuring services running within my internal home cloud.\nGetting Started DevContainers The DevContainer environment can be started by opening the repository in VSCode and installing the \u0026lsquo;Remote - Containers\u0026rsquo; extension. When started, the prompt will build and image and configure the container.\nThe deployments to any of the environment can be triggered by running any of the helper scripts available in /opt/bin. To deploy the codelab environment, you can run codelab.","tags":["org:jrbeverly"],"title":"home","uri":"/2020/11/jrbeverly-home/","year":"2020"},{"content":"Running code-server on AWS Lightsail Summary Run VS Code on an AWS Lightsail instance with auto-generated password and static IP. Early experiments with cloud-driven development environments configured on-demand using terraform.\nInitial exploratory work for seeing what changes exist in the workflows, and any issues that may arise as a result of working in Lightsail.\nNotes Below are some quick points noted while experimenting with this:\nAWS Access Keys In comparison to running this on ECS or EC2, AWS access keys need to be generated and supplied if you wish to run AWS commands.\nThis is nice to some degree if I rely on aws sso to have temporary credentials. Each instance can then focus on being configured for aws sso (or related sso tooling) to get keys.\nPulling data from the instance would be an issue, and make cross-cutting concerns (like backups / logging) more difficult.\nBaked Images Unlike ECS or EC2, it doesn\u0026rsquo;t offer the options of pre-baking an image. That would be ideal, as it would allow for layering (base -\u0026gt; developer -\u0026gt; devops -\u0026gt; admin) that has the minimum necessary tools and recommended specs. Branching could also be an option depending on the type of workloads (perf, load, cross-browser validation).\nProvisioning with a single shell script works fine for the demo, but a fully featured environment would need something like ansible.\nPorts Terraform doesn\u0026rsquo;t support defining the ports at this time, and requires a workaround mechanism. This isn\u0026rsquo;t ideal, and makes it difficult to have a more flexible model.\nManual modification of the firewall is less than ideal, as the desired intent is to be completely automated. In this way running in an ECS/EC2 environment is superior as it allows for restricting incoming traffic to just the VPN. Ports can then be exposed in blocks without risking open internet access.\nOverall Thoughts AWS Lightsail doesn\u0026rsquo;t really scale to the desired solution. I\u0026rsquo;d like something that allows public access but can be locked behind sufficient guardrails (such as a VPN + SSO).\nWorking with terraform to provision the environment is nice, and with a gitops approach could make provisioning dev environments on demand really easy.\nDefining the environments may encounter some problems, as I\u0026rsquo;d like to avoid YAML sprawl when figuring out what should be on the instance.\n","id":49,"section":"posts","summary":"Running code-server on AWS Lightsail Summary Run VS Code on an AWS Lightsail instance with auto-generated password and static IP. Early experiments with cloud-driven development environments configured on-demand using terraform.\nInitial exploratory work for seeing what changes exist in the workflows, and any issues that may arise as a result of working in Lightsail.\nNotes Below are some quick points noted while experimenting with this:\nAWS Access Keys In comparison to running this on ECS or EC2, AWS access keys need to be generated and supplied if you wish to run AWS commands.","tags":["org:jrbeverly"],"title":"aws-lightsail-codespaces","uri":"/2020/08/jrbeverly-aws-lightsail-codespaces/","year":"2020"},{"content":"Express in Deployments A simple Express application built with the intent to test an Express server running in different environments (local, docker, lambda).\nUsage The available make commands can be listed with make help:\nUsage: make \u0026lt;target\u0026gt; help This help text. Serverless deploy Deploy the lambda with serverless remove Destroys the instructure Express local Locally run the app Docker docker Build and run in a docker container Notes Simple service with the intent to be used for some AWS work involving cloud costing, cold starts and on-demand provisioning of services. There are a couple projects that I\u0026rsquo;m looking at that don\u0026rsquo;t need a server with 100% uptime. An on-demand server would be ideal, as it significantly reduces the costs associated with the server.\nAdditionally if it is running in a FaaS infrastructure, then there isn\u0026rsquo;t a need to worry about any real infrastructure. Anything that is a bit complex can be addressed with serverless or terraform (if necessary).\nIt does raise some concerns about how to perform monitoring and balance concerns (costing / on-demand approach).\nOn Usages In terms of project layout, the Makefile works to act as a coordinater for all the components. It is less than an ideal, but I aimed to keep the structure as simple as possible. With it I can pretty easily test some use cases with docker and a server app. The split off app/ avoids the over-reliance on serverless or AWS lock-in.\n","id":50,"section":"posts","summary":"Express in Deployments A simple Express application built with the intent to test an Express server running in different environments (local, docker, lambda).\nUsage The available make commands can be listed with make help:\nUsage: make \u0026lt;target\u0026gt; help This help text. Serverless deploy Deploy the lambda with serverless remove Destroys the instructure Express local Locally run the app Docker docker Build and run in a docker container Notes Simple service with the intent to be used for some AWS work involving cloud costing, cold starts and on-demand provisioning of services.","tags":["org:jrbeverly"],"title":"aws-lambda-simple-service","uri":"/2020/08/jrbeverly-aws-lambda-simple-service/","year":"2020"},{"content":"jrbeverly Represents the infrastructure resources of \u0026lsquo;jrbeverly\u0026rsquo;, keeping track of infrastructure components, assets and other resources that are needed for components.\nDirectory Mappings The directory layout is modelled after the Linux directories (/var, /etc/, /root, /srv, /opt, etc). This is intended to handle cases as they are adopted into the standards.\n/srv Represents the website resources for any hosted domain.\n","id":51,"section":"posts","summary":"jrbeverly Represents the infrastructure resources of \u0026lsquo;jrbeverly\u0026rsquo;, keeping track of infrastructure components, assets and other resources that are needed for components.\nDirectory Mappings The directory layout is modelled after the Linux directories (/var, /etc/, /root, /srv, /opt, etc). This is intended to handle cases as they are adopted into the standards.\n/srv Represents the website resources for any hosted domain.","tags":["org:jrbeverly"],"title":"jrbeverly.web","uri":"/2020/06/jrbeverly-jrbeverly.web/","year":"2020"},{"content":"blockycraft Summary Blockycraft is a Minecraft inspired Block Engine written in Unity3D and built using GitHub Actions. The intent of this project is to better learn Unity, and discover some new use cases with GitHub Actions.\nThe project is available as binary releases for different operating systems, and includes a hosted WebGL version that can be played in supported browsers:\nblockycraft.jrbeverly.dev/play\nThere is no formal feature list or any intentions to carry the project long-term into a fully featured block engine.\nBuild Pipeline The build pipeline is done using GitHub Actions and the unity-builder actions. To avoid over-using the runners, the builds only trigger for pull requests, web deploys and releases. Using only GitHub services, it removes the need to manage deploy keys and infrastructure.\nA playable WebGL copy is available at blockycraft.jrbeverly.dev/play, and the binaries for the project are available on the releases tab.\nAcknowledgements The project assets were created by kenney.nl and available for download by others. The game assets are used under CC0 1.0 Universal.\nThe project icon uses assets by Kenney from kenney.nl/. The game assets have been combined together to produce the output.\n","id":52,"section":"posts","summary":"blockycraft Summary Blockycraft is a Minecraft inspired Block Engine written in Unity3D and built using GitHub Actions. The intent of this project is to better learn Unity, and discover some new use cases with GitHub Actions.\nThe project is available as binary releases for different operating systems, and includes a hosted WebGL version that can be played in supported browsers:\nblockycraft.jrbeverly.dev/play\nThere is no formal feature list or any intentions to carry the project long-term into a fully featured block engine.","tags":["org:blockycraft"],"title":"blockycraft","uri":"/2020/05/blockycraft-blockycraft/","year":"2020"},{"content":"Blockycraft Summary Blockycraft is a Minecraft inspired Block Engine written in Unity3D and built using GitHub Actions. The intent of this project is to better learn Unity, and discover some new use cases with GitHub Actions.\nThe project is available as binary releases for different operating systems, and includes a hosted WebGL version that can be played in supported browsers:\nblockycraft.jrbeverly.dev/play\nThere is no formal feature list or any intentions to carry the project long-term into a fully featured block engine.\nHistory The Blockycraft project was first developed for a University of Waterloo graphics course, and this codebase is available as blockycraft-classic. The first codebase went through some revisions in an attempt to address the technical debt incurred in the original development cycle for the demo. While these did make some improvements, they still left the codebase in a less than ideal state.\nWith the introduced of GitHub Actions, and a recent spark of interest in using Unity, I looked to rebuild parts of the project in Unity. The project isn\u0026rsquo;t intended to be feature complete with the original blockycraft, but instead be a enjoyable project for tinkering with unity.\n","id":53,"section":"posts","summary":"Blockycraft Summary Blockycraft is a Minecraft inspired Block Engine written in Unity3D and built using GitHub Actions. The intent of this project is to better learn Unity, and discover some new use cases with GitHub Actions.\nThe project is available as binary releases for different operating systems, and includes a hosted WebGL version that can be played in supported browsers:\nblockycraft.jrbeverly.dev/play\nThere is no formal feature list or any intentions to carry the project long-term into a fully featured block engine.","tags":["org:blockycraft"],"title":"readme","uri":"/2020/05/blockycraft-readme/","year":"2020"},{"content":"cardboardci-dockerfiles Dockerfiles for CardboardCI\u0026rsquo;s Docker images.\n","id":54,"section":"posts","summary":"cardboardci-dockerfiles Dockerfiles for CardboardCI\u0026rsquo;s Docker images.","tags":["org:cardboardci"],"title":"dockerfiles","uri":"/2020/04/cardboardci-dockerfiles/","year":"2020"},{"content":"Friending Summary Friending is an online dating, friendship, and social networking mobile application that features user-created questionnaires. Friending has two primary features: joining groups to find people similar to you or signing up for events happening in your local area. Friending is a prototype built with the Proto.io application prototyping tool.\nThe Friending prototype was built as part of a requirements specification project. The project focused on the development of a user manual around a fictional matchmaking application called Friending. The application centered around user-created questionnaires that could be used to find relationship matches. The user manual was designed with the goal of deceiving the reader into believing that the application existed. A project vision document and set of user requirements guided the development of scenarios and use cases for the application. The fully-interactive high-fidelity prototype created for the requirements specification project is provided in this repository, available as a standalone HTML page.\nManual The Friending user manual provides info and tips to help you understand the mobile application. The requirements specification project involved the creation of a user manual for the fictional mobile application Friending. The Friending prototype is the actualization of a user vision and set of requirements to construct a matchmaking application. The vision and requirements were used to develop the expected behaviour of the prototype, although not all requirements were actualized into the interactive prototype. The prototype merely needed to present a faithful representation of the original vision.\n","id":55,"section":"posts","summary":"Friending Summary Friending is an online dating, friendship, and social networking mobile application that features user-created questionnaires. Friending has two primary features: joining groups to find people similar to you or signing up for events happening in your local area. Friending is a prototype built with the Proto.io application prototyping tool.\nThe Friending prototype was built as part of a requirements specification project. The project focused on the development of a user manual around a fictional matchmaking application called Friending.","tags":["org:thefriending"],"title":"readme","uri":"/2020/04/thefriending-readme/","year":"2020"},{"content":"XPlatformer Summary XPlatformer is a simple video game reminiscent of the classic side-scrolling arcade game, using the XLib API. The point of the game is to control a character through a terrain to meet an objective. The project makes use of the XLib API (XOrg) and focus on code that was developed to accomplish tasks for the assignment task.\nAcknowledgements The project icon is retrieved from kenney.nl. The original source material has been altered for the purposes of the project. The icon is used under the terms of the CC0 1.0 Universal.\nThe project icon uses assets by Kenney from kenney.nl/.\nResources All art assets were acquired from http://opengameart.org/ in particular from http://opengameart.org/users/kenney. Majority of art assets come from a particular package known as \u0026ldquo;Platformer Art Deluxe\u0026rdquo; available at http://opengameart.org/content/platformer-art-deluxe. If you would like to know more about these art assets, look into http://open.commonly.cc/ or the \u0026ldquo;Open Bundle\u0026rdquo; [See http://www.kenney.nl/]. The art assets are available with the Creative Commons License (CC0)\n","id":56,"section":"posts","summary":"XPlatformer Summary XPlatformer is a simple video game reminiscent of the classic side-scrolling arcade game, using the XLib API. The point of the game is to control a character through a terrain to meet an objective. The project makes use of the XLib API (XOrg) and focus on code that was developed to accomplish tasks for the assignment task.\nAcknowledgements The project icon is retrieved from kenney.nl. The original source material has been altered for the purposes of the project.","tags":["org:xplatformer"],"title":"readme","uri":"/2020/04/xplatformer-readme/","year":"2020"},{"content":"Predicting Boston Housing Prices Evaluate the performance and predictive power of a model that has been trained and tested on data collected from homes in suburbs of Boston, Massachusetts. A model trained on this data that is seen as a good fit could then be used to make certain predictions about a home  in particular, its monetary value. This model would prove to be invaluable for someone like a real estate agent who could make use of such information on a daily basis.\nGetting Started You can spin up the environment using docker by running the following commands:\nbash docker.bash # inside docker container bash .build/conda.bash Alternatively you can manually install Python and the following Python libraries installed:\n NumPy Pandas matplotlib scikit-learn  You will also need to have software installed to run and execute a Jupyter Notebook\nData The modified Boston housing dataset consists of 489 data points, with each datapoint having 3 features. This dataset is a modified version of the Boston Housing dataset found on the UCI Machine Learning Repository.\nFeatures  RM: average number of rooms per dwelling LSTAT: percentage of population considered lower status PTRATIO: pupil-teacher ratio by town MEDV: median value of owner-occupied homes (Target Variable)  ","id":57,"section":"posts","summary":"Predicting Boston Housing Prices Evaluate the performance and predictive power of a model that has been trained and tested on data collected from homes in suburbs of Boston, Massachusetts. A model trained on this data that is seen as a good fit could then be used to make certain predictions about a home  in particular, its monetary value. This model would prove to be invaluable for someone like a real estate agent who could make use of such information on a daily basis.","tags":["org:jrbeverly"],"title":"boston-housing","uri":"/2020/02/jrbeverly-boston-housing/","year":"2020"},{"content":"Finding Donors for CharityML Employ several supervised algorithms to accurately model individuals' income using data collected from the 1994 U.S. Census. From the best candidate algorithm from preliminary results and further optimize this algorithm to best model the data. Construct a model that accurately predicts whether an individual makes more than $50,000. This sort of task can arise in a non-profit setting, where organizations survive on donations. Understanding an individual\u0026rsquo;s income can help a non-profit better understand how large of a donation to request, or whether or not they should reach out to begin with. While it can be difficult to determine an individual\u0026rsquo;s general income bracket directly from public sources, we can (as we will see) infer this value from other publically available features.\nGetting Started You can spin up the environment using docker by running the following commands:\nbash docker.bash # inside docker container bash .build/conda.bash Alternatively you can manually install Python and the following Python libraries installed:\n NumPy Pandas matplotlib scikit-learn  You will also need to have software installed to run and execute a Jupyter Notebook\nData The modified census dataset consists of approximately 32,000 data points, with each datapoint having 13 features. This dataset is a modified version of the dataset published in the paper \u0026ldquo;Scaling Up the Accuracy of Naive-Bayes Classifiers: a Decision-Tree Hybrid\u0026rdquo;, by Ron Kohavi. You may find this paper online, with the original dataset hosted on UCI.\nFeatures  age: Age workclass: Working Class (Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked) education_level: Level of Education (Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool) education-num: Number of educational years completed marital-status: Marital status (Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse) occupation: Work Occupation (Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces) relationship: Relationship Status (Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried) race: Race (White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black) sex: Sex (Female, Male) capital-gain: Monetary Capital Gains capital-loss: Monetary Capital Losses hours-per-week: Average Hours Per Week Worked native-country: Native Country (United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad\u0026amp;Tobago, Peru, Hong, Holand-Netherlands) income: Income Class (\u0026lt;=50K, \u0026gt;50K) (Target Variable)  ","id":58,"section":"posts","summary":"Finding Donors for CharityML Employ several supervised algorithms to accurately model individuals' income using data collected from the 1994 U.S. Census. From the best candidate algorithm from preliminary results and further optimize this algorithm to best model the data. Construct a model that accurately predicts whether an individual makes more than $50,000. This sort of task can arise in a non-profit setting, where organizations survive on donations. Understanding an individual\u0026rsquo;s income can help a non-profit better understand how large of a donation to request, or whether or not they should reach out to begin with.","tags":["org:jrbeverly"],"title":"charityml","uri":"/2020/02/jrbeverly-charityml/","year":"2020"},{"content":"Creating Customer Segments Analyze a dataset containing data on various customers' annual spending amounts (reported in monetary units) of diverse product categories for internal structure. One goal of this project is to best describe the variation in the different types of customers that a wholesale distributor interacts with. Doing so would equip the distributor with insight into how to best structure their delivery service to meet the needs of each customer.\nGetting Started You can spin up the environment using docker by running the following commands:\nbash docker.bash # inside docker container bash .build/conda.bash Alternatively you can manually install Python and the following Python libraries installed:\n NumPy Pandas matplotlib scikit-learn  You will also need to have software installed to run and execute a Jupyter Notebook\nData The customer segments data is included as a selection of 440 data points collected on data found from clients of a wholesale distributor in Lisbon, Portugal. More information can be found on the UCI Machine Learning Repository.\nNote (m.u.) is shorthand for monetary units.\nFeatures  Fresh: annual spending (m.u.) on fresh products (Continuous); Milk: annual spending (m.u.) on milk products (Continuous); Grocery: annual spending (m.u.) on grocery products (Continuous); Frozen: annual spending (m.u.) on frozen products (Continuous); Detergents_Paper: annual spending (m.u.) on detergents and paper products (Continuous); Delicatessen: annual spending (m.u.) on and delicatessen products (Continuous); Channel: {Hotel/Restaurant/Cafe - 1, Retail - 2} (Nominal) Region: {Lisbon - 1, Oporto - 2, or Other - 3} (Nominal)  ","id":59,"section":"posts","summary":"Creating Customer Segments Analyze a dataset containing data on various customers' annual spending amounts (reported in monetary units) of diverse product categories for internal structure. One goal of this project is to best describe the variation in the different types of customers that a wholesale distributor interacts with. Doing so would equip the distributor with insight into how to best structure their delivery service to meet the needs of each customer.","tags":["org:jrbeverly"],"title":"customer-segments","uri":"/2020/02/jrbeverly-customer-segments/","year":"2020"},{"content":"Summary Blockycraft is a interactive graphics demo to create a Minecraft inspired demo which revolves around breaking and placing blocks. The game world is composed of rough cubes arranged in a fixed grid pattern and representing different materials, such as dirt, stone, and snow. The techniques used in the demo can be toggled using keyboard commands. The Blockycraft project is written using C++ and OpenGL.\nNotes The project was developed for a University of Waterloo Graphics course in Summer 2016. The focus was on graphics techniques (e.g. transparency, ambient occlusion) or gameplay.\nThe original blockycraft project was a hack \u0026amp; slack project built under a pretty short timeline (~month). After completing it, the project went through a second round of refactoring to break up the main.cpp file which stored almost all of the codebase. During this refactoring, improvements were pulled in, or code from external projects was adopted to improve the overall codebase.\nDue to the difficulty of building the project, I recently recorded video of the project to go along with the screenshots.\nAcknowledgements I would like to take a moment to acknowledge worked that has been included or incorporated into the refactored version of Blockycraft:\n Perlin Noise - Replaced previous implementation with a modified version of sol-prog\u0026rsquo;s improved perlin noise implementation fogleman/Craft - Improved rendering mechanism and faster means of performing lookups of Chunk data (originally implemented was significantly slower) CS488 Course Assets - The project is built within the course assets of CS488. This has not changed in the refactoring  Kenney.nl The project icon is retrieved from kenney.nl. The original source material has been altered for the purposes of the project. The icon is used under the terms of the CC0 1.0 Universal.\nThe project uses assets by Kenney from kenney.nl/, and the icon is built from these assets.\n","id":60,"section":"posts","summary":"Summary Blockycraft is a interactive graphics demo to create a Minecraft inspired demo which revolves around breaking and placing blocks. The game world is composed of rough cubes arranged in a fixed grid pattern and representing different materials, such as dirt, stone, and snow. The techniques used in the demo can be toggled using keyboard commands. The Blockycraft project is written using C++ and OpenGL.\nNotes The project was developed for a University of Waterloo Graphics course in Summer 2016.","tags":["org:blockycraft"],"title":"blockycraft-classic","uri":"/2020/01/blockycraft-blockycraft-classic/","year":"2020"},{"content":"Classification of Dogs My implementation of the Convolutional Neural Networks (CNN) algorithm for identifying a canines breed from an image. Additionally, it supply the resembled dog breed if provided an image of a human.\nClassification You can see an example classification for the German Shepherd picture below:\nAnd an example of a misclassification for a rotweiler.\nNotes You can see my full analysis of the classifier in the notebook, but a snippet is included below\nThe output result is about where I expected it to be. ResNet50 is very good with large image data, and I provided minimal layers to the algorithm, I expected a good base performance. However the model has only achieved 81% score, which would not work very well in a production environment (app or SaaS).\nPotential improvements that are available for this model are:\n Reduce overfitting with usages of dropout and batch_normalization layers Add batch_normalization to reduce covariate shift in the calculation process Change the optimizer to another type, to find a better optimizer fit for my problem (adagrad or adam)  None of the above are guaranteed to produce a better result, just potential directions that I could pursue to improve the performance of the machine.\nProject This project was submitted by Jonathan Beverly as part of the Nanodegree At Udacity. The source was originally pulled from https://github.com/udacity/dog-project.git.\n","id":61,"section":"posts","summary":"Classification of Dogs My implementation of the Convolutional Neural Networks (CNN) algorithm for identifying a canines breed from an image. Additionally, it supply the resembled dog breed if provided an image of a human.\nClassification You can see an example classification for the German Shepherd picture below:\nAnd an example of a misclassification for a rotweiler.\nNotes You can see my full analysis of the classifier in the notebook, but a snippet is included below","tags":["org:jrbeverly"],"title":"dog-project","uri":"/2020/01/jrbeverly-dog-project/","year":"2020"},{"content":"Quadcopter using Reinforcement Learning My implementation of the DDPG reinforcement learning algorithm to solve the problem of a quadcopter taking flight.\nI have included a reference to the DDPG paper used in the development of the flying agent:\n Continuous control with deep reinforcement learning Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra\nWe adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.\n Flight Path You can see the best take-off that was produced from my reward function and DDPG implementation below:\nAlthough I am partial to this series of flight paths that occurred during the development phase of this project.\nNotes You can see my full analysis of my quadcopter in the notebook, but a snippet is included below\nI went with take-off in the end, although I did try landing at first. I was having a lot of difficulty getting started with landing, so I switched over to take-off. I think one of the issues with my landing approach was that my reward function was a distance based one, which meant there was no attempt to setup an \u0026lsquo;approach\u0026rsquo;.\nI went through a couple of iterations with my reward function, each time implementing a new factor. I split the function into the computation of two components: reward and penalty. I wanted to incentivize the agent into certain behaviours, while discouraging results that were not optimal. I describe it this way, because I wished to reflect these ideas:\n If you are stable, you have a lower penalty. If you are reckless (flying sideways), you have a higher penalty. The closer you are, the lower the penalty. The further you are, the higher the penalty. You are rewarded for proximity (cumulative) You are rewarded for still flying  These ideas didn\u0026rsquo;t translate well in the end, and in the end only a weighted distance was used for computing the reward. You can see that reward=0 =\u0026gt; abs(0-penalty) = penalty for the function. This means that most of the code in the get_reward function is not actually needed. The best decision was to ensure that the reward function was normalized, as that made the simulation a lot more consistent between runs.\n","id":62,"section":"posts","summary":"Quadcopter using Reinforcement Learning My implementation of the DDPG reinforcement learning algorithm to solve the problem of a quadcopter taking flight.\nI have included a reference to the DDPG paper used in the development of the flying agent:\n Continuous control with deep reinforcement learning Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra\nWe adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain.","tags":["org:jrbeverly"],"title":"quadcopter","uri":"/2020/01/jrbeverly-quadcopter/","year":"2020"},{"content":"BMX BMX grants you API access to your AWS accounts, based on Okta credentials that you already own.\nIt uses your Okta identity to create short-term AWS STS tokens, as an alternative to long-term IAM access keys. BMX manages your STS tokens with the following commands:\n bmx print writes your short-term tokens to stdout as AWS environment variables. You can execute bmx print\u0026rsquo;s output to make the environment variables available to your shell. bmx write writes your short-term tokens to ~/.aws/credentials.  BMX prints detailed usage information when you run bmx -h or bmx \u0026lt;cmd\u0026gt; -h.\nBMX was developed by D2L (Brightspace/bmx), and modifications have been made to the project by Arctic Wolf.\nFeatures  BMX is multi-platform: it runs on Linux, Windows, and Mac. BMX maintains your Okta session for 12 hours: you enter your Okta password once a day, and BMX takes care of the rest. Project scoped configurations BMX supports Web and SMS MFA.  Installation Available versions of BMX are available on the releases page.\nGetting Started To authenticate and obtain a session via the command line, run the following:\nbmx login This will prompt you for your Okta organization and credentials. When you have successfully connected, you can run the following to get a set of IAM STS credentials for use with the AWS API:\nbmx print The command will print a series of environment set commands, that can be used to set the environment variables of the current shell session:\nexport AWS_SESSION_TOKEN=... export AWS_ACCESS_KEY_ID=... export AWS_SECRET_ACCESS_KEY=... # Run AWSCLI using environment variables for credentials aws sts get-caller-identity If you\u0026rsquo;d like to learn about the ways BMX assists with authenticating to AWS, you can review in the getting started documentation.\nVersioning BMX is maintained under the Semantic Versioning guidelines.\nGetting Involved See CONTRIBUTING.md for guidelines.\n","id":63,"section":"posts","summary":"BMX BMX grants you API access to your AWS accounts, based on Okta credentials that you already own.\nIt uses your Okta identity to create short-term AWS STS tokens, as an alternative to long-term IAM access keys. BMX manages your STS tokens with the following commands:\n bmx print writes your short-term tokens to stdout as AWS environment variables. You can execute bmx print\u0026rsquo;s output to make the environment variables available to your shell.","tags":["org:jrbeverly"],"title":"bmx","uri":"/2020/01/jrbeverly-bmx/","year":"2020"},{"content":"Git Timeline Allows bulk modification of the commit dates of a repository, changing the history of a repository.\nUsage # Creates the demo repository ./git-timeline.bash clone # Copies the demo repository to the working environment ./git-timeline.bash working # Exports the history of the git repository to files ./git-timeline.bash history You can then edit the dates of the three files emitted:\n FIRST - The first commit to the repository HISTORY - The commit history LATEST - The most recent commit to the repository  After you have done this, you can then run apply and show:\n./git-timeline.bash apply ./git-timeline.bash show If you are finding it difficult to get the right timelines (or just working with the scripts), you can use cycle to start fresh, and re-apply. This does not alter the modified time files.\n","id":64,"section":"posts","summary":"Git Timeline Allows bulk modification of the commit dates of a repository, changing the history of a repository.\nUsage # Creates the demo repository ./git-timeline.bash clone # Copies the demo repository to the working environment ./git-timeline.bash working # Exports the history of the git repository to files ./git-timeline.bash history You can then edit the dates of the three files emitted:\n FIRST - The first commit to the repository HISTORY - The commit history LATEST - The most recent commit to the repository  After you have done this, you can then run apply and show:","tags":["org:jrbeverly"],"title":"git-timeline","uri":"/2020/01/jrbeverly-git-timeline/","year":"2020"},{"content":"AWS Lambda PowerShell Example A simple Lambda function written in PowerShell to validate if New-PSSession can be leveraged from an AWS Lambda.\nThe objective of this repository was to determine if it was possible to use New-PSSession from an AWS Lambda. This would allow for modification of services like Office365 using remote sessions from AWS without requiring an ECS/EC2 container. Without a custom lambda runtime (or some way of enabling WSMan), it would be difficult to do this with a vanilla lambda execution environment. The error encountered during runtime is included below:\nThis parameter set requires WSMan, and no supported WSMan client library was found. WSMan is either not installed or unavailable for this system These errors lead to the following github issues for PowerShell: PowerShell/5561 \u0026amp; PowerShell/11159.\nThe lambda function can be published by Publish-AWSPowerShellLambda -ScriptPath .\\RemoteSession.ps1 -Name RemoteSession -Region us-east-1.\n","id":65,"section":"posts","summary":"AWS Lambda PowerShell Example A simple Lambda function written in PowerShell to validate if New-PSSession can be leveraged from an AWS Lambda.\nThe objective of this repository was to determine if it was possible to use New-PSSession from an AWS Lambda. This would allow for modification of services like Office365 using remote sessions from AWS without requiring an ECS/EC2 container. Without a custom lambda runtime (or some way of enabling WSMan), it would be difficult to do this with a vanilla lambda execution environment.","tags":["org:jrbeverly"],"title":"aws-lambda-remote-session","uri":"/2020/01/jrbeverly-aws-lambda-remote-session/","year":"2020"},{"content":"Friending Landing Page This repository contains the source for the webpage friending.jrbeverly.dev.\nThe webpage is built off the Hugo template by by themefisher used under CC by 3.0 - Alterations to theme have been made to the underlying template.\nThose changes are made available in the themes/vex directory.\n","id":66,"section":"posts","summary":"Friending Landing Page This repository contains the source for the webpage friending.jrbeverly.dev.\nThe webpage is built off the Hugo template by by themefisher used under CC by 3.0 - Alterations to theme have been made to the underlying template.\nThose changes are made available in the themes/vex directory.","tags":["org:thefriending"],"title":"website","uri":"/2020/01/thefriending-website/","year":"2020"},{"content":"AWS Chat App SAM Application for a simple chat application based on API Gateways new WebSocket API feature. This was originally developed as an experiment to see how viable running a chat-bot in a fully serverless environment, as opposed to just running on a container in ECS.\nThis repository is based on Announcing WebSocket APIs in Amazon API Gateway, with the cloudformation and lambdas from simple-websockets-chat-app.\nUsage A build-harness created with make is available for the repository. This harness simplifies the command necessary to build and deploy the project. You can see the available targets by running make help:\nUsage: make \u0026lt;target\u0026gt; help This help text. Deploy pack Package the yaml files as a single package Package an AWS SAM application. deploy Deploy an AWS SAM application. publish Re-package and deploy an AWS SAM application. Cloudformation describe Describes the deployed cloudformation stack. outputs Describes the outputs of the deployed cloudformation stack. Docker docker Runs AWS SAM in docker. Tests chat Login to one of the chat sessions. stop Halt the existing chat services. logs Dump the logs from the docker image. The build-harness does a bit more in this case, as I experimented with a couple different tools while working on this. Notably:\n AWS SAM for packaging and deploying the code cfpack.js for bundling multiple cloudformation templates into a single one docker-compose for testing the service with wscat awscli for describing and working with the deployed cloudformation stack  Deploy Using docker, you can build and deploy the sample using the following steps:\nmake docker make pack make package make deploy You can get the URL for the websocket by calling make outputs. This can then be provided to the docker-compose template in docker/.\nmake outputs echo \u0026#34;WSCAT_URI=wss://\u0026lt;id\u0026gt;.execute-api.\u0026lt;region\u0026gt;.amazonaws.com/\u0026lt;env\u0026gt;\u0026#34; \u0026gt; docker/.env make chat # wscat -c $WSCAT_URI # {\u0026#34;message\u0026#34;:\u0026#34;sendmessage\u0026#34;, \u0026#34;data\u0026#34;:\u0026#34;hello world\u0026#34;} Notes I built this while I was looking into serverless chat-ops. At the time I was tinkering with ideas about chat-ops like services that could be used to manage key rotation in external services. This lead to some experiments with Hubot and AWS Lambda. I thought this example would offer sufficient complexity for tinkering with the idea of a lambda-based application for working with different services. The ideas were as follows:\n If each service is its own lambda, permissions to service tokens (GitHub / CircleCI / TravisCI / Jenkins) can be locked down at the route level Each service token could have its own secret store, allowing granular access permissions Rotations would occur on set intervals, which meant the service didn\u0026rsquo;t need to remain running at all times Trigger actions (revocations / deletes / freezes) could be trigger by API Routes Authentication could potentially leverage existing AWS services (useful for a secure service)  Terraform I had originally intended to write this in terraform, however at time of development (\u0026amp; writing), Terraform does not have support for the new version of API Gateway (V2). I have included CloudFormation in Terraform prototype in the directory tests/ for future reference. Rather than trying to get this working, I looked for ways to setup a terraform-like experience in cloudformation.\nCfPack I used cfpack.js to split the cloudformation template into smaller templates. Although this added another dependency (\u0026amp; build step), I found it a lot easier to work with multiple smaller templates as opposed to one big one. I did some early experiments with nested stacks, but personally found it like juggling.\n cfpack is a small CLI tool that can help you to deal with huge CloudFormation templates by splitting it into multiple smaller templates. Using this tool you can also build sharable drop-in templates that you can share across your projects.\n ","id":67,"section":"posts","summary":"AWS Chat App SAM Application for a simple chat application based on API Gateways new WebSocket API feature. This was originally developed as an experiment to see how viable running a chat-bot in a fully serverless environment, as opposed to just running on a container in ECS.\nThis repository is based on Announcing WebSocket APIs in Amazon API Gateway, with the cloudformation and lambdas from simple-websockets-chat-app.\nUsage A build-harness created with make is available for the repository.","tags":["org:jrbeverly"],"title":"aws-chat-app","uri":"/2019/12/jrbeverly-aws-chat-app/","year":"2019"},{"content":"Hubot in AWS ECS Hubot deployment in AWS using AWS ECS Fargate. This was prototyped out while I was evaluating ChatOps strategies that could be used to wrap existing web interfaces or require minimal overhead.\nUsage A build-harness created with make is available for the repository. This harness simplifies the commands necessary to build and deploy the project. You can see the available targets by running make help:\nUsage: make \u0026lt;target\u0026gt; help This help text. Terraform ecr Deploy the ECR for the Hubot image ecs Deploy the Hubot ECS service destroy Destroys the terraform instructure Hubot launch Launches the hubot application in docker Docker docker Build the hubot image for deployment deploy Deploys the hubot image to ECR Deploy Using docker, you can build and deploy the sample using the following steps:\nmake docker make ecr make deploy make ecs make launch Notes At the time I was building this I was trying to figure out a nice way to have a ChatOps style interface for a web service. The reason for ChatOps is that I was after something that would:\n Allow me to execute commands easily while on the go (e.g. slack + phone) Offer multiple operations for diagnostic and configuration Require minimal knowledge of how the system might work (e.g. \u0026lt;tool\u0026gt; help)  I do enjoy what Hubot offers but it did not exactly fit the target needs I was going for:\n I didn\u0026rsquo;t need the more advanced input format Slash commands were sufficient for the interface I didn\u0026rsquo;t wish to develop the app within the context of the Hubot application Additional infrastructure (ECS Service) was worrying, as I was aiming for high reliability  ","id":68,"section":"posts","summary":"Hubot in AWS ECS Hubot deployment in AWS using AWS ECS Fargate. This was prototyped out while I was evaluating ChatOps strategies that could be used to wrap existing web interfaces or require minimal overhead.\nUsage A build-harness created with make is available for the repository. This harness simplifies the commands necessary to build and deploy the project. You can see the available targets by running make help:\nUsage: make \u0026lt;target\u0026gt; help This help text.","tags":["org:jrbeverly"],"title":"hubot-in-aws","uri":"/2019/12/jrbeverly-hubot-in-aws/","year":"2019"},{"content":"Docker AWSCLI Built with Bazel  The AWS Command Line Interface (CLI) is a unified tool to manage your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts.\n CardboardCI aims to create a collection of docker images that can be used in continuous integration. These images will have all dependencies pinned, to ensure that any commit will produce the exact same image (or as close to as possible). The aim of this repository is to build docker-awscli in Bazel, to evaluate whether it would help fit those goals.\nAdditionally it helps to test whether Bazel could work under GitHub Actions, or would require another service to build.\nNotes If I were to use this for CardboardCI, I\u0026rsquo;d probably need to develop/use packer manager rules for the following:\n GitHub Releases NPM RubyGems AptGet  And potentially some edge cases with LaTeX and Lua (luarocks).\nPackage Manager Rules For the package managers that would need rules, they would follow the format of the existing download_pkgs (f(deps[]) =\u0026gt; tarball). Most of the package managers used in CardboardCI have some way to download but not install the packages. Here are some examples of the rule usages:\ndownload_npm( name = \u0026#34;pkgs\u0026#34;, image_tar = \u0026#34;@ubuntu//image\u0026#34;, packages = [ \u0026#34;surge:0.0.1\u0026#34;, ], # Could we read all of the packages from a file? # packages_file = \u0026#34;:file\u0026#34; ) If the rule could support a way of providing a file, then the same automated process as now could be used for upgrading the dependencies. The big problem I see with this is that if only 1 dependency changes, we must re-download all of the other dependencies. The option of bazel-ifying each dependency as a rule would create problems with updating (e.g. how to update version). This isn\u0026rsquo;t too bad if there exists a file like so:\nload(\u0026#34;@io_bazel_rules_docker//docker/package_managers:download_npm.bzl\u0026#34;, \u0026#34;download_npm\u0026#34;) download_npm( name = \u0026#34;pkg_surge\u0026#34;, image_tar = \u0026#34;@ubuntu//image\u0026#34;, src = \u0026#34;:surge.dep\u0026#34; ) With the .dep file looking something like this:\nname=\u0026#34;surge\u0026#34; version=\u0026#34;0.0.1\u0026#34; Conclusions Below I have included jot notes for my conclusions on using Bazel to build docker images for CardboardCI:\n Most of the images can be built using Bazel Some images may require bazel-ifying the source projects Windows support is lacking, and the resulting errors are difficult to investigate The errors are sometimes opaque (e.g. /tmp/installer OCI runtime error) Bazel rules would be needed for other package managers (npm, rubygems, luarocks, etc) container_run_and_commit allows for half-in half-out development Having each image defined with bazel could have some benefits if it was all-in  I don\u0026rsquo;t feel that there is a compelling reason to try and implement the docker images in Bazel, and more than enough reasons why it isn\u0026rsquo;t worthwhile at the moment.\n","id":69,"section":"posts","summary":"Docker AWSCLI Built with Bazel  The AWS Command Line Interface (CLI) is a unified tool to manage your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts.\n CardboardCI aims to create a collection of docker images that can be used in continuous integration. These images will have all dependencies pinned, to ensure that any commit will produce the exact same image (or as close to as possible).","tags":["org:cardboardci"],"title":"bazel-docker-awscli","uri":"/2019/11/cardboardci-bazel-docker-awscli/","year":"2019"},{"content":"Powershell Library on GitHub Summary A powershell library that is installed from GitHub, rather than from Powershellgallery.\nUsage Downloading from GitHub:\n# Enable installing from github Install-Module -Name InstallModuleFromGitHub # Install the module Install-ModuleFromGitHub -GitHubRepo jrbeverly/pwsh-from-github # Perform Actions Write-Hello -Name \u0026#34;World\u0026#34; Write-World -Message \u0026#34;LFG\u0026#34; Notes Experimenting with using InstallModuleFromGitHub, instead of using Powershell gallery.\nI have noticed that this requires all of the scripts be at the root of the repository, rather than using a folder structure like so:\n\u0026gt; lib/ \u0026gt; Namespace1/ \u0026gt; Something1-1.ps1 \u0026gt; Something1-2.ps1 \u0026gt; Namespace2/ \u0026gt; Something2-2.ps1 \u0026gt; Main.ps1 ","id":70,"section":"posts","summary":"Powershell Library on GitHub Summary A powershell library that is installed from GitHub, rather than from Powershellgallery.\nUsage Downloading from GitHub:\n# Enable installing from github Install-Module -Name InstallModuleFromGitHub # Install the module Install-ModuleFromGitHub -GitHubRepo jrbeverly/pwsh-from-github # Perform Actions Write-Hello -Name \u0026#34;World\u0026#34; Write-World -Message \u0026#34;LFG\u0026#34; Notes Experimenting with using InstallModuleFromGitHub, instead of using Powershell gallery.\nI have noticed that this requires all of the scripts be at the root of the repository, rather than using a folder structure like so:","tags":["org:jrbeverly"],"title":"pwsh-from-github","uri":"/2019/11/jrbeverly-pwsh-from-github/","year":"2019"},{"content":"AWS IAM External Role Terraform module for a continuous integration user-role pairing.\nThese types of resources are supported:\n IAM Role  Usage module \u0026#34;cicd_setup\u0026#34; { source = \u0026#34;git::https://gitlab.com/infraprints/modules/aws/iam-ci-role\u0026#34; username = \u0026#34;infraprints-iam-ci-role-basic\u0026#34; role_name = \u0026#34;infraprints-iam-ci-role-basic\u0026#34; environment_variable = { s3_bucket = \u0026#34;infraprints-bucket-example\u0026#34; hello_world = \u0026#34;hello world\u0026#34; } } Examples  Basic Example  Notes  Environment variables are prefixed with ENV_ to prevent them  Inputs    Name Description Type Default Required     environment_variable Times map \u0026lt;map\u0026gt; no   labels  map \u0026lt;map\u0026gt; no   length The length of the external id desired. string \u0026quot;16\u0026quot; no   path  string \u0026quot;ci\u0026quot; no   period  string \u0026quot;32400\u0026quot; no   role_name The name of the role. string n/a yes   service  string \u0026quot;GitLab\u0026quot; no   tags Key-value mapping of tags for the IAM role. map \u0026lt;map\u0026gt; no   username The name of the user. string n/a yes    Outputs    Name Description     arn The Amazon Resource Name (ARN) specifying the role.   create_date The Amazon Resource Name (ARN) specifying the role.   unique_id The Amazon Resource Name (ARN) specifying the role.    ","id":71,"section":"posts","summary":"AWS IAM External Role Terraform module for a continuous integration user-role pairing.\nThese types of resources are supported:\n IAM Role  Usage module \u0026#34;cicd_setup\u0026#34; { source = \u0026#34;git::https://gitlab.com/infraprints/modules/aws/iam-ci-role\u0026#34; username = \u0026#34;infraprints-iam-ci-role-basic\u0026#34; role_name = \u0026#34;infraprints-iam-ci-role-basic\u0026#34; environment_variable = { s3_bucket = \u0026#34;infraprints-bucket-example\u0026#34; hello_world = \u0026#34;hello world\u0026#34; } } Examples  Basic Example  Notes  Environment variables are prefixed with ENV_ to prevent them  Inputs    Name Description Type Default Required     environment_variable Times map \u0026lt;map\u0026gt; no   labels  map \u0026lt;map\u0026gt; no   length The length of the external id desired.","tags":["org:infraprints"],"title":"terraform-aws-iam-ci-role","uri":"/2019/11/infraprints-terraform-aws-iam-ci-role/","year":"2019"},{"content":"AWS S3 Terraform State Output Terraform module which creates an S3 Object containing terraform outputs.\nThese types of resources are supported:\n S3 Bucket Object Template  Usage locals { topics = [\u0026#34;aws\u0026#34;, \u0026#34;s3\u0026#34;, \u0026#34;terraform\u0026#34;] tags = { name = \u0026#34;infraprints\u0026#34;, description = \u0026#34;Infrastructure as Code.\u0026#34; } } module \u0026#34;output_resources\u0026#34; { source = \u0026#34;git::https://gitlab.com/infraprints/modules/aws/terraform-state-output\u0026#34; bucket = \u0026#34;infraprints-terraform-state-output\u0026#34; key = \u0026#34;aws/infraprints/project/outputs.tf\u0026#34; terraform_output = [ { key = \u0026#34;aws_account_id\u0026#34; value = \u0026#34;123412341234\u0026#34; }, { key = \u0026#34;topics\u0026#34; value = jsonencode(local.topics) }, { key = \u0026#34;tags\u0026#34; value = jsonencode(local.tags) }, ] } With using the output of this module here:\nmodule \u0026#34;example\u0026#34; { source = \u0026#34;s3::https://s3.amazonaws.com/infraprints-terraform-state-output/aws/infraprints/project\u0026#34; } output \u0026#34;aws_account_id\u0026#34; { value = module.example.aws_account_id } output \u0026#34;topics\u0026#34; { value = module.example.topics } output \u0026#34;tags\u0026#34; { value = module.example.tags } Examples  Basic Example Arrays Example Maps Example  Notes  S3 Versioning should be enabled  Inputs    Name Description Type Default Required     bucket The name of the bucket to put the file in. string n/a yes   key The name of the object once it is in the bucket. Should end with the .tf file extension. string n/a yes   tags A mapping of tags to assign to the object. map \u0026lt;map\u0026gt; no   terraform_output A set of terraform outputs to make available. list n/a yes    Outputs    Name Description     etag The ETag generated for the object (an MD5 sum of the object content). For plaintext objects or objects encrypted with an AWS-managed key, the hash is an MD5 digest of the object data. For objects encrypted with a KMS key or objects created by either the Multipart Upload or Part Copy operation, the hash is not an MD5 digest, regardless of the method of encryption. More information on possible values can be found on Common Response Headers.   id The key of the resource supplied above.   rendered The final rendered template.   version_id A unique version ID value for the object, if bucket versioning is enabled.    ","id":72,"section":"posts","summary":"AWS S3 Terraform State Output Terraform module which creates an S3 Object containing terraform outputs.\nThese types of resources are supported:\n S3 Bucket Object Template  Usage locals { topics = [\u0026#34;aws\u0026#34;, \u0026#34;s3\u0026#34;, \u0026#34;terraform\u0026#34;] tags = { name = \u0026#34;infraprints\u0026#34;, description = \u0026#34;Infrastructure as Code.\u0026#34; } } module \u0026#34;output_resources\u0026#34; { source = \u0026#34;git::https://gitlab.com/infraprints/modules/aws/terraform-state-output\u0026#34; bucket = \u0026#34;infraprints-terraform-state-output\u0026#34; key = \u0026#34;aws/infraprints/project/outputs.tf\u0026#34; terraform_output = [ { key = \u0026#34;aws_account_id\u0026#34; value = \u0026#34;123412341234\u0026#34; }, { key = \u0026#34;topics\u0026#34; value = jsonencode(local.","tags":["org:infraprints"],"title":"terraform-aws-terraform-state-output","uri":"/2019/11/infraprints-terraform-aws-terraform-state-output/","year":"2019"},{"content":"terraform-aws-acm-certificate ","id":73,"section":"posts","summary":"terraform-aws-acm-certificate ","tags":["org:infraprints"],"title":"terraform-aws-route53-subdomains","uri":"/2019/11/infraprints-terraform-aws-route53-subdomains/","year":"2019"},{"content":"AWS Terraform Remote State with Lock Table Terraform module which creates a terraform remote state storage in S3 with a DynamoDB lock table.\nThese types of resources are supported:\n S3 Bucket DynamoDB Table  Usage module \u0026#34;remote_state\u0026#34; { source = \u0026#34;git::https://gitlab.com/infraprints/modules/aws/terraform-remote-state\u0026#34; bucket = \u0026#34;infraprints-terraform-remote-state\u0026#34; table = \u0026#34;infraprints-terraform-lock-table\u0026#34; } resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;terraform\u0026#34; { name = \u0026#34;infraprints-terraform\u0026#34; assume_role_policy = \u0026lt;\u0026lt;EOF { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;ec2.amazonaws.com\u0026#34; }, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Sid\u0026#34;: \u0026#34;\u0026#34; } ] } EOF } resource \u0026#34;aws_iam_role_policy\u0026#34; \u0026#34;terraform_policy\u0026#34; { name = \u0026#34;TerraformRemoteState\u0026#34; role = aws_iam_role.terraform.id policy = module.remote_state.write_policy } Examples  Basic Example  Notes  Only SSL access is permitted to the remote state storage Only encrypted objects can be uploaded to the remote state storage The lock table is not encrypted  Inputs Placeholder.\nOutputs Placeholder.\n","id":74,"section":"posts","summary":"AWS Terraform Remote State with Lock Table Terraform module which creates a terraform remote state storage in S3 with a DynamoDB lock table.\nThese types of resources are supported:\n S3 Bucket DynamoDB Table  Usage module \u0026#34;remote_state\u0026#34; { source = \u0026#34;git::https://gitlab.com/infraprints/modules/aws/terraform-remote-state\u0026#34; bucket = \u0026#34;infraprints-terraform-remote-state\u0026#34; table = \u0026#34;infraprints-terraform-lock-table\u0026#34; } resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;terraform\u0026#34; { name = \u0026#34;infraprints-terraform\u0026#34; assume_role_policy = \u0026lt;\u0026lt;EOF { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;ec2.","tags":["org:infraprints"],"title":"terraform-aws-terraform-remote-state","uri":"/2019/11/infraprints-terraform-aws-terraform-remote-state/","year":"2019"},{"content":"AWS S3 Terraform State Output Terraform module for an tiered storage S3 bucket with eventual object expiration. Primary use key is for a build artifacts storage.\nThese types of resources are supported:\n S3 Bucket  Usage module \u0026#34;build_artifacts\u0026#34; { source = \u0026#34;git::https://gitlab.com/infraprints/modules/aws/s3-artifacts\u0026#34; bucket = \u0026#34;infraprints-s3-artifacts\u0026#34; standard_transition_days = 10 glacier_transition_days = 30 expiration_days = 365 tags = { Longevity = \u0026#34;Yearly\u0026#34; Expiration = \u0026#34;True\u0026#34; } } Examples  Basic Example Adjusted Example  Notes  With the default configuration, all objects in the S3 bucket will expire in 90 days. The S3 bucket uses tiered storage with eventual expiration. This bucket is not designed for long term persistence.  Inputs    Name Description Type Default Required     bucket The name of the bucket. string n/a yes   expiration_days Number of days until objects are expunged. string \u0026quot;90\u0026quot; no   force_destroy A boolean that indicates all objects should be deleted from the bucket so that the bucket can be destroyed without error. These objects are not recoverable. string \u0026quot;false\u0026quot; no   glacier_transition_days Number of days until objects are transitioned to the GLACIER storage class. string \u0026quot;60\u0026quot; no   region If specified, the AWS region this bucket should reside in. Otherwise, the region used by the callee. string \u0026quot;\u0026quot; no   standard_transition_days Number of days until objects are transitioned to the STANDARD_IA storage class. string \u0026quot;30\u0026quot; no   tags A mapping of tags to assign to the bucket. map \u0026lt;map\u0026gt; no    Outputs    Name Description     arn The ARN of the bucket. Will be of format arn:aws:s3:::bucketname.   bucket The name of the bucket.   bucket_domain_name The bucket domain name. Will be of format bucketname.s3.amazonaws.com.   id The name of the bucket.   region The AWS region this bucket resides in.    ","id":75,"section":"posts","summary":"AWS S3 Terraform State Output Terraform module for an tiered storage S3 bucket with eventual object expiration. Primary use key is for a build artifacts storage.\nThese types of resources are supported:\n S3 Bucket  Usage module \u0026#34;build_artifacts\u0026#34; { source = \u0026#34;git::https://gitlab.com/infraprints/modules/aws/s3-artifacts\u0026#34; bucket = \u0026#34;infraprints-s3-artifacts\u0026#34; standard_transition_days = 10 glacier_transition_days = 30 expiration_days = 365 tags = { Longevity = \u0026#34;Yearly\u0026#34; Expiration = \u0026#34;True\u0026#34; } } Examples  Basic Example Adjusted Example  Notes  With the default configuration, all objects in the S3 bucket will expire in 90 days.","tags":["org:infraprints"],"title":"terraform-aws-s3-artifacts","uri":"/2019/11/infraprints-terraform-aws-s3-artifacts/","year":"2019"},{"content":"terraform-aws-acm-certificate ","id":76,"section":"posts","summary":"terraform-aws-acm-certificate ","tags":["org:infraprints"],"title":"terraform-aws-codepipeline-terraform","uri":"/2019/11/infraprints-terraform-aws-codepipeline-terraform/","year":"2019"},{"content":"AWS ACM DNS Validated Certificate Terraform module for provisioning a DNS validated certificate, along with the required validation records. The module will wait for validation to complete.\nThese types of resources are supported:\n ACM Certificate ACM Certificate Validation  Usage module \u0026#34;certificate\u0026#34; { source = \u0026#34;git::https://gitlab.com/infraprints/modules/aws/acm-certificate\u0026#34; zone_id = \u0026#34;${data.aws_route53_zone.zone.id}\u0026#34; domain_name = \u0026#34;infraprints.io\u0026#34; subject_alternative_names = [ \u0026#34;api.infraprints.io\u0026#34;, \u0026#34;dev.infraprints.io\u0026#34;, ] } data \u0026#34;aws_route53_zone\u0026#34; \u0026#34;zone\u0026#34; { name = \u0026#34;infraprints.io\u0026#34; } Examples  Basic Example Multiple Records Example  Notes  The module deploys the required validation records and wait for validation to complete, which can take upwards to 30 minutes.  Inputs    Name Description Type Default Required     domain_name A domain name for which the certificate should be issued string n/a yes   subject_alternative_names A list of domains that should be SANs in the issued certificate list \u0026lt;list\u0026gt; no   ttl The TTL of the validation record(s). string \u0026quot;60\u0026quot; no   zone_id The ID of the hosted zone to contain the validation record(s). string n/a yes    Outputs    Name Description     arn The ARN of the certificate that is being validated.   domain_name The domain name for which the certificate is issued.   fqdn FQDN built using the zone domain and name.   id The ARN of the certificate.   validation_record_fqdns List of FQDNs that implement the validation.    ","id":77,"section":"posts","summary":"AWS ACM DNS Validated Certificate Terraform module for provisioning a DNS validated certificate, along with the required validation records. The module will wait for validation to complete.\nThese types of resources are supported:\n ACM Certificate ACM Certificate Validation  Usage module \u0026#34;certificate\u0026#34; { source = \u0026#34;git::https://gitlab.com/infraprints/modules/aws/acm-certificate\u0026#34; zone_id = \u0026#34;${data.aws_route53_zone.zone.id}\u0026#34; domain_name = \u0026#34;infraprints.io\u0026#34; subject_alternative_names = [ \u0026#34;api.infraprints.io\u0026#34;, \u0026#34;dev.infraprints.io\u0026#34;, ] } data \u0026#34;aws_route53_zone\u0026#34; \u0026#34;zone\u0026#34; { name = \u0026#34;infraprints.io\u0026#34; } Examples  Basic Example Multiple Records Example  Notes  The module deploys the required validation records and wait for validation to complete, which can take upwards to 30 minutes.","tags":["org:infraprints"],"title":"terraform-aws-acm-certificate","uri":"/2019/11/infraprints-terraform-aws-acm-certificate/","year":"2019"},{"content":"AWS IAM External Role Terraform module for describing an IAM role responsible for delegating cross-account access.\nThese types of resources are supported:\n IAM Role  Usage module \u0026#34;example\u0026#34; { source = \u0026#34;git::https://gitlab.com/infraprints/modules/aws/iam-external-role\u0026#34; name = \u0026#34;infraprints-iam-external-role\u0026#34; external_id = \u0026#34;TXAiS9rfgQghzWW2\u0026#34; role_arn = [\u0026#34;${aws_iam_role.default.arn}\u0026#34;] count = \u0026#34;1\u0026#34; } resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;default\u0026#34; { name = \u0026#34;infraprints-ec2-role\u0026#34; assume_role_policy = \u0026lt;\u0026lt;EOF { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;ec2.amazonaws.com\u0026#34; }, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, } ] } EOF } Examples  Basic Example ExternalID Example Multiple roles Example  Notes  The count property is required as a constant as a workaround to a Terraform issue.  Inputs    Name Description Type Default Required     count The number of principal entities. string n/a yes   description The description of the role. string \u0026quot;\u0026quot; no   external_id External Identifier set on the role. string \u0026quot;\u0026quot; no   force_detach_policies Specifies to force detaching any policies the role has before destroying it. Defaults to false. string \u0026quot;true\u0026quot; no   max_session_duration The maximum session duration (in seconds) that you want to set for the specified role. If you do not specify a value for this setting, the default maximum of one hour is applied. This setting can have a value from 1 hour to 12 hours. string \u0026quot;3600\u0026quot; no   name The name of the role. string n/a yes   path The path to the role. See IAM Identifiers for more information. string \u0026quot;/external/\u0026quot; no   permissions_boundary The ARN of the policy that is used to set the permissions boundary for the role. string \u0026quot;\u0026quot; no   role_arn The list of principal entities that is allowed to assume the role. list n/a yes   tags Key-value mapping of tags for the IAM role. map \u0026lt;map\u0026gt; no    Outputs    Name Description     arn The Amazon Resource Name (ARN) specifying the role.   create_date The Amazon Resource Name (ARN) specifying the role.   unique_id The Amazon Resource Name (ARN) specifying the role.    ","id":78,"section":"posts","summary":"AWS IAM External Role Terraform module for describing an IAM role responsible for delegating cross-account access.\nThese types of resources are supported:\n IAM Role  Usage module \u0026#34;example\u0026#34; { source = \u0026#34;git::https://gitlab.com/infraprints/modules/aws/iam-external-role\u0026#34; name = \u0026#34;infraprints-iam-external-role\u0026#34; external_id = \u0026#34;TXAiS9rfgQghzWW2\u0026#34; role_arn = [\u0026#34;${aws_iam_role.default.arn}\u0026#34;] count = \u0026#34;1\u0026#34; } resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;default\u0026#34; { name = \u0026#34;infraprints-ec2-role\u0026#34; assume_role_policy = \u0026lt;\u0026lt;EOF { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;ec2.amazonaws.com\u0026#34; }, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, } ] } EOF } Examples  Basic Example ExternalID Example Multiple roles Example  Notes  The count property is required as a constant as a workaround to a Terraform issue.","tags":["org:infraprints"],"title":"terraform-aws-iam-external-role","uri":"/2019/11/infraprints-terraform-aws-iam-external-role/","year":"2019"},{"content":"","id":79,"section":"posts","summary":"","tags":["org:infraprints"],"title":"readme","uri":"/2019/11/infraprints-readme/","year":"2019"},{"content":"Terraform Resource ID Construct a formatted name for a Terraform resource.\n","id":80,"section":"posts","summary":"Terraform Resource ID Construct a formatted name for a Terraform resource.","tags":["org:infraprints"],"title":"terraform-id","uri":"/2019/11/infraprints-terraform-id/","year":"2019"},{"content":"Terraform Netlify Gitlab CI/CD Terraform module which creates a site on Netlify with the necessary variables for GitLab CI deployments.\nThese types of resources are supported:\n Netlify Site  Usage module \u0026#34;cloudability\u0026#34; { source = \u0026#34;git::https://gitlab.com/infraprints/modules/netlify/gitlab-cicd.git?ref=master\u0026#34; name = \u0026#34;my-netlify-website\u0026#34; custom_domain = \u0026#34;www.example.com\u0026#34; project = \u0026#34;01234567\u0026#34; } Examples  Basic Example  Notes  Three GitLab CI Environment Variables (NETLIFY_SITE_ID, NETLIFY_NAME, NETLIFY_CUSTOM_DOMAIN)  Inputs    Name Description Type Default Required     name A friendly name for the netlify site. string - yes   custom_domain FQDN built using the zone domain and name. string - yes   project The integer that uniquely identifies the project within the gitlab install. string - yes    Outputs    Name Description     id The unique identifier.   name Name of your site on netlify.   custom_domain A custom domain name, must be configured using a cname in accordance with netlify\u0026rsquo;s docs.    ","id":81,"section":"posts","summary":"Terraform Netlify Gitlab CI/CD Terraform module which creates a site on Netlify with the necessary variables for GitLab CI deployments.\nThese types of resources are supported:\n Netlify Site  Usage module \u0026#34;cloudability\u0026#34; { source = \u0026#34;git::https://gitlab.com/infraprints/modules/netlify/gitlab-cicd.git?ref=master\u0026#34; name = \u0026#34;my-netlify-website\u0026#34; custom_domain = \u0026#34;www.example.com\u0026#34; project = \u0026#34;01234567\u0026#34; } Examples  Basic Example  Notes  Three GitLab CI Environment Variables (NETLIFY_SITE_ID, NETLIFY_NAME, NETLIFY_CUSTOM_DOMAIN)  Inputs    Name Description Type Default Required     name A friendly name for the netlify site.","tags":["org:infraprints"],"title":"terraform-gitlab-netlify-cicd","uri":"/2019/11/infraprints-terraform-gitlab-netlify-cicd/","year":"2019"},{"content":"Website The work-in-progress stub for the infraprints website.\n","id":82,"section":"posts","summary":"Website The work-in-progress stub for the infraprints website.","tags":["org:infraprints"],"title":"website","uri":"/2019/11/infraprints-website/","year":"2019"},{"content":"Proposals Experimenting with the underlying infrastructure for a GitHub based proposals mechanisms that deploys to a web resource (website/subpage/etc)\nMotivations  Automation to ensure contribution guidelines are both accessible and followed (e.g. first contribution bot, linting) Contribution previews, either by links to markdown rendering or branch previews Support for supplemental resources (e.g. assets/list-of-items.csv) Methods to organize the proposals better (date? slugs? IDs?) Simple layout in code (minimal overhead), with tooling to move assets into the correct locations When working with code, the proposals have some organization mechanism Minimal barriers when trying to write a proposal  Notes Using date-based directory organization helps for filtering based on the latest proposals. If you are looking for the proposals in the most recent year, its fairly easy to complete that query (ls designs/2019/). I find that this falls apart when navigating the history for proposals related to a project. I could enforce directory naming standards, but I feel that makes the repository difficult to approach. If I want all the proposals related to a certain project, I\u0026rsquo;d need to use tools like jq or grep.\nA model that involves namespaces paired with status directories (archived/etc) would help resolve these issues. My concern with this though is the solution becomes over-engineered and closely connected to the concept of directories. I think the \u0026lsquo;Namespaces and Status\u0026rsquo; example really shows how it can become difficult to work with when sticking with tree structure for organization.\nSince each proposal is a self-contained package with metadata, the proposals can be moved freely about the file system. From this, I\u0026rsquo;d say that the best solution is to just use namespace slugs for directory names. Each proposal will have metadata in the form of the DESIGN schema, so the post-processing stage can organize it by that.\nThis repository does a simple example using mkdocs to create a website hosting the proposals. The proposals are put in hardcoded locations to avoid the need for a post-processing step.\nNamespaces and Status \u0026gt; designs/ \u0026gt; project1/ \u0026gt; sub1/ \u0026gt; remove-items-from/ \u0026gt; README.md \u0026gt; assets/ \u0026gt; diagram.png \u0026gt; establish-baseline/ \u0026gt; README.md \u0026gt; supplementary/ \u0026gt; NOTES.md \u0026gt; sub2/ \u0026gt; remove-items-from/ \u0026gt; README.md \u0026gt; project2/ \u0026gt; sandbox-components-by/ \u0026gt; README.md \u0026gt; replace-grpc-with/ \u0026gt; README.md \u0026gt; archived/ \u0026gt; 2019 \u0026gt; project1/ \u0026gt; establish-project1-for/ \u0026gt; README.md \u0026gt; 2018 \u0026gt; project2/ \u0026gt; establish-project2-for/ \u0026gt; README.md Directory scoping \u0026gt; xyz/ \u0026gt; README.md \u0026gt; DESIGN \u0026gt; assets/ \u0026gt; diagram.png \u0026gt; architecture.svg \u0026gt; supplmentary/ \u0026gt; NOTES.md \u0026gt; ESTIMATES.md ","id":83,"section":"posts","summary":"Proposals Experimenting with the underlying infrastructure for a GitHub based proposals mechanisms that deploys to a web resource (website/subpage/etc)\nMotivations  Automation to ensure contribution guidelines are both accessible and followed (e.g. first contribution bot, linting) Contribution previews, either by links to markdown rendering or branch previews Support for supplemental resources (e.g. assets/list-of-items.csv) Methods to organize the proposals better (date? slugs? IDs?) Simple layout in code (minimal overhead), with tooling to move assets into the correct locations When working with code, the proposals have some organization mechanism Minimal barriers when trying to write a proposal  Notes Using date-based directory organization helps for filtering based on the latest proposals.","tags":["org:jrbeverly"],"title":"proposals-concept","uri":"/2019/10/jrbeverly-proposals-concept/","year":"2019"},{"content":"Experimenting with CircleCI Orbs Experimenting with CircleCI Orbs for reducing code re-use in templates\nUsages cp Deploy to S3\norbs: awscli: jrbeverly/awscli@0.0.5 version: 2.1 workflows: Deploy to S3: jobs: - awscli/aws-copy-to: bucket: hello-bucket namespace: some/terraform source: some/path sync Deploy to S3\norbs: awscli: jrbeverly/awscli@0.0.5 version: 2.1 workflows: Deploy to S3: jobs: - awscli/aws-sync-to: bucket: hello-bucket namespace: some/terraform ","id":84,"section":"posts","summary":"Experimenting with CircleCI Orbs Experimenting with CircleCI Orbs for reducing code re-use in templates\nUsages cp Deploy to S3\norbs: awscli: jrbeverly/awscli@0.0.5 version: 2.1 workflows: Deploy to S3: jobs: - awscli/aws-copy-to: bucket: hello-bucket namespace: some/terraform source: some/path sync Deploy to S3\norbs: awscli: jrbeverly/awscli@0.0.5 version: 2.1 workflows: Deploy to S3: jobs: - awscli/aws-sync-to: bucket: hello-bucket namespace: some/terraform ","tags":["org:jrbeverly"],"title":"exp-circleci-orbs","uri":"/2019/10/jrbeverly-exp-circleci-orbs/","year":"2019"},{"content":"Docker image for MarkdownLint A tool to check markdown files and flag style issues. To have markdownlint check your markdown files, simply run mdl with the filenames as a parameter:\nmdl README.md Markdownlint can also take a directory, and it will scan all markdown files within the directory (and nested directories):\nmdl docs/ You can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/markdownlint Test interactively docker run -it cardboardci/markdownlint /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/markdownlint aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/markdownlint aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/markdownlint:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/markdownlint:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/markdownlint:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/markdownlint:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":85,"section":"posts","summary":"Docker image for MarkdownLint A tool to check markdown files and flag style issues. To have markdownlint check your markdown files, simply run mdl with the filenames as a parameter:\nmdl README.md Markdownlint can also take a directory, and it will scan all markdown files within the directory (and nested directories):\nmdl docs/ You can see the cli reference here.\nUsage You can run awscli to manage your AWS services.","tags":["org:cardboardci"],"title":"docker-markdownlint","uri":"/2019/10/cardboardci-docker-markdownlint/","year":"2019"},{"content":"CI Core CI-core is a special Docker image that is configured for running in CI environments. It is Ubuntu, with:\n A docker user A directory workspace Mechanisms for running build tools (json, web requests, etc)  ","id":86,"section":"posts","summary":"CI Core CI-core is a special Docker image that is configured for running in CI environments. It is Ubuntu, with:\n A docker user A directory workspace Mechanisms for running build tools (json, web requests, etc)  ","tags":["org:cardboardci"],"title":"docker-ci-core","uri":"/2019/10/cardboardci-docker-ci-core/","year":"2019"},{"content":"Docker image for CppCheck Cppcheck is an analysis tool for C/C++ code. It provides unique code analysis to detect bugs and focuses on detecting undefined behaviour and dangerous coding constructs. The goal is to detect only real errors in the code (i.e. have very few false positives).\nYou can see the source repository here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/awscli Test interactively docker run -it cardboardci/awscli /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/awscli aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/awscli aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/awscli:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/awscli:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/awscli:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/awscli:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":87,"section":"posts","summary":"Docker image for CppCheck Cppcheck is an analysis tool for C/C++ code. It provides unique code analysis to detect bugs and focuses on detecting undefined behaviour and dangerous coding constructs. The goal is to detect only real errors in the code (i.e. have very few false positives).\nYou can see the source repository here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.","tags":["org:cardboardci"],"title":"docker-cppcheck","uri":"/2019/10/cardboardci-docker-cppcheck/","year":"2019"},{"content":"Docker image for AWS CLI A command line client for Dropbox built using the Go SDK\n Supports basic file operations like ls, cp, mkdir, mv (via the Files API) Supports search Supports file revisions and file restore Chunked uploads for large files, paginated listing for large directories Supports a growing set of Team operations  You can see the source repository here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/awscli Test interactively docker run -it cardboardci/awscli /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/awscli aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/awscli aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/awscli:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/awscli:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/awscli:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/awscli:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":88,"section":"posts","summary":"Docker image for AWS CLI A command line client for Dropbox built using the Go SDK\n Supports basic file operations like ls, cp, mkdir, mv (via the Files API) Supports search Supports file revisions and file restore Chunked uploads for large files, paginated listing for large directories Supports a growing set of Team operations  You can see the source repository here.\nUsage You can run awscli to manage your AWS services.","tags":["org:cardboardci"],"title":"docker-dbxcli","uri":"/2019/10/cardboardci-docker-dbxcli/","year":"2019"},{"content":"Docker image for HTMLHint The static code analysis tool you need for your HTML.\nYou can see the source repository here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/htmlhint Test interactively docker run -it cardboardci/htmlhint /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/htmlhint aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/htmlhint aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/htmlhint:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/htmlhint:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/htmlhint:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/htmlhint:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":89,"section":"posts","summary":"Docker image for HTMLHint The static code analysis tool you need for your HTML.\nYou can see the source repository here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/htmlhint Test interactively docker run -it cardboardci/htmlhint /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/htmlhint aws s3 cp file.","tags":["org:cardboardci"],"title":"docker-htmlhint","uri":"/2019/10/cardboardci-docker-htmlhint/","year":"2019"},{"content":"Docker image for LuaCheck Luacheck is a static analyzer and a linter for Lua. Luacheck detects various issues such as usage of undefined global variables, unused variables and values, accessing uninitialized variables, unreachable code and more. Most aspects of checking are configurable: there are options for defining custom project-related globals, for selecting set of standard globals (version of Lua standard library), for filtering warnings by type and name of related variable, etc. The options can be used on the command line, put into a config or directly into checked files as Lua comments.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/luacheck Test interactively docker run -it cardboardci/luacheck /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/luacheck aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/luacheck aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/luacheck:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/luacheck:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/luacheck:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/luacheck:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":90,"section":"posts","summary":"Docker image for LuaCheck Luacheck is a static analyzer and a linter for Lua. Luacheck detects various issues such as usage of undefined global variables, unused variables and values, accessing uninitialized variables, unreachable code and more. Most aspects of checking are configurable: there are options for defining custom project-related globals, for selecting set of standard globals (version of Lua standard library), for filtering warnings by type and name of related variable, etc.","tags":["org:cardboardci"],"title":"docker-luacheck","uri":"/2019/10/cardboardci-docker-luacheck/","year":"2019"},{"content":"Docker image for PyLint Pylint is a Python static code analysis tool which looks for programming errors, helps enforcing a coding standard, sniffs for code smells and offers simple refactoring suggestions.\nIt\u0026rsquo;s highly configurable, having special pragmas to control its errors and warnings from within your code, as well as from an extensive configuration file. It is also possible to write your own plugins for adding your own checks or for extending pylint in one way or another.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/pylint Test interactively docker run -it cardboardci/pylint /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/pylint aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/pylint aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/pylint:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/pylint:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/pylint:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/pylint:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":91,"section":"posts","summary":"Docker image for PyLint Pylint is a Python static code analysis tool which looks for programming errors, helps enforcing a coding standard, sniffs for code smells and offers simple refactoring suggestions.\nIt\u0026rsquo;s highly configurable, having special pragmas to control its errors and warnings from within your code, as well as from an extensive configuration file. It is also possible to write your own plugins for adding your own checks or for extending pylint in one way or another.","tags":["org:cardboardci"],"title":"docker-pylint","uri":"/2019/10/cardboardci-docker-pylint/","year":"2019"},{"content":"Docker image for Render SVGs A utility to render Scalable Vector Graphics (SVG), associated with the GNOME Project. It renders SVG files to Cairo surfaces. Cairo is the 2D, antialiased drawing library that GNOME uses to draw things to the screen or to generate output for printing.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/rsvg Test interactively docker run -it cardboardci/rsvg /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/rsvg aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/rsvg aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/rsvg:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/rsvg:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/rsvg:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/rsvg:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":92,"section":"posts","summary":"Docker image for Render SVGs A utility to render Scalable Vector Graphics (SVG), associated with the GNOME Project. It renders SVG files to Cairo surfaces. Cairo is the 2D, antialiased drawing library that GNOME uses to draw things to the screen or to generate output for printing.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.","tags":["org:cardboardci"],"title":"docker-rsvg","uri":"/2019/10/cardboardci-docker-rsvg/","year":"2019"},{"content":"Docker image for Rubocop RuboCop is a Ruby static code analyzer and code formatter. Out of the box it will enforce many of the guidelines outlined in the community Ruby Style Guide.\nRuboCop is extremely flexible and most aspects of its behavior can be tweaked via various configuration options.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/rubocop Test interactively docker run -it cardboardci/rubocop /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/rubocop aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/rubocop aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/rubocop:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/rubocop:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/rubocop:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/rubocop:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":93,"section":"posts","summary":"Docker image for Rubocop RuboCop is a Ruby static code analyzer and code formatter. Out of the box it will enforce many of the guidelines outlined in the community Ruby Style Guide.\nRuboCop is extremely flexible and most aspects of its behavior can be tweaked via various configuration options.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.","tags":["org:cardboardci"],"title":"docker-rubocop","uri":"/2019/10/cardboardci-docker-rubocop/","year":"2019"},{"content":"Docker image for StyleLint A mighty, modern linter that helps you avoid errors and enforce conventions in your styles.\nIt\u0026rsquo;s mighty because it:\n understands the latest CSS syntax including custom properties and level 4 selectors extracts embedded styles from HTML, markdown and CSS-in-JS object \u0026amp; template literals parses CSS-like syntaxes like SCSS, Sass, Less and SugarSS has over 170 built-in rules to catch errors, apply limits and enforce stylistic conventions supports plugins so you can create your own rules or make use of plugins written by the community automatically fixes some violations (experimental feature) is well tested with over 10000 unit tests supports shareable configs that you can extend or create your own of is unopinionated so you can tailor the linter to your exact needs  You can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/stylelint Test interactively docker run -it cardboardci/stylelint /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/stylelint aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/stylelint aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/stylelint:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/stylelint:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/stylelint:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/stylelint:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":94,"section":"posts","summary":"Docker image for StyleLint A mighty, modern linter that helps you avoid errors and enforce conventions in your styles.\nIt\u0026rsquo;s mighty because it:\n understands the latest CSS syntax including custom properties and level 4 selectors extracts embedded styles from HTML, markdown and CSS-in-JS object \u0026amp; template literals parses CSS-like syntaxes like SCSS, Sass, Less and SugarSS has over 170 built-in rules to catch errors, apply limits and enforce stylistic conventions supports plugins so you can create your own rules or make use of plugins written by the community automatically fixes some violations (experimental feature) is well tested with over 10000 unit tests supports shareable configs that you can extend or create your own of is unopinionated so you can tailor the linter to your exact needs  You can see the cli reference here.","tags":["org:cardboardci"],"title":"docker-stylelint","uri":"/2019/10/cardboardci-docker-stylelint/","year":"2019"},{"content":"CardboardCI A collection of docker images that provide a common core for use in continuous integration.\nThe idea of these images is to balance the following:\n Frequency of updates Standard set of tooling Common Environment  Background Notes I encountered a lot of difficult with trying to automate the updating of those docker images. GitLab does not really have the idea of bots. Instead it would require a personal access token for my account, if I wanted to make commits. I didn\u0026rsquo;t like, as that would grant access to pretty much every one of my repositories. At the time I was self-hosting Gitlab, so I opted to just create additional users that would be my bots.\nHaving a bot user worked pretty well, in the sense that a scheduled Gitlab repository would execute the updates. However, at that time the building of each image was a burden on my local self-hosted CI machine. When I opted to use Gitlab.com instead of self-hosted, I ended up abandoning the idea of automated upgrading.\nLater on when I was doing some more work in the frontend realm, I found that I didn\u0026rsquo;t like working with the available docker images. They fit into a couple categories:\n Very big Dependencies I did not like/want Slow update schedule  That is when I decided to move my docker images into a separate project called CardboardCI. The hope was a set of small images, with automated updates and a standard set of dependencies (jq/zip/tar/etc). These images were based on alpine, until ubuntu released minimal images (~35MB). At that time I started the process of moving them over to ubuntu.\nGitHub Actions When GitHub Actions became available, I noticed that you would be able to use the GITHUB_TOKEN to commit against the repository itself. This meant you could define a workflow that checked for updates, and if existed, create a pull request with those updates.\nIn essence, you would be able to roll your own dependabot. I thought this would be ideal for CardboardCI, and moved it from GitLab to GitHub.\n","id":95,"section":"posts","summary":"CardboardCI A collection of docker images that provide a common core for use in continuous integration.\nThe idea of these images is to balance the following:\n Frequency of updates Standard set of tooling Common Environment  Background Notes I encountered a lot of difficult with trying to automate the updating of those docker images. GitLab does not really have the idea of bots. Instead it would require a personal access token for my account, if I wanted to make commits.","tags":["org:cardboardci"],"title":"readme","uri":"/2019/10/cardboardci-readme/","year":"2019"},{"content":"Bazel CSharp Rules Examples Overview This repository provides a set of usages for the bazel csharp rules. The idea behind these examples is to cover edge cases that are encountered during development, and provide a comprehensive test (\u0026amp; prototype) suite.\nWhile working on the bazel csharp rules, I have encountered bugs or small quirks that I would like to encode records of. Some of these are very minor details, so I felt it would work best to have them as an external repository.\nI defined the idea behind each program here.\nSetup If you\u0026rsquo;d like to test the rules in your own repository, you can add the following to your WORKSPACE file to add the external repositories:\nload(\u0026#34;@bazel_tools//tools/build_defs/repo:http.bzl\u0026#34;, \u0026#34;http_archive\u0026#34;) http_archive( name = \u0026#34;d2l_rules_csharp\u0026#34;, strip_prefix = \u0026#34;rules_csharp-0.6\u0026#34;, urls = [\u0026#34;https://github.com/Brightspace/rules_csharp/archive/v0.6.tar.gz\u0026#34;], ) load( \u0026#34;@d2l_rules_csharp//csharp:defs.bzl\u0026#34;, \u0026#34;csharp_register_toolchains\u0026#34;, \u0026#34;csharp_repositories\u0026#34;, ) csharp_repositories() csharp_register_toolchains() Or you can consult the minimal example in usage/.\n","id":96,"section":"posts","summary":"Bazel CSharp Rules Examples Overview This repository provides a set of usages for the bazel csharp rules. The idea behind these examples is to cover edge cases that are encountered during development, and provide a comprehensive test (\u0026amp; prototype) suite.\nWhile working on the bazel csharp rules, I have encountered bugs or small quirks that I would like to encode records of. Some of these are very minor details, so I felt it would work best to have them as an external repository.","tags":["org:jrbeverly"],"title":"bazel-csharp-testcases","uri":"/2019/10/jrbeverly-bazel-csharp-testcases/","year":"2019"},{"content":"Packer with GitHub Actions Experimenting with GitHub Actions for building machine images with Packer. Ideally trying to figure out what it takes for building the following on GitHub Actions:\n VirtualBox ISO Hyper-V ISO Docker Image  Docker Image A base case. I want to confirm that I am in fact able to use packer without issue on GitHub Actions.\nHyperV I have had to do some work with Windows containers recently, and have found them to have performance issues for disk intense work. I was curious, if some of the provisioning scripts for the docker container could be used in a HyperV machine. That would allow for some performance experimentation between the two types.\nVirtualBox One of my previous projects involved using Vagrant to provision desktop environments. The problem with those environments is that the provisioning process was not hermetic. Building the same commit a month apart could result in significantly different images, or even failures. This was not ideal, so I wanted to investigate building images (ISOs) and using the Vagrantfile for environment configuration (e.g. ISO = environment, Vagrantfile = how you connect to this environment).\nAt the time I didn\u0026rsquo;t like the available options for CI/CD, so decided to shelve the idea for now. With GitHub Actions, I\u0026rsquo;m curious if I would be able to build these images, then deploy them to something like VagrantCloud.\n","id":97,"section":"posts","summary":"Packer with GitHub Actions Experimenting with GitHub Actions for building machine images with Packer. Ideally trying to figure out what it takes for building the following on GitHub Actions:\n VirtualBox ISO Hyper-V ISO Docker Image  Docker Image A base case. I want to confirm that I am in fact able to use packer without issue on GitHub Actions.\nHyperV I have had to do some work with Windows containers recently, and have found them to have performance issues for disk intense work.","tags":["org:devkitspaces"],"title":"packer-on-github","uri":"/2019/10/devkitspaces-packer-on-github/","year":"2019"},{"content":"Viewdocs Autodoc This was an early concept I was working with for converting metadata files (json/yml) into standard README markdown files using simple bash and templates. Ultimately I did not go in this direction, as I found that I was not making the best use of the customization yielded from using with bash + templates. A simple templating engine provided all the basics that I was ultimately after.\nBelow I have described some of the template concepts I was exploring at the time:\nIcon Acknowledgements I like to use icons from thenounproject.com, which I include an acknowledgements file (icon.json) in projects. I was looking to take this file, and convert it into a readable markdown file. Additionally I was looking at some simple scripts for generating the icons across all projects.\nBriefs I have been exploring having a repository.{json|yml} metadata file in all repositories. This would include all the header information you see on GitHub / GitLab, such as description, icon, social preview, tags, etc. The template would involve taking this data and converting it into a simple README.md that could be included in a list/aggregate site.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by MICHAEL G BROWN from the Noun Project.\n","id":98,"section":"posts","summary":"Viewdocs Autodoc This was an early concept I was working with for converting metadata files (json/yml) into standard README markdown files using simple bash and templates. Ultimately I did not go in this direction, as I found that I was not making the best use of the customization yielded from using with bash + templates. A simple templating engine provided all the basics that I was ultimately after.\nBelow I have described some of the template concepts I was exploring at the time:","tags":["org:jrbeverly"],"title":"viewdocs-autodoc","uri":"/2019/10/jrbeverly-viewdocs-autodoc/","year":"2019"},{"content":"Awesome Terraform Prototype An experiment using mkdocs and a series of json/yml files to define an awesome list.\nThe markdown files are automatically generated from the yml files that define each element of the list (tags/metadata/etc). These files are then piped into mkdocs, which yields a material theme website for the project.\n","id":99,"section":"posts","summary":"Awesome Terraform Prototype An experiment using mkdocs and a series of json/yml files to define an awesome list.\nThe markdown files are automatically generated from the yml files that define each element of the list (tags/metadata/etc). These files are then piped into mkdocs, which yields a material theme website for the project.","tags":["org:jrbeverly"],"title":"awesome-terraform-prototype","uri":"/2019/10/jrbeverly-awesome-terraform-prototype/","year":"2019"},{"content":"Minimizing Docker Builds Summary A GitlabCI repository designed to experiment with potential avenues for minimizing the size of a build image.\nNotes  Experiment with flags from docker:1.13 for minimizing the process Compress the build-context that is sent to the daemon (compress) Squash the layers using the --squash flag Experiment with the tool docker-slim for shrinking the build image Issues with permissions on the tooling are preventing deployment in Gitlab  ","id":100,"section":"posts","summary":"Minimizing Docker Builds Summary A GitlabCI repository designed to experiment with potential avenues for minimizing the size of a build image.\nNotes  Experiment with flags from docker:1.13 for minimizing the process Compress the build-context that is sent to the daemon (compress) Squash the layers using the --squash flag Experiment with the tool docker-slim for shrinking the build image Issues with permissions on the tooling are preventing deployment in Gitlab  ","tags":["org:cardboardci"],"title":"slim-docker-with-gitlab","uri":"/2019/10/cardboardci-slim-docker-with-gitlab/","year":"2019"},{"content":"SVG Icon Processing Summary Experiment with programmatically generating color variants for SVG files using a JSON definition file with the source SVG.\nUsage The variants of each of the files is defined as a dictionary (string:object). The key for the dictionary matches the name of the variant. The object defines a collection of id and properties. These will be merged into the SVG to generate the variant icon. An example definition file is included below.\n{ \u0026#34;name\u0026#34;: \u0026#34;photo\u0026#34;, \u0026#34;variants\u0026#34;: { \u0026#34;pink\u0026#34;: { \u0026#34;outline\u0026#34;: { \u0026#34;stroke\u0026#34;: \u0026#34;#E54E7A\u0026#34; }, \u0026#34;outline_bg\u0026#34;: { \u0026#34;stroke\u0026#34;: \u0026#34;#E54E7A\u0026#34; }, \u0026#34;land\u0026#34;: { \u0026#34;fill\u0026#34;: \u0026#34;#E54E7A\u0026#34; }, \u0026#34;sun\u0026#34;: { \u0026#34;fill\u0026#34;: \u0026#34;#E54E7A\u0026#34; } }, \u0026#34;default\u0026#34;: { \u0026#34;outline\u0026#34;: { \u0026#34;stroke\u0026#34;: \u0026#34;#B35C00\u0026#34; }, \u0026#34;outline_bg\u0026#34;: { \u0026#34;stroke\u0026#34;: \u0026#34;#B35C00\u0026#34; }, \u0026#34;land\u0026#34;: { \u0026#34;fill\u0026#34;: \u0026#34;#3BB300\u0026#34; }, \u0026#34;sun\u0026#34;: { \u0026#34;fill\u0026#34;: \u0026#34;#FFF200\u0026#34; } } } } The above definition will generate two variants (named pink \u0026amp; default). For the pink variant, the list of SVG elements that match the IDs (outline, outline_bg, land, sun) will have the defined properties merged. In this case you would have something like:\n\u0026lt;circle id=\u0026#34;sun\u0026#34; cx=\u0026#34;23.3\u0026#34; cy=\u0026#34;43\u0026#34; r=\u0026#34;4.8\u0026#34;\u0026gt;\u0026lt;/circle\u0026gt; That will add the attribute (fill=#E54E7A) to the SVG element.\n\u0026lt;circle id=\u0026#34;sun\u0026#34; cx=\u0026#34;23.3\u0026#34; cy=\u0026#34;43\u0026#34; r=\u0026#34;4.8\u0026#34; fill=\u0026#34;#E54E7A\u0026#34;/\u0026gt; ","id":101,"section":"posts","summary":"SVG Icon Processing Summary Experiment with programmatically generating color variants for SVG files using a JSON definition file with the source SVG.\nUsage The variants of each of the files is defined as a dictionary (string:object). The key for the dictionary matches the name of the variant. The object defines a collection of id and properties. These will be merged into the SVG to generate the variant icon. An example definition file is included below.","tags":["org:jrbeverly"],"title":"exp-svg-icon-processing","uri":"/2019/10/jrbeverly-exp-svg-icon-processing/","year":"2019"},{"content":"BullsEye Experimentation Summary Experiment with BullsEye for building command-driven tooling (build-systems).\nUsage Experimenting with using BullsEye in a dotnet project. BullsEye doesn\u0026rsquo;t handle parsing of command line arguments, instead recommends using a tool for parsing them.\nvar app = new CommandLineApplication(throwOnUnexpectedArg: false); var foo = app.Option\u0026lt;string\u0026gt;(\u0026#34;--foo\u0026#34;, \u0026#34;foo\u0026#34;, CommandOptionType.SingleValue); BullsEye can then be used to built a higher level build system for languages (terraform, docker, etc).\nNotes  Creates common build-systems for templates (terraform-module, docker image) Auto-generate the console apps (BullsEye, CommandLineApplication) from a definition Define the system, then generate interfaces (service, cli, client, etc)  ","id":102,"section":"posts","summary":"BullsEye Experimentation Summary Experiment with BullsEye for building command-driven tooling (build-systems).\nUsage Experimenting with using BullsEye in a dotnet project. BullsEye doesn\u0026rsquo;t handle parsing of command line arguments, instead recommends using a tool for parsing them.\nvar app = new CommandLineApplication(throwOnUnexpectedArg: false); var foo = app.Option\u0026lt;string\u0026gt;(\u0026#34;--foo\u0026#34;, \u0026#34;foo\u0026#34;, CommandOptionType.SingleValue); BullsEye can then be used to built a higher level build system for languages (terraform, docker, etc).\nNotes  Creates common build-systems for templates (terraform-module, docker image) Auto-generate the console apps (BullsEye, CommandLineApplication) from a definition Define the system, then generate interfaces (service, cli, client, etc)  ","tags":["org:jrbeverly"],"title":"bullseye-exp","uri":"/2019/10/jrbeverly-bullseye-exp/","year":"2019"},{"content":"Vagrant Continuous Integration Prototype Pre-built Vagrant Box: -vagrant init ubuntu/trusty64\nThis example vagrant configuration installs and configures Ubuntu Trusty using simple Ruby scripts.\nThe objective is to move as much of the Vagrantfile configuration into external ruby scripts. These scripts could then be split into testable functions and modules. Currently the focus is on providing a consistent installation process using shellcheck and rubocop.\nAiming to have a simple vagrant example in this repository, and then use GitLab CI to perform linting on the provisioning scripts (shell+ruby).\nRequirements The following software must be installed/present on your local machine before you can use Vagrant to build the virtual machine:\n Vagrant VirtualBox  Usage Make sure all the required software (listed above) is installed, then cd to the directory containing this README.md file, and run:\nvagrant up  After a few minutes, Vagrant should tell you the machine was generated successfully.\n","id":103,"section":"posts","summary":"Vagrant Continuous Integration Prototype Pre-built Vagrant Box: -vagrant init ubuntu/trusty64\nThis example vagrant configuration installs and configures Ubuntu Trusty using simple Ruby scripts.\nThe objective is to move as much of the Vagrantfile configuration into external ruby scripts. These scripts could then be split into testable functions and modules. Currently the focus is on providing a consistent installation process using shellcheck and rubocop.\nAiming to have a simple vagrant example in this repository, and then use GitLab CI to perform linting on the provisioning scripts (shell+ruby).","tags":["org:devkitspaces"],"title":"vagrant-cicd","uri":"/2019/10/devkitspaces-vagrant-cicd/","year":"2019"},{"content":"GitHub Learning Lab - Lab Starter Noticed this when working with GitHub Actions that you can have a automated \u0026lsquo;teacher\u0026rsquo; by using linting + GitHub Bot. Thought this was an interesting idea, and have started this repository to get a better understanding of how this actually works.\nWith GitHub now having its own CI/CD Pipeline, it may be possible to create all sorts of tutorials for setting up applications. The first one that comes to mind is gamedev with something like Godot. Since a written tutorial is more static, this would allow a learner to follow a structure process for writing their own game, while having step by step linting to ensure that nothing went off the guard-rails.\nPotential concern would be how the linting would handle a learner that introduced unexpected aspects to the code itself. For example, if the code is testing the output from the program, what if something like Console.WriteLine(\u0026quot;mydebuggingcode\u0026quot;) is present, would that be a failure? How should that best be handled? Etc.\n","id":104,"section":"posts","summary":"GitHub Learning Lab - Lab Starter Noticed this when working with GitHub Actions that you can have a automated \u0026lsquo;teacher\u0026rsquo; by using linting + GitHub Bot. Thought this was an interesting idea, and have started this repository to get a better understanding of how this actually works.\nWith GitHub now having its own CI/CD Pipeline, it may be possible to create all sorts of tutorials for setting up applications. The first one that comes to mind is gamedev with something like Godot.","tags":["org:jrbeverly"],"title":"lab-starter","uri":"/2019/09/jrbeverly-lab-starter/","year":"2019"},{"content":"Welcome to \u0026ldquo;Hello World\u0026rdquo; with GitHub Actions This course will walk you through writing your first action and using it with a workflow file.\nReady to get started? Navigate to the first issue.\n","id":105,"section":"posts","summary":"Welcome to \u0026ldquo;Hello World\u0026rdquo; with GitHub Actions This course will walk you through writing your first action and using it with a workflow file.\nReady to get started? Navigate to the first issue.","tags":["org:jrbeverly"],"title":"hello-github-actions","uri":"/2019/09/jrbeverly-hello-github-actions/","year":"2019"},{"content":"Docker image for WkHtmlToPDF wkhtmltopdf and wkhtmltoimage are open source (LGPLv3) command line tools to render HTML into PDF and various image formats using the Qt WebKit rendering engine. These run entirely \u0026ldquo;headless\u0026rdquo; and do not require a display or display service.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/wkhtmltopdf Test interactively docker run -it cardboardci/wkhtmltopdf /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/wkhtmltopdf aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/wkhtmltopdf aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/wkhtmltopdf:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/wkhtmltopdf:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/wkhtmltopdf:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/wkhtmltopdf:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":106,"section":"posts","summary":"Docker image for WkHtmlToPDF wkhtmltopdf and wkhtmltoimage are open source (LGPLv3) command line tools to render HTML into PDF and various image formats using the Qt WebKit rendering engine. These run entirely \u0026ldquo;headless\u0026rdquo; and do not require a display or display service.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.","tags":["org:cardboardci"],"title":"docker-wkhtmltopdf","uri":"/2019/03/cardboardci-docker-wkhtmltopdf/","year":"2019"},{"content":"Docker image for SVG Tools SVG Tools are a collection of tools for working with vector graphics.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/svgtools Test interactively docker run -it cardboardci/svgtools /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/svgtools aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/svgtools aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/svgtools:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/svgtools:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/svgtools:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/svgtools:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":107,"section":"posts","summary":"Docker image for SVG Tools SVG Tools are a collection of tools for working with vector graphics.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/svgtools Test interactively docker run -it cardboardci/svgtools /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/svgtools aws s3 cp file.","tags":["org:cardboardci"],"title":"docker-svgtools","uri":"/2019/03/cardboardci-docker-svgtools/","year":"2019"},{"content":"Docker image for Surge This is the CLI client for the surge.sh hosted service. Its what gets installed when you run npm install -g surge.\nThis CLI library manages access tokens locally and handles the upload and subsequent reporting when you publish a project using surge.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/surge Test interactively docker run -it cardboardci/surge /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/surge aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/surge aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/surge:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/surge:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/surge:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/surge:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":108,"section":"posts","summary":"Docker image for Surge This is the CLI client for the surge.sh hosted service. Its what gets installed when you run npm install -g surge.\nThis CLI library manages access tokens locally and handles the upload and subsequent reporting when you publish a project using surge.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.","tags":["org:cardboardci"],"title":"docker-surge","uri":"/2019/03/cardboardci-docker-surge/","year":"2019"},{"content":"Docker image for Shellcheck ShellCheck is a GPLv3 tool that gives warnings and suggestions for bash/sh shell scripts:\nThe goals of ShellCheck are:\n To point out and clarify typical beginner\u0026rsquo;s syntax issues that cause a shell to give cryptic error messages. To point out and clarify typical intermediate level semantic problems that cause a shell to behave strangely and counter-intuitively. To point out subtle caveats, corner cases and pitfalls that may cause an advanced user\u0026rsquo;s otherwise working script to fail under future circumstances.  You can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/shellcheck Test interactively docker run -it cardboardci/shellcheck /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/shellcheck aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/shellcheck aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/shellcheck:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/shellcheck:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/shellcheck:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/shellcheck:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":109,"section":"posts","summary":"Docker image for Shellcheck ShellCheck is a GPLv3 tool that gives warnings and suggestions for bash/sh shell scripts:\nThe goals of ShellCheck are:\n To point out and clarify typical beginner\u0026rsquo;s syntax issues that cause a shell to give cryptic error messages. To point out and clarify typical intermediate level semantic problems that cause a shell to behave strangely and counter-intuitively. To point out subtle caveats, corner cases and pitfalls that may cause an advanced user\u0026rsquo;s otherwise working script to fail under future circumstances.","tags":["org:cardboardci"],"title":"docker-shellcheck","uri":"/2019/03/cardboardci-docker-shellcheck/","year":"2019"},{"content":"Docker image for PSScriptAnalyzer PSScriptAnalyzer is a static code checker for Windows PowerShell modules and scripts. PSScriptAnalyzer checks the quality of Windows PowerShell code by running a set of rules. The rules are based on PowerShell best practices identified by PowerShell Team and the community. It generates DiagnosticResults (errors and warnings) to inform users about potential code defects and suggests possible solutions for improvements.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/psscriptanalyzer Test interactively docker run -it cardboardci/psscriptanalyzer pwsh Test interactively with Bash docker run -it cardboardci/psscriptanalyzer bash Emit version table docker run -it cardboardci/psscriptanalyzer pwsh -Command \u0026#39;$PSVersionTable\u0026#39; Emit versions of installed modules docker run -it cardboardci/psscriptanalyzer pwsh -Command \u0026#39;Get-InstalledModule\u0026#39; Run format invocation docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/psscriptanalyzer pwsh -Command \u0026#39;Invoke-Formatter -ScriptDefinition (Get-Content -Path \u0026#39;File.ps1\u0026#39; -Raw)\u0026#39; Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/psscriptanalyzer:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/psscriptanalyzer:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/psscriptanalyzer:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/psscriptanalyzer:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":110,"section":"posts","summary":"Docker image for PSScriptAnalyzer PSScriptAnalyzer is a static code checker for Windows PowerShell modules and scripts. PSScriptAnalyzer checks the quality of Windows PowerShell code by running a set of rules. The rules are based on PowerShell best practices identified by PowerShell Team and the community. It generates DiagnosticResults (errors and warnings) to inform users about potential code defects and suggests possible solutions for improvements.\nYou can see the cli reference here.","tags":["org:cardboardci"],"title":"docker-psscriptanalyzer","uri":"/2019/03/cardboardci-docker-psscriptanalyzer/","year":"2019"},{"content":"Docker image for PdfTools Scientific articles are typically locked away in PDF format, a format designed primarily for printing but not so great for searching or indexing. The new pdftools package allows for extracting text and metadata from pdf files in R. From the extracted plain-text one could find articles discussing a particular drug or species name, without having to rely on publishers providing metadata, or pay-walled search engines.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/pdftools Test interactively docker run -it cardboardci/pdftools /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/pdftools aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/pdftools aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/pdftools:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/pdftools:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/pdftools:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/pdftools:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":111,"section":"posts","summary":"Docker image for PdfTools Scientific articles are typically locked away in PDF format, a format designed primarily for printing but not so great for searching or indexing. The new pdftools package allows for extracting text and metadata from pdf files in R. From the extracted plain-text one could find articles discussing a particular drug or species name, without having to rely on publishers providing metadata, or pay-walled search engines.\nYou can see the cli reference here.","tags":["org:cardboardci"],"title":"docker-pdftools","uri":"/2019/03/cardboardci-docker-pdftools/","year":"2019"},{"content":"Docker image for Pdf2HtmlEX pdf2htmlEX renders PDF files in HTML, utilizing modern Web technologies. Academic papers with lots of formulas and figures? Magazines with complicated layouts? No problem!\nFeatures:\n Native HTML text with precise font and location. Flexible output: all-in-one HTML or on demand page loading (needs JavaScript). Moderate file size, sometimes even smaller than PDF. Supporting links, outlines (bookmarks), printing, SVG background, Type 3 fonts and more.  You can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/pdf2htmlex Test interactively docker run -it cardboardci/pdf2htmlex /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/pdf2htmlex aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/pdf2htmlex aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/pdf2htmlex:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/pdf2htmlex:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/pdf2htmlex:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/pdf2htmlex:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":112,"section":"posts","summary":"Docker image for Pdf2HtmlEX pdf2htmlEX renders PDF files in HTML, utilizing modern Web technologies. Academic papers with lots of formulas and figures? Magazines with complicated layouts? No problem!\nFeatures:\n Native HTML text with precise font and location. Flexible output: all-in-one HTML or on demand page loading (needs JavaScript). Moderate file size, sometimes even smaller than PDF. Supporting links, outlines (bookmarks), printing, SVG background, Type 3 fonts and more.  You can see the cli reference here.","tags":["org:cardboardci"],"title":"docker-pdf2htmlex","uri":"/2019/03/cardboardci-docker-pdf2htmlex/","year":"2019"},{"content":"Docker image for Netlify The Netlify CLI facilitates the deployment of websites to Netlify, to improve the site building experience.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/netlify Test interactively docker run -it cardboardci/netlify /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/netlify aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/netlify aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/netlify:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/netlify:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/netlify:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/netlify:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":113,"section":"posts","summary":"Docker image for Netlify The Netlify CLI facilitates the deployment of websites to Netlify, to improve the site building experience.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/netlify Test interactively docker run -it cardboardci/netlify /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/netlify aws s3 cp file.","tags":["org:cardboardci"],"title":"docker-netlify","uri":"/2019/03/cardboardci-docker-netlify/","year":"2019"},{"content":"Docker image for LaTeX LaTeX is a high-quality typesetting system; it includes features designed for the production of technical and scientific documentation. LaTeX is the de facto standard for the communication and publication of scientific documents. LaTeX is available as free software.\nYou can see the LaTeX reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/latex Test interactively docker run -it cardboardci/latex /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/latex aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/latex aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/latex:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/latex:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/latex:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/latex:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":114,"section":"posts","summary":"Docker image for LaTeX LaTeX is a high-quality typesetting system; it includes features designed for the production of technical and scientific documentation. LaTeX is the de facto standard for the communication and publication of scientific documents. LaTeX is available as free software.\nYou can see the LaTeX reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.","tags":["org:cardboardci"],"title":"docker-latex","uri":"/2019/03/cardboardci-docker-latex/","year":"2019"},{"content":"Docker image for Hugo Hugo is one of the most popular open-source static site generators. With its amazing speed and flexibility, Hugo makes building websites fun again.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/hugo Test interactively docker run -it cardboardci/hugo /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/hugo aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/hugo aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/hugo:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/hugo:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/hugo:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/hugo:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":115,"section":"posts","summary":"Docker image for Hugo Hugo is one of the most popular open-source static site generators. With its amazing speed and flexibility, Hugo makes building websites fun again.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/hugo Test interactively docker run -it cardboardci/hugo /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/hugo aws s3 cp file.","tags":["org:cardboardci"],"title":"docker-hugo","uri":"/2019/03/cardboardci-docker-hugo/","year":"2019"},{"content":"Docker image for GitLabCLI What is GitLabCLI ?\n It\u0026rsquo;s a cross platform GitLab command line tool to quickly \u0026amp; naturally perform frequent tasks on GitLab project. It does not force you to hand craft json or use other unnatural ways (for example ids, concatenating of strings) like other CLI\u0026rsquo;s to interact with GitLab. It does not have any dependencies. It\u0026rsquo;s self contained .NET core application - you don\u0026rsquo;t need to have .NET installed for it to work.  You can see the source repository here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/awscli Test interactively docker run -it cardboardci/awscli /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/awscli aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/awscli aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/awscli:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/awscli:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/awscli:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/awscli:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":116,"section":"posts","summary":"Docker image for GitLabCLI What is GitLabCLI ?\n It\u0026rsquo;s a cross platform GitLab command line tool to quickly \u0026amp; naturally perform frequent tasks on GitLab project. It does not force you to hand craft json or use other unnatural ways (for example ids, concatenating of strings) like other CLI\u0026rsquo;s to interact with GitLab. It does not have any dependencies. It\u0026rsquo;s self contained .NET core application - you don\u0026rsquo;t need to have .","tags":["org:cardboardci"],"title":"docker-gitlab","uri":"/2019/03/cardboardci-docker-gitlab/","year":"2019"},{"content":"Docker image for AWS CLI \u0026amp; Docker The AWS Command Line Interface (CLI) is a unified tool to manage your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts. This container includes docker, allowing deployments to Amazon Elastic Container Registry (ECR), a fully-managed Docker container registry.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/awscli Test interactively docker run -it cardboardci/awscli /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/awscli aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/awscli aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/awscli:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/awscli:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/awscli:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/awscli:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":117,"section":"posts","summary":"Docker image for AWS CLI \u0026amp; Docker The AWS Command Line Interface (CLI) is a unified tool to manage your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts. This container includes docker, allowing deployments to Amazon Elastic Container Registry (ECR), a fully-managed Docker container registry.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.","tags":["org:cardboardci"],"title":"docker-ecr","uri":"/2019/03/cardboardci-docker-ecr/","year":"2019"},{"content":"Docker image for Bats (Bash Automated Testing System) Bats is a TAP-compliant testing framework for Bash. It provides a simple way to verify that the UNIX programs you write behave as expected.\nA Bats test file is a Bash script with special syntax for defining test cases. Under the hood, each test case is just a function with a description.\n#!/usr/bin/env bats  @test \u0026#34;addition using bc\u0026#34; { result=\u0026#34;$(echo 2+2 | bc)\u0026#34; [ \u0026#34;$result\u0026#34; -eq 4 ] } @test \u0026#34;addition using dc\u0026#34; { result=\u0026#34;$(echo 2 2+p | dc)\u0026#34; [ \u0026#34;$result\u0026#34; -eq 4 ] } You can see the source repository here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/awscli Test interactively docker run -it cardboardci/awscli /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/awscli aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/awscli aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/awscli:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/awscli:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/awscli:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/awscli:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":118,"section":"posts","summary":"Docker image for Bats (Bash Automated Testing System) Bats is a TAP-compliant testing framework for Bash. It provides a simple way to verify that the UNIX programs you write behave as expected.\nA Bats test file is a Bash script with special syntax for defining test cases. Under the hood, each test case is just a function with a description.\n#!/usr/bin/env bats  @test \u0026#34;addition using bc\u0026#34; { result=\u0026#34;$(echo 2+2 | bc)\u0026#34; [ \u0026#34;$result\u0026#34; -eq 4 ] } @test \u0026#34;addition using dc\u0026#34; { result=\u0026#34;$(echo 2 2+p | dc)\u0026#34; [ \u0026#34;$result\u0026#34; -eq 4 ] } You can see the source repository here.","tags":["org:cardboardci"],"title":"docker-bats","uri":"/2019/03/cardboardci-docker-bats/","year":"2019"},{"content":"Docker image for AWS CLI The AWS Command Line Interface (CLI) is a unified tool to manage your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/awscli Test interactively docker run -it cardboardci/awscli /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/awscli aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/awscli aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/awscli:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/awscli:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/awscli:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/awscli:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":119,"section":"posts","summary":"Docker image for AWS CLI The AWS Command Line Interface (CLI) is a unified tool to manage your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.","tags":["org:cardboardci"],"title":"docker-awscli","uri":"/2019/03/cardboardci-docker-awscli/","year":"2019"},{"content":"Docker image for GitHubCLI hub is an extension to command-line git that helps you do everyday GitHub tasks without ever leaving the terminal. hub can be safely aliased as git so you can type $ git \u0026lt;command\u0026gt;in the shell and get all the usual hub features.\nYou can see the source repository here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/awscli Test interactively docker run -it cardboardci/awscli /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/awscli aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/awscli aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/awscli:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/awscli:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/awscli:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/awscli:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":120,"section":"posts","summary":"Docker image for GitHubCLI hub is an extension to command-line git that helps you do everyday GitHub tasks without ever leaving the terminal. hub can be safely aliased as git so you can type $ git \u0026lt;command\u0026gt;in the shell and get all the usual hub features.\nYou can see the source repository here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.","tags":["org:cardboardci"],"title":"docker-github","uri":"/2019/03/cardboardci-docker-github/","year":"2019"},{"content":"Docker image for HadoLint A smarter Dockerfile linter that helps you build best practice Docker images. The linter is parsing the Dockerfile into an AST and performs rules on top of the AST. It is standing on the shoulders of ShellCheck to lint the Bash code inside RUN instructions.\nYou can see the source repository here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/awscli Test interactively docker run -it cardboardci/awscli /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/awscli aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/awscli aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/awscli:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/awscli:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/awscli:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/awscli:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":121,"section":"posts","summary":"Docker image for HadoLint A smarter Dockerfile linter that helps you build best practice Docker images. The linter is parsing the Dockerfile into an AST and performs rules on top of the AST. It is standing on the shoulders of ShellCheck to lint the Bash code inside RUN instructions.\nYou can see the source repository here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.","tags":["org:cardboardci"],"title":"docker-hadolint","uri":"/2019/03/cardboardci-docker-hadolint/","year":"2019"},{"content":"Docker AWSCLI A docker image based on Windows containing the awscli.\n","id":122,"section":"posts","summary":"Docker AWSCLI A docker image based on Windows containing the awscli.","tags":["org:cardboardci"],"title":"docker-win-awscli","uri":"/2019/02/cardboardci-docker-win-awscli/","year":"2019"},{"content":".NET Core Native Compilation Experiments A simple CI/CD pipeline making use of CoreRT to build linux and windows copies of a \u0026ldquo;Hello World\u0026rdquo; console application.\nNotes  CoreRT currently does not support compilation cross-compilation (as it is not supported yet). AOT for a linux binary requires non-dotnet dependencies (that need to be installed to be base image)  At present it is likely best to use Windows docker images for building the dotnet native jobs.\n","id":123,"section":"posts","summary":".NET Core Native Compilation Experiments A simple CI/CD pipeline making use of CoreRT to build linux and windows copies of a \u0026ldquo;Hello World\u0026rdquo; console application.\nNotes  CoreRT currently does not support compilation cross-compilation (as it is not supported yet). AOT for a linux binary requires non-dotnet dependencies (that need to be installed to be base image)  At present it is likely best to use Windows docker images for building the dotnet native jobs.","tags":["org:jrbeverly"],"title":"dotnet-native-corert","uri":"/2019/02/jrbeverly-dotnet-native-corert/","year":"2019"},{"content":"Prototype CodePipeline Terraform Repository A repository for terraform execution in a Codepipeline task. This repository is part of an original experiment\nI wanted to have an terraform executor that met the following requirements:\n Use official terraform docker image (hashicorp/terraform:light) No external dependencies or custom images (e.g. terragrunt, astro, etc) Customizable execution process with minimal overhead Support in-repository modules No credential management (AWS Codepipeline execution) Multiple AWS environments within a single repository No single state file, state file per component (controlled by terraform.tf file) State files map to location in repository Potential for custom IAM role per component (as opposed to single access permission)  This was a quick prototype to see if I would be able to get something rough running, with the shell executor being just the bare essentials that I need.\nIssues with the final result There are a couple of issues I noted when setting this up, and the eventual improvements made to the later executors. I have listed them below:\n CodePipeline requires cloudwatch to provide notifications on failure CodeBuild log for the terraform plan/show outputs is not very pretty CodePipeline is limited to a single branch, impacting the idea of \u0026lsquo;preview\u0026rsquo; builds Shell based executors are simple, but require maintenance YAML Executors (see GitLab / GitHub Actions / CircleCI) have isolated executor for each component Deployments to 2 or more accounts requires a \u0026lsquo;GitFlow\u0026rsquo; style approach, which has a lot of overhead Restricting permissions on the component level requires a fair bit of extra work, and isn\u0026rsquo;t really sound Control options are handled by files (e.g. ORDER, IGNORE, APPLY_ONLY), which doesn\u0026rsquo;t lend itself to customization well  It is nice to have everything in AWS, with serverless architecture but I find the limitations are a big issue. If I was starting from scratch, I would be interested in the idea of deploying a GitLab + GitLabCI architecture in AWS, then restricting IAM access by executors (that would be provisioned by AWS Lambda).\nThe big problems I have with this style of terraform executor is the edge cases. I\u0026rsquo;ll mention three that have bugged me the most:\nDeployments into another AWS ACcount Allowing another team (or individual) to have a component (e.g. teraform deployment) that exists within an account I own. I would want to restrict access for that role, and keep knowledge of that terraform within my infrastructure repository. Also wouldn\u0026rsquo;t want to require my approval on every PR, but at the same time not allow free reign within the AWS account.\nIt is an option to just expose a role in an account, then allow the terraform+executor to be handled by the team in another repo/executor. This can mean back and forth a bit, as the IAM permissions for that role need to be fine tuned\nExisting infrastructure / failed deployments This can happen when you are not using uniqely named resources (e.g. my-s3-bucket-x2r1), where terraform will fail when it encounters an existing resource. Failures can also happen when working with a non-terraform managed resource (e.g. IAM policy usages). When using a codepipeline executor, everything is in a single log file so it can be difficult to know:\n Who failed? What resource had failed? Where is that in the repository?  Cross-cutting concerns Deployments with codepipeline executors do not work well for cross-cutting concerns. If I want to deploy something into every single AWS account (such as an IAM role or S3 bucket), it just doesn\u0026rsquo;t work well. I don\u0026rsquo;t get the advantages of parallel execution, and a failure midway cannot be reverted easily. Permissions are also way too open for my taste, in that the single executor is running in every single account within any real guard-rails.\nAt least with an external ID per account role, with isolated execution (in say GitLabCI / CircleCI) I have more confidence that I won\u0026rsquo;t see misconfiguration in an account.\n","id":124,"section":"posts","summary":"Prototype CodePipeline Terraform Repository A repository for terraform execution in a Codepipeline task. This repository is part of an original experiment\nI wanted to have an terraform executor that met the following requirements:\n Use official terraform docker image (hashicorp/terraform:light) No external dependencies or custom images (e.g. terragrunt, astro, etc) Customizable execution process with minimal overhead Support in-repository modules No credential management (AWS Codepipeline execution) Multiple AWS environments within a single repository No single state file, state file per component (controlled by terraform.","tags":["org:infraprints"],"title":"simple-terraform","uri":"/2019/01/infraprints-simple-terraform/","year":"2019"},{"content":"Docker image for TfLint TFLint is a Terraform linter focused on possible errors, best practices, etc.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/tflint Test interactively docker run -it cardboardci/tflint /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/tflint aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/tflint aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/tflint:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/tflint:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/tflint:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/tflint:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":125,"section":"posts","summary":"Docker image for TfLint TFLint is a Terraform linter focused on possible errors, best practices, etc.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/tflint Test interactively docker run -it cardboardci/tflint /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/tflint aws s3 cp file.","tags":["org:cardboardci"],"title":"docker-tflint","uri":"/2018/12/cardboardci-docker-tflint/","year":"2018"},{"content":"Makefile Experiments Summary Experimenting with using makefiles as a build harness type structure. The idea is to package makefile using GitHub, that can then be downloaded when running.\nConceptual Usage As the structure is simply an experiment, no targets are actually implemented. At the top of the Makefile, you can include the makefile using the following:\n-include $(shell curl -sSL -o .build-system \u0026#34;https://.../makefile\u0026#34;; echo .build-system) This will download a Makefile called .build-system and include it at run-time. Once it has been successfully downloaded, you can run make help for a lsit of available targets.\nLimitations Although the targets for a library are included, that doesn\u0026rsquo;t mean that any dependencies are included. For example, if a makefile library relies on non-standard binaries (e.g. jq, awscli, docker), then they would need to be installed on the local environment.\nI have found this doesn\u0026rsquo;t scale that well, especially for dependencies that are frequently updated (pre-1.0.0). It does have a bit of advantage in scrapping together quick and dirty build systems when working with Dev-Ops style work (docker, terraform, ansible, packer). As I have found it pretty nice to get useful commands out of the box (and readily available with make help).\nInspiration These experiments are inspired by cloudposse/build-harness.\n","id":126,"section":"posts","summary":"Makefile Experiments Summary Experimenting with using makefiles as a build harness type structure. The idea is to package makefile using GitHub, that can then be downloaded when running.\nConceptual Usage As the structure is simply an experiment, no targets are actually implemented. At the top of the Makefile, you can include the makefile using the following:\n-include $(shell curl -sSL -o .build-system \u0026#34;https://.../makefile\u0026#34;; echo .build-system) This will download a Makefile called .","tags":["org:jrbeverly"],"title":"make-exp","uri":"/2018/12/jrbeverly-make-exp/","year":"2018"},{"content":"Infrastructure Summary The specification of jrbeverlylabs as a set of terraform modules.\nUsage To run this you need to execute:\nterraform init terraform plan terraform apply Notes This was a simple experiment making use of the gitlab provider of terraform. The idea was to see if it would assist in the process of maintaining jrbeverlylabs between gitlab.com and my internal gitlab instance.\n","id":127,"section":"posts","summary":"Infrastructure Summary The specification of jrbeverlylabs as a set of terraform modules.\nUsage To run this you need to execute:\nterraform init terraform plan terraform apply Notes This was a simple experiment making use of the gitlab provider of terraform. The idea was to see if it would assist in the process of maintaining jrbeverlylabs between gitlab.com and my internal gitlab instance.","tags":["org:jrbeverly"],"title":"infrastructure-labs","uri":"/2018/11/jrbeverly-infrastructure-labs/","year":"2018"},{"content":"XUnit.Metadata Summary Strongly-typed attributes for the management and organization of tests. As opposed to using strings throughout the code, [Trait(\u0026quot;Category\u0026quot;, \u0026quot;Unit\u0026quot;)], you can use strongly-typed attributes for organizing tests.\nGetting Started With xUnit v2 you can markup tests with traits, particullary of interest is the category key. Using traits you can sort tests or control execution of tests from the command line. You can install in your project using the Nuget Manager Console:\nInstall-Package xUnit.Metadata Or from the .NET CLI with the following command:\ndotnet add package xUnit.Metadata Usage You can use the attributes just as you would a TraitAttribute from xUnit v2.\n[Fact] [Unit] [Nighty] public void ExceptionWhenDivideByZero() { var value = 10; Assert.Throws\u0026lt;ArgumentException\u0026gt;(() =\u0026gt; _adder.Divide(value, 0)); } Or as a class attribute:\n[Unit] public sealed class AdderTests { [Theory] [Nighty] [InlineData(3, 1, 2)] [InlineData(4, 2, 2)] [InlineData(1, 1, 0)] [InlineData(0, -2, 2)] public void AddSet(int expected, int a, int b) { Assert.Equal(expected, _adder.Add(a, b)); } [Fact] [Weekly] public void ExceptionWhenDivideByZero() { var value = 10; Assert.Throws\u0026lt;ArgumentException\u0026gt;(() =\u0026gt; _adder.Divide(value, 0)); } } Running from Command Line When using the .NET CLI, you can use dotnet test to execute tests from the command line. You can selectively execute tests based on filtering condition through --filter. You can do this with the following:\ndotnet test --filter \u0026#34;Category = Unit\u0026#34; ProjectA.csproj Running the above: dotnet test --filter Category=Unit will run tests which are annotated with the strongly typed attribute [Unit] or [Trait(\u0026quot;Category\u0026quot;, \u0026quot;Unit\u0026quot;)].\nBuild You can build the project using the scripts available in the build/ directory. The scripts are used in the project pipeline, and can be run locally or in a docker container. You can do this with the following:\nsh build/build.sh Or you can use the docker based script, docker-build.sh if you do not have the necessary commands installed locally on your machine. You can do this with the following:\nsh build/docker-build.sh Packaging You can package the library into a nuget package using the package.sh script. You can do this with the following:\nsh build/package.sh Tests All scripts in the repository use xUnit for testing. You can view the tests in the tests/ directory. An example testing project that makes use of the attributes is available under sample/. However, this project is more designed as a prototyping ground for new attributes. To run the tests for the project, you can do so with the following:\nsh build/test.sh Or you can use the docker version of the script, docker-test.sh if you do not have the necessary commands installed locally on your machine. You can do this with the following:\nsh build/docker-test.sh Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Ana Paula Tello from the Noun Project.\n","id":128,"section":"posts","summary":"XUnit.Metadata Summary Strongly-typed attributes for the management and organization of tests. As opposed to using strings throughout the code, [Trait(\u0026quot;Category\u0026quot;, \u0026quot;Unit\u0026quot;)], you can use strongly-typed attributes for organizing tests.\nGetting Started With xUnit v2 you can markup tests with traits, particullary of interest is the category key. Using traits you can sort tests or control execution of tests from the command line. You can install in your project using the Nuget Manager Console:","tags":["org:jrbeverly"],"title":"xunit-metadata","uri":"/2018/11/jrbeverly-xunit-metadata/","year":"2018"},{"content":"XPlatformer-Workspace Summary A meta-repository for facilitating development of the many-repository XPlatformer project.\nGetting Started XPlatformer-Workspace is a git-submodules oriented approach for dealing with the multiple repositories of the XPlatformer project (XPlatformer / XGameLib / XSamples). The project provides all the repositories in one, using vagrant-desktop to provide an X11 development environment. You can clone all the repositories using the --recursive directive of git as such:\ngit clone --recursive git@gitlab:.../XPlatformer-Workspace.git Or if the repository has already been cloned, you can use:\ngit submodule init git submodule update Usage The Workspace architecture aims to provide a way of provisioning a complete environment for working with multiple repositories. The desired architecture of the meta-project is specified below:\n* Repository * bin * lib * build * build-all.sh * ... * Repositories * [r] XPlatformer (git@gitlab/XPlatformer.git) * [r] XGameLib (git@gitlab/XGameLib.git) * ... * Environments * [r] vagrant-desktop-x11 (git@gitlab/vagrant-desktop-x11.git) * ... * README.md The XPlatformer-Workspace is a modified version of the proposed architecture where the environments and repositories directories are merged. This is done to simplify the working environment.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Maxi Koichi from the Noun Project.\n","id":129,"section":"posts","summary":"XPlatformer-Workspace Summary A meta-repository for facilitating development of the many-repository XPlatformer project.\nGetting Started XPlatformer-Workspace is a git-submodules oriented approach for dealing with the multiple repositories of the XPlatformer project (XPlatformer / XGameLib / XSamples). The project provides all the repositories in one, using vagrant-desktop to provide an X11 development environment. You can clone all the repositories using the --recursive directive of git as such:\ngit clone --recursive git@gitlab:.../XPlatformer-Workspace.git Or if the repository has already been cloned, you can use:","tags":["org:xplatformer"],"title":"xplatformer-workspace","uri":"/2018/11/xplatformer-xplatformer-workspace/","year":"2018"},{"content":"Wifi Web Summary Wifi Web provides an autorun USB for connecting to wireless access points for devices that do not have access to a camera. It opens an HTML page that provides easy access to the Wifi connection details.\nIf you have a camera-enabled device, you can scan Wifi connection details using a QR Code (or any barcode type).\nInstallation You can install Wifi Web onto a USB stick by unzipping the most recent build. You can use the autorun.sh or autorun.bat file to open the HTML page. If autorun is possible in the environment (unlikely), you will have the opporunity to open the HTML page when the USB device is connected.\nConfiguration You need to manually edit the connection details, you can do so with the js/connections.js file. The file is of the format:\nexports = { connections: [ { name: \u0026#34;My Wifi 2.4-Ghz\u0026#34;, password: \u0026#34;Passw0rd!\u0026#34; }, { name: \u0026#34;My Wifi 5-Ghz\u0026#34;, password: \u0026#34;L337Pass\u0026#34; } ] }; You can see an example of the file format in js/default.js.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by novita dian from the Noun Project.\n","id":130,"section":"posts","summary":"Wifi Web Summary Wifi Web provides an autorun USB for connecting to wireless access points for devices that do not have access to a camera. It opens an HTML page that provides easy access to the Wifi connection details.\nIf you have a camera-enabled device, you can scan Wifi connection details using a QR Code (or any barcode type).\nInstallation You can install Wifi Web onto a USB stick by unzipping the most recent build.","tags":["org:jrbeverly"],"title":"wifi-web","uri":"/2018/11/jrbeverly-wifi-web/","year":"2018"},{"content":"stack-opengl Summary stack-opengl is a variant of stack-net written in OpenGL. It uses extremely simple shaders and OpenGL programming to create a block stacking application.\nGetting Started The project uses premake4 as the cross-platform build system. You will need to build the external dependencies of the project, by running a root level build. You can then build the project itself. You can do so as such:\npremake4 gmake make cd src/ premake4 gmake make ./Stack Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Christina Witt George from the Noun Project.\n","id":131,"section":"posts","summary":"stack-opengl Summary stack-opengl is a variant of stack-net written in OpenGL. It uses extremely simple shaders and OpenGL programming to create a block stacking application.\nGetting Started The project uses premake4 as the cross-platform build system. You will need to build the external dependencies of the project, by running a root level build. You can then build the project itself. You can do so as such:\npremake4 gmake make cd src/ premake4 gmake make .","tags":["org:jrbeverly"],"title":"stack-opengl","uri":"/2018/11/jrbeverly-stack-opengl/","year":"2018"},{"content":"Stack-NET Summary A block blueprinter, built using a visual graph style approach to graphics.\nGetting Started The project is based on the old approach to C# projects. The project should be opened in Visual Studio, built, then run.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Christina Witt George from the Noun Project.\n","id":132,"section":"posts","summary":"Stack-NET Summary A block blueprinter, built using a visual graph style approach to graphics.\nGetting Started The project is based on the old approach to C# projects. The project should be opened in Visual Studio, built, then run.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.","tags":["org:jrbeverly"],"title":"stack-net","uri":"/2018/11/jrbeverly-stack-net/","year":"2018"},{"content":"RayTracer Summary A Raytracer that receives a scene defined in lua, and produces an image output.\nGetting Started Compilation follows the standard process defined by the UWaterloo CS488 sample projects.\nWe use premake4 as our cross-platform build system. First you will need to build all the static libraries that the projects depend on. To build the libraries, open up a terminal, and cd to the top level of the CS488 project directory and then run the following:\npremake4 gmake make This will build the following static libraries, and place them in the top level lib folder of your cs488 project directory.\n libcs488-framework.a libglfw3.a libimgui.a  NOTE: As the following is only the relevant Raytracer files, the build system/dependencies and premake4.lua definition files are all missing. The files in this project are provided as-is.\nDependencies  OpenGL 3.2+ GLFW  http://www.glfw.org/   Lua  http://www.lua.org/   Premake4  https://github.com/premake/premake-4.x/wiki http://premake.github.io/download.html   GLM  http://glm.g-truc.net/0.9.7/index.html   AntTweakBar  http://anttweakbar.sourceforge.net/doc/    Notes Objectives were completed as defined by the assignment.\nThe sample.lua scene that is defined is based on the simple.lua, and uses a compositions of items from the sample lua files.\nFeatures:\n Uses a multi-threaded design to increase performance (number of threads can be specified at compile-time) Mirror reflections was the supported offical feature that was added to the project  Standing Issues:\n Currently there is an issue with the Mesh models. As it stands they only partially render (certain values do) This likely has to do with an issue with the algorithm that was being used Originally used an angle based algorithm (described here) but it did not render the objects  Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Arthur Schmitt from the Noun Project.\n","id":133,"section":"posts","summary":"RayTracer Summary A Raytracer that receives a scene defined in lua, and produces an image output.\nGetting Started Compilation follows the standard process defined by the UWaterloo CS488 sample projects.\nWe use premake4 as our cross-platform build system. First you will need to build all the static libraries that the projects depend on. To build the libraries, open up a terminal, and cd to the top level of the CS488 project directory and then run the following:","tags":["org:jrbeverly"],"title":"raytracer","uri":"/2018/11/jrbeverly-raytracer/","year":"2018"},{"content":"jrbeverly.profile Summary This is a one page user profile for Jonathan Beverly (jrbeverly - i.e. me), linking to multiple online identities, relevant external sites, and popular social networking websites. Not all of them are included, but most of the relevant ones are.\nBuild Process The process of minimizing the web resources is handled using the command line utility of Minify which is available here. The process is used manually as opposed to leveraging a specific build system, is to experiment with more granular controls for website compilation.\nWhy not minify with ___? The static pages are minified using Minify CLI for the simple reason of tinkering. I wanted to be able to fiddle with various ways of optimizing a static HTML project, and a one page user profile project seemed to be the perfect fit.\nDeployment The static pages can be downloaded from the pipeline artifacts, which can then be run on any web server. For example, using it with Gitlab Pages can be done by creating the project jrbeverly.gitlab.io. The .gitlab-ci.yml of the jrbeverly.gitlab.io project can download the artifacts, and deploy the artifacts to gitlab.io.\nRead more about user/group Pages and project Pages.\nTemplate The webpage is based on the template designed by mRova available here.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Daniel Gamage from the Noun Project.\n","id":134,"section":"posts","summary":"jrbeverly.profile Summary This is a one page user profile for Jonathan Beverly (jrbeverly - i.e. me), linking to multiple online identities, relevant external sites, and popular social networking websites. Not all of them are included, but most of the relevant ones are.\nBuild Process The process of minimizing the web resources is handled using the command line utility of Minify which is available here. The process is used manually as opposed to leveraging a specific build system, is to experiment with more granular controls for website compilation.","tags":["org:jrbeverly"],"title":"profile","uri":"/2018/11/jrbeverly-profile/","year":"2018"},{"content":"jrbeverly portfolio Summary Collections data from a specified list of gitlab projects, then converts them into static HTML briefs.\nGetting Started The project is designed to git clone a series of repository, then collect information from each of them. This includes the project icon, name, license, path, etc.\nFrom this information, the project will then for each repository create:\n A brief - A simple static page featuring the README.md of the project rendered in a viewdocs format A redirect link, allowing one to navigate to jrbeverly.me/ref/reponame to be redirected to the primary git repo  Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Kiryl Sytsko from the Noun Project.\n","id":135,"section":"posts","summary":"jrbeverly portfolio Summary Collections data from a specified list of gitlab projects, then converts them into static HTML briefs.\nGetting Started The project is designed to git clone a series of repository, then collect information from each of them. This includes the project icon, name, license, path, etc.\nFrom this information, the project will then for each repository create:\n A brief - A simple static page featuring the README.md of the project rendered in a viewdocs format A redirect link, allowing one to navigate to jrbeverly.","tags":["org:jrbeverly"],"title":"exp-portfolio","uri":"/2018/11/jrbeverly-exp-portfolio/","year":"2018"},{"content":"Packer Desktop Summary This build configuration installs the vagrant-desktop environments onto minimal ubuntu images that can be used with Virtualbox.\nGetting Started Make sure all the required software (listed above) is installed, then you can build the images as follows:\n# cd x11/ # cd opengl/ # cd base/ packer build ubuntu.json After a few minutes, Packer should tell you the image was generated successfully.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Arthur Schmitt from the Noun Project.\n","id":136,"section":"posts","summary":"Packer Desktop Summary This build configuration installs the vagrant-desktop environments onto minimal ubuntu images that can be used with Virtualbox.\nGetting Started Make sure all the required software (listed above) is installed, then you can build the images as follows:\n# cd x11/ # cd opengl/ # cd base/ packer build ubuntu.json After a few minutes, Packer should tell you the image was generated successfully.\nAcknowledgements The project icon is retrieved from the Noun Project.","tags":["org:devkitspaces"],"title":"packer-desktop","uri":"/2018/11/devkitspaces-packer-desktop/","year":"2018"},{"content":"office-depot Summary office-depot is a container based software development stack.\nGetting Started Getting started is as simple as using docker-compose. You can do so as such:\ndocker-compose up --env-file=office-depot.env -d Updating and Upgrading If you wish to upgrade the container stack, you need to run the following commands:\ndocker-compose stop docker-compose rm -v docker-compose pull You can then start the docker environment.\ndocker-compose up --env-file=office-depot.env -d Cleaning After upgrading, you can be left with unused images or containers. To remove all unused containers, volumes, networks and images (both dangling and unreferenced), you can use the following command:\ndocker system prune Running this after upgrade will ensure that any dangling images are cleaned up free up valuable disk space.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Marco Olgio from the Noun Project.\n","id":137,"section":"posts","summary":"office-depot Summary office-depot is a container based software development stack.\nGetting Started Getting started is as simple as using docker-compose. You can do so as such:\ndocker-compose up --env-file=office-depot.env -d Updating and Upgrading If you wish to upgrade the container stack, you need to run the following commands:\ndocker-compose stop docker-compose rm -v docker-compose pull You can then start the docker environment.\ndocker-compose up --env-file=office-depot.env -d Cleaning After upgrading, you can be left with unused images or containers.","tags":["org:jrbeverly"],"title":"office-depot","uri":"/2018/11/jrbeverly-office-depot/","year":"2018"},{"content":"Mirroring Summary A lightweight bash script that allows easy mirroring of projects to external git hosts.\nGetting started Simply fork this repository, as it has all the scripts necessary for performing mirrors. You can then add your repositories into the assets/ directory. You will want to store them as such:\n\u0026gt; repoA/\r\u0026gt; bitbucket.config\r\u0026gt; github.config\r\u0026gt; gitlab.config\r\u0026gt; someService.config\r\u0026gt; REPO\r\u0026gt; repoB/\r\u0026gt; ...\r\u0026gt; repoC/\r\u0026gt; ...\r Each repositroy is its own directory. The REPO file is expected, and will contain only one variable, SOURCE. The source variable is used for retrieving the repository from the original source location. For each site, you can then create a \u0026lt;site\u0026gt;.config file that contains the following variables:\n|Name|Description| ||| |MIRROR_NAME|The name of the host site| |MIRROR_SSH|The ssh url of the git repository.| |MIRROR_HTTP|The http url of the git repository.|\nYou can see an example github.config file presented below:\nMIRROR_NAME=\u0026#34;github\u0026#34; MIRROR_SSH=\u0026#34;git@github.com:jrbeverly/XPlatformer.git\u0026#34; MIRROR_HTTP=\u0026#34;https://github.com/jrbeverly/XPlatformer.git\u0026#34; ","id":138,"section":"posts","summary":"Mirroring Summary A lightweight bash script that allows easy mirroring of projects to external git hosts.\nGetting started Simply fork this repository, as it has all the scripts necessary for performing mirrors. You can then add your repositories into the assets/ directory. You will want to store them as such:\n\u0026gt; repoA/\r\u0026gt; bitbucket.config\r\u0026gt; github.config\r\u0026gt; gitlab.config\r\u0026gt; someService.config\r\u0026gt; REPO\r\u0026gt; repoB/\r\u0026gt; ...\r\u0026gt; repoC/\r\u0026gt; ...\r Each repositroy is its own directory.","tags":["org:jrbeverly"],"title":"mirroring","uri":"/2018/11/jrbeverly-mirroring/","year":"2018"},{"content":"Localization.NET Concept Summary A simple experiment prototyping a concept for strongly typed language terms.\nNote: The generated component is not built with this. This is a usage prototype only (no generator is included).\nGetting Started The idea that Localization.NET is attempting to conceptualize is one where an interface is used as the primary mechanism for declaring language terms. Attributes can be used to include more contextual information (usage, type, namespace). The Roslyn compiler can then use this information to generate the underlying code to facilitate the Language terms.\nWith the underlying code generated on-the-fly, a dependency injection framework (for example: Ninject, TinyIoC) can then be used for setting the ILanguageTerms _languageTerms field.\npublic class MyGreeter { private ILanguageTerms _languageTerms; public void SayHello(string name) { Console.WriteLine(_languageTerms.Hello(name)); } [LanguageTerms(\u0026#34;myapp.greeter\u0026#34;)] public interface ILanguageTerms { [LangTerm(\u0026#34;hello\u0026#34;)] LanguageTerm Hello(string name); } } The language resources could be defined in any format, below is an example using json (mygreeter.en-CA.json):\n{ \u0026#34;myapp\u0026#34;: { \u0026#34;greeter\u0026#34;: { \u0026#34;hello\u0026#34;: \u0026#34;Hello {0}! How are you?\u0026#34; } } } Implementing new language terms is a simple process of adding the term to ILanguageTerms, with an invariant set:\npublic class MyGreeter { // .... [LanguageTerms(\u0026#34;myapp.greeter\u0026#34;)] public interface ILanguageTerms { [LangTerm(\u0026#34;no_name\u0026#34;)] [Invariant(\u0026#34;Sorry! Didn\u0026#39;t catch that name?\u0026#34;)] LanguageTerm Message { get; } } } The [Invariant] attribute can later be added into the globalaization pipeline by an automated system (powered by Roslyn).\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Percy Batalier from the Noun Project.\n","id":139,"section":"posts","summary":"Localization.NET Concept Summary A simple experiment prototyping a concept for strongly typed language terms.\nNote: The generated component is not built with this. This is a usage prototype only (no generator is included).\nGetting Started The idea that Localization.NET is attempting to conceptualize is one where an interface is used as the primary mechanism for declaring language terms. Attributes can be used to include more contextual information (usage, type, namespace). The Roslyn compiler can then use this information to generate the underlying code to facilitate the Language terms.","tags":["org:jrbeverly"],"title":"localization-net","uri":"/2018/11/jrbeverly-localization-net/","year":"2018"},{"content":"Issues.Style Summary A style guide for issue management, release versioning, Git Flow and repository documentation.\nDescription In order to speed up the initialization process of a new gitlab project, Issues.Style provides a set of common labels and issues that might be used when setting up a new project. The project provides methods for quickly setting up the project, specifically providing the following:\n Labels - Grouped by color, according to broad themes Setup Issues - Initialization labels, including licensing, documentation, CI and metadata.  Labels The project labels can be viewed on Gitlab in the Issues \u0026gt; Labels section, or you can view them here in labels.md.\nGetting Started First, make sure you have valid GitLab account tokens for both source and destination GitLab installations. They are used to access GitLab resources without authentication. GitLab private tokens are availble in \u0026ldquo;Profile Settings -\u0026gt; Account\u0026rdquo;.\ngitlab-copy is a simple tool for copying issues/labels/milestones/notes from one GitLab project to another, possibly running on different GitLab instances.\nUsage To copy an template issues or labels, you can use the batch copy utility gitlab-copy. You will need to create a YAML configuration file to be used by the gitlab utility gitlab-copy. You can view a sample template for copying over labels here. The configuration file will specify source and targets, along with the access tokens for each of the gitlab instances. The gitlab.yml will be of the form:\nfrom: url: https://gitlab.com token: ${TOKEN} project: jrbeverly/Issues.Style labelsOnly: true to: url: https://gitlab.com token: ${TOKEN} project: jrbeverly/new_project The above will copy just the labels from the Issue.Styles project. Using gitlab-copy, you can then copy the labels from the Issues.Style to another project. You can do this with the following command:\ngitlab-copy -y gitlab.yml If you would like to perform a smoke run, you can omit the -y flag to not execute. You can do that as such:\ngitlab-copy -y gitlab.yml Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Desbenoit from the Noun Project.\n","id":140,"section":"posts","summary":"Issues.Style Summary A style guide for issue management, release versioning, Git Flow and repository documentation.\nDescription In order to speed up the initialization process of a new gitlab project, Issues.Style provides a set of common labels and issues that might be used when setting up a new project. The project provides methods for quickly setting up the project, specifically providing the following:\n Labels - Grouped by color, according to broad themes Setup Issues - Initialization labels, including licensing, documentation, CI and metadata.","tags":["org:jrbeverly"],"title":"issues-style","uri":"/2018/11/jrbeverly-issues-style/","year":"2018"},{"content":"jrbeverly.icons Summary A collection of scalable vector graphics (SVG) that define project and group icons.\nBuild You can build the icons using the tool rsvg-convert. To build with rsvg-convert, you can do the following:\nrsvg-convert -f svg icon.svg \u0026gt; output.svg rsvg-convert -f png icon.svg \u0026gt; output.png It is recommend to use the build scripts available in build/ or in the local source directory. These scripts are used in the build pipeline, ensuring that all arguments and attributes are set for compilation of the icons. These should be run from the root of the project directory.\nsh build/compile.sh Or if running in an environment with rsvg-convert installed (such as a docker container), you can do the following:\nsh build/build.sh Docker Environment You can start a docker container with rsvg-convert installed to experiment with building the icons. To build with rsvg-convert, you can do the following:\nsh build/run.sh Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Margot Nadot from the Noun Project.\n","id":141,"section":"posts","summary":"jrbeverly.icons Summary A collection of scalable vector graphics (SVG) that define project and group icons.\nBuild You can build the icons using the tool rsvg-convert. To build with rsvg-convert, you can do the following:\nrsvg-convert -f svg icon.svg \u0026gt; output.svg rsvg-convert -f png icon.svg \u0026gt; output.png It is recommend to use the build scripts available in build/ or in the local source directory. These scripts are used in the build pipeline, ensuring that all arguments and attributes are set for compilation of the icons.","tags":["org:jrbeverly"],"title":"project-icons","uri":"/2018/11/jrbeverly-project-icons/","year":"2018"},{"content":"gitlab-ci.yml A collection of GitLab CI configuration files that are used by my projects. Stored here as the process of docker projects are polished and standardized.\nGetting Started Each of the dockerfiles is presented with a simple .gitlab-ci.yml file that uses one of my docker images. The resources referenced by the definition are not included in this project. You can start by copying the .gitlab-cy.yml, then replacing the relevant bits.\nstages: - build build: stage: build image: jrbeverly/minify:baseimage script: - minify -o index-min.html index.html only: - master artifacts: paths: - public/ expire_in: 1 hour Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Icons8 from the Noun Project.\n","id":142,"section":"posts","summary":"gitlab-ci.yml A collection of GitLab CI configuration files that are used by my projects. Stored here as the process of docker projects are polished and standardized.\nGetting Started Each of the dockerfiles is presented with a simple .gitlab-ci.yml file that uses one of my docker images. The resources referenced by the definition are not included in this project. You can start by copying the .gitlab-cy.yml, then replacing the relevant bits.","tags":["org:jrbeverly"],"title":"gitlab-ci-yml","uri":"/2018/11/jrbeverly-gitlab-ci-yml/","year":"2018"},{"content":"Entity.NET Concept Summary A simple experiment prototyping a concept for strongly typed ORMs.\nGetting Started The idea that Entity.NET is attempting to conceptualize is one where strongly-typed objects are used with an ORM system. The primary objective is to use strongly typed identifiers (Keys.Customer) that restricts the usages of an applications ORM. The concept is from the following scenario:\nvar cust = new Models.Customer() { Name = \u0026#34;John Doe\u0026#34; }; var entity = Repository.Add(cust); //entity.Key =\u0026gt; underlying int (10) Repository.Update(entity.Key, new Models.Customer() { Name = \u0026#34;David Smith\u0026#34; }); //...  var existing = (Keys.Customer)10; var updatedCust = Repository.Find(entity.Key); Console.WriteLine(updatedCust.Name); //David Smith The code would be generated from the base definition of an entity below. Roslyn would generate the strongly-typed code using Attributes on the models to control the code outputs.\n[Entity(typeof(int))] class Customer { public string Name { get; set; } public string Address { get; set; } } Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Maxi Koichi from the Noun Project.\n","id":143,"section":"posts","summary":"Entity.NET Concept Summary A simple experiment prototyping a concept for strongly typed ORMs.\nGetting Started The idea that Entity.NET is attempting to conceptualize is one where strongly-typed objects are used with an ORM system. The primary objective is to use strongly typed identifiers (Keys.Customer) that restricts the usages of an applications ORM. The concept is from the following scenario:\nvar cust = new Models.Customer() { Name = \u0026#34;John Doe\u0026#34; }; var entity = Repository.","tags":["org:jrbeverly"],"title":"entity-net","uri":"/2018/11/jrbeverly-entity-net/","year":"2018"},{"content":"Dockerized WKHtmlToPDF Summary A super small image with wkhtmltopdf installed. The project icon is from cre.ativo mustard, HK from the Noun Project\nNOTE: This image is marked EOL, and use is discouraged.\nUsage You can use this image locally with docker run, calling wkhtmltopdf:\ndocker run -v $(pwd):/media/ jrbeverly/wkhtmltopdf:privileged wkhtmltopdf http://google.com google.pdf Gitlab You can setup a build job using .gitlab-ci.yml:\nbuild: image: jrbeverly/wkhtmltopdf:baseimage script: - wkhtmltopdf http://google.com google.pdf artifacts: paths: - google.pdf Components Metadata Arguments Metadata build arguments used with the Label Schema Convention.\n   Variable Value Description     BUILD_DATE see metadata.variable The Date/Time the image was built.   VERSION see metadata.variable Release identifier for the contents of the image.   VCS_REF see metadata.variable Identifier for the version of the source code from which this image was built.    Build Arguments Build arguments used in the image.\n   Variable Value Description     USER see Makefile.options Sets the user to use when running the image.   DUID see user.variable The user id of the docker user.   DGID see user.variable The group id of the docker user\u0026rsquo;s group.    Volumes No volumes are exposed by the docker container. However, while running the image with limited permissions (baseimage), it is necessary to ensure that the docker user has permission to access mounted volumes. You will need to ensure that the docker user can read/write to the mounted volumes. (see User / Group Identifiers)\nThe working directory of the image is /media/.\nBuild Process To build the docker image, use the included Makefile. It is recommended to use the makefile to ensure all build arguments are provided.\nmake VERSION=\u0026lt;version\u0026gt; build You can view the build/README.md for more on using the Makefile to build the image.\nLabels The docker image follows the Label Schema Convention. Label Schema is a community project to provide a shared namespace for use by multiple tools, specifically org.label-schema. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.label-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/wkhtmltopdf:\u0026lt;TAG\u0026gt; Label Extension The label namespace org.doc-schema is an extension of org.label-schema. The namespace stores internal variables often used when interacting with the image. These variables will often be application versions or exposed internal variables. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/wkhtmltopdf:\u0026lt;TAG\u0026gt; User and Group Mapping All processes within the baseimage docker container will be run as the docker user, a non-root user. The docker user is created on build with the user id DUID and a member of a group with group id DGID.\nAny permissions on the host operating system (OS) associated with either the user (DUID) or group (DGID) will be associated with the docker user. The values of DUID and DGID are visible in the Build Arguments, and can be accessed by the commands:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.user\u0026#34; }}\u0026#39; jrbeverly/wkhtmltopdf:baseimage docker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.group\u0026#34; }}\u0026#39; jrbeverly/wkhtmltopdf:baseimage The notation of the build variables is short form for docker user id (DUID) and docker group id (DGID).\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Creative Commons By 3.0.\nThe project icon is by cre.ativo mustard from the Noun Project.\n","id":144,"section":"posts","summary":"Dockerized WKHtmlToPDF Summary A super small image with wkhtmltopdf installed. The project icon is from cre.ativo mustard, HK from the Noun Project\nNOTE: This image is marked EOL, and use is discouraged.\nUsage You can use this image locally with docker run, calling wkhtmltopdf:\ndocker run -v $(pwd):/media/ jrbeverly/wkhtmltopdf:privileged wkhtmltopdf http://google.com google.pdf Gitlab You can setup a build job using .gitlab-ci.yml:\nbuild: image: jrbeverly/wkhtmltopdf:baseimage script: - wkhtmltopdf http://google.com google.pdf artifacts: paths: - google.","tags":["org:cardboardci"],"title":"ci-wkhtmltopdf","uri":"/2018/11/cardboardci-ci-wkhtmltopdf/","year":"2018"},{"content":"Dockerized Optipng Summary A super small Alpine image with OptiPNG installed. The project icon is from cre.ativo mustard, HK from the Noun Project\nNOTE: This image is marked EOL, and use is discouraged.\nUsage You can use this image locally with docker run, calling as such:\ndocker run -v $(pwd):/media/ jrbeverly/optipng:baseimage optipng test.png Gitlab You can setup a build job using .gitlab-ci.yml:\ncompile_pdf: image: jrbeverly/optipng:baseimage script: - optipng test.png artifacts: paths: - test.png Components Metadata Arguments Metadata build arguments used with the Label Schema Convention.\n   Variable Value Description     BUILD_DATE see metadata.variable The Date/Time the image was built.   VERSION see metadata.variable Release identifier for the contents of the image.   VCS_REF see metadata.variable Identifier for the version of the source code from which this image was built.    Build Arguments Build arguments used in the image.\n   Variable Value Description     USER see Makefile.options Sets the user to use when running the image.   DUID see user.variable The user id of the docker user.   DGID see user.variable The group id of the docker user\u0026rsquo;s group.    Volumes No volumes are exposed by the docker container. However, while running the image with limited permissions (baseimage), it is necessary to ensure that the docker user has permission to access mounted volumes. You will need to ensure that the docker user can read/write to the mounted volumes. (see User / Group Identifiers)\nThe working directory of the image is /media/.\nBuild Process To build the docker image, use the included Makefile. It is recommended to use the makefile to ensure all build arguments are provided.\nmake VERSION=\u0026lt;version\u0026gt; build You can view the build/README.md for more on using the Makefile to build the image.\nLabels The docker image follows the Label Schema Convention. Label Schema is a community project to provide a shared namespace for use by multiple tools, specifically org.label-schema. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.label-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/optipng Label Extension The label namespace org.doc-schema is an extension of org.label-schema. The namespace stores internal variables often used when interacting with the image. These variables will often be application versions or exposed internal variables. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/optipng User and Group Mapping All processes within the baseimage docker container will be run as the docker user, a non-root user. The docker user is created on build with the user id DUID and a member of a group with group id DGID.\nAny permissions on the host operating system (OS) associated with either the user (DUID) or group (DGID) will be associated with the docker user. The values of DUID and DGID are visible in the Build Arguments, and can be accessed by the commands:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.user\u0026#34; }}\u0026#39; jrbeverly/optipng:baseimage docker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.group\u0026#34; }}\u0026#39; jrbeverly/optipng:baseimage The notation of the build variables is short form for docker user id (DUID) and docker group id (DGID).\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Creative Commons By 3.0.\nThe project icon is by cre.ativo mustard from the Noun Project.\n","id":145,"section":"posts","summary":"Dockerized Optipng Summary A super small Alpine image with OptiPNG installed. The project icon is from cre.ativo mustard, HK from the Noun Project\nNOTE: This image is marked EOL, and use is discouraged.\nUsage You can use this image locally with docker run, calling as such:\ndocker run -v $(pwd):/media/ jrbeverly/optipng:baseimage optipng test.png Gitlab You can setup a build job using .gitlab-ci.yml:\ncompile_pdf: image: jrbeverly/optipng:baseimage script: - optipng test.png artifacts: paths: - test.","tags":["org:cardboardci"],"title":"ci-optipng","uri":"/2018/11/cardboardci-ci-optipng/","year":"2018"},{"content":"Dockerized Minify Summary A super small image with Minify. The project icon is from cre.ativo mustard, HK from the Noun Project\nNOTE: This image is marked EOL, and use is discouraged.\nUsage You can use this image locally with docker run, calling minify as such:\ndocker run -v /media/:/media/ jrbeverly/minify:privileged minify -o index-min.html index.html Gitlab You can setup a build job using .gitlab-ci.yml:\ncompile: image: jrbeverly/minify:baseimage script: - minify -o index-min.html index.html artifacts: paths: - index.html Components Metadata Arguments Metadata build arguments used with the Label Schema Convention.\n   Variable Value Description     BUILD_DATE see metadata.variable The Date/Time the image was built.   VERSION see metadata.variable Release identifier for the contents of the image.   VCS_REF see metadata.variable Identifier for the version of the source code from which this image was built.    Build Arguments Build arguments used in the image.\n   Variable Value Description     USER see Makefile.options Sets the user to use when running the image.   DUID see user.variable The user id of the docker user.   DGID see user.variable The group id of the docker user\u0026rsquo;s group.    Volumes No volumes are exposed by the docker container. However, while running the image with limited permissions (baseimage), it is necessary to ensure that the docker user has permission to access mounted volumes. You will need to ensure that the docker user can read/write to the mounted volumes. (see User / Group Identifiers)\nThe working directory of the image is /media/.\nBuild Process To build the docker image, use the included Makefile. It is recommended to use the makefile to ensure all build arguments are provided.\nmake VERSION=\u0026lt;version\u0026gt; build You can view the build/README.md for more on using the Makefile to build the image.\nLabels The docker image follows the Label Schema Convention. Label Schema is a community project to provide a shared namespace for use by multiple tools, specifically org.label-schema. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.label-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/minify Label Extension The label namespace org.doc-schema is an extension of org.label-schema. The namespace stores internal variables often used when interacting with the image. These variables will often be application versions or exposed internal variables. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/minify User and Group Mapping All processes within the baseimage docker container will be run as the docker user, a non-root user. The docker user is created on build with the user id DUID and a member of a group with group id DGID.\nAny permissions on the host operating system (OS) associated with either the user (DUID) or group (DGID) will be associated with the docker user. The values of DUID and DGID are visible in the Build Arguments, and can be accessed by the commands:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.user\u0026#34; }}\u0026#39; jrbeverly/minify:baseimage docker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.group\u0026#34; }}\u0026#39; jrbeverly/minify:baseimage The notation of the build variables is short form for docker user id (DUID) and docker group id (DGID).\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Creative Commons By 3.0.\nThe project icon is by cre.ativo mustard from the Noun Project.\n","id":146,"section":"posts","summary":"Dockerized Minify Summary A super small image with Minify. The project icon is from cre.ativo mustard, HK from the Noun Project\nNOTE: This image is marked EOL, and use is discouraged.\nUsage You can use this image locally with docker run, calling minify as such:\ndocker run -v /media/:/media/ jrbeverly/minify:privileged minify -o index-min.html index.html Gitlab You can setup a build job using .gitlab-ci.yml:\ncompile: image: jrbeverly/minify:baseimage script: - minify -o index-min.","tags":["org:cardboardci"],"title":"ci-minify","uri":"/2018/11/cardboardci-ci-minify/","year":"2018"},{"content":"DistributedRPC Summary A multi-client, multi-server environment that relies on a binder to facilitate an RPC system.\nGetting Started To make (\u0026ldquo;compile and link\u0026rdquo;) all components of the project, you can quickly get started with\nmake exec Or if you are doing quick debugging\nmake exec \u0026amp;\u0026amp; ./binder You can also invidiaully build each component with make \u0026lt;component\u0026gt;.\nNotes The project is over-commented to explain each line of code. This is for the purposes of explaining how the overall project connects together.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by marcela contreras from the Noun Project.\n","id":147,"section":"posts","summary":"DistributedRPC Summary A multi-client, multi-server environment that relies on a binder to facilitate an RPC system.\nGetting Started To make (\u0026ldquo;compile and link\u0026rdquo;) all components of the project, you can quickly get started with\nmake exec Or if you are doing quick debugging\nmake exec \u0026amp;\u0026amp; ./binder You can also invidiaully build each component with make \u0026lt;component\u0026gt;.\nNotes The project is over-commented to explain each line of code. This is for the purposes of explaining how the overall project connects together.","tags":["org:jrbeverly"],"title":"distributedrpc","uri":"/2018/11/jrbeverly-distributedrpc/","year":"2018"},{"content":"ContentBundler Summary A proof of concept for generation of strongly typed paths using the Roslyn Framework.\nGetting Started ContentBundler is provided as an command line application, originally adapted from an XNA Content Compiler. The new version greatly simplifies the code requirements, leveraging Roslyn for the code generation. An example is available from test/assets, which will generate the result [PatchQ.cs]\n./ContentBundler.exe --archive patch-Q.zip --file PatchQ.cs --class PatchQ --namespace PlatformerGame.Assets This will then output a series of static classes.\nnamespace PlatformerGame.Assets { using System; public static class PatchQ { public static class Textures { public readonly static string Tex0 = \u0026#34;textures/tex0.png\u0026#34;; public readonly static string Tex01 = \u0026#34;textures/tex01.png\u0026#34;; public readonly static string Tex02 = \u0026#34;textures/tex02.png\u0026#34;; public readonly static string Tex03 = \u0026#34;textures/tex03.png\u0026#34;; public readonly static string Tex04 = \u0026#34;textures/tex04.png\u0026#34;; public static class Backgrounds { public readonly static string Background = \u0026#34;textures/backgrounds/background.png\u0026#34;; } } public static class Images { public readonly static string Icon = \u0026#34;images/icon.png\u0026#34;; public readonly static string Logo = \u0026#34;images/logo.png\u0026#34;; } public static class Sounds { public readonly static string Day01 = \u0026#34;sounds/day01.wav\u0026#34;; public readonly static string Day02 = \u0026#34;sounds/day02.wav\u0026#34;; public readonly static string Day03 = \u0026#34;sounds/day03.wav\u0026#34;; public readonly static string Day04 = \u0026#34;sounds/day04.wav\u0026#34;; public readonly static string Night01 = \u0026#34;sounds/night01.wav\u0026#34;; public readonly static string Night02 = \u0026#34;sounds/night02.wav\u0026#34;; public readonly static string Night03 = \u0026#34;sounds/night03.wav\u0026#34;; public readonly static string Night04 = \u0026#34;sounds/night04.wav\u0026#34;; public readonly static string Night05 = \u0026#34;sounds/night05.wav\u0026#34;; } } } Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by jeanbaptiste sautelet from the Noun Project.\n","id":148,"section":"posts","summary":"ContentBundler Summary A proof of concept for generation of strongly typed paths using the Roslyn Framework.\nGetting Started ContentBundler is provided as an command line application, originally adapted from an XNA Content Compiler. The new version greatly simplifies the code requirements, leveraging Roslyn for the code generation. An example is available from test/assets, which will generate the result [PatchQ.cs]\n./ContentBundler.exe --archive patch-Q.zip --file PatchQ.cs --class PatchQ --namespace PlatformerGame.Assets This will then output a series of static classes.","tags":["org:jrbeverly"],"title":"contentbundler","uri":"/2018/11/jrbeverly-contentbundler/","year":"2018"},{"content":"Boxstarter.Workspace Summary A set of chocolatey packages for setting up a Windows PC.\nGetting Started The packages are not available on chocolatey.org, so they will need to be manually built. You can pack them up using the following:\ncd src/baseenv choco pack This will create a package.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Five by Five from the Noun Project.\n","id":149,"section":"posts","summary":"Boxstarter.Workspace Summary A set of chocolatey packages for setting up a Windows PC.\nGetting Started The packages are not available on chocolatey.org, so they will need to be manually built. You can pack them up using the following:\ncd src/baseenv choco pack This will create a package.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.","tags":["org:devkitspaces"],"title":"boxstarter-workspace","uri":"/2018/11/devkitspaces-boxstarter-workspace/","year":"2018"},{"content":"UdaciCards Summary For the UdaciCards project, you will build a mobile application (Android or iOS - or both) that allows users to study collections of flashcards. The app will allow users to create different categories of flashcards called \u0026ldquo;decks\u0026rdquo;, add flashcards to those decks, then take quizzes on those decks.\nInstallation You can install the project dependencies using:\ncd src/udacicards/ yarn The you can start the application:\nyarn start To troubleshoot the issues of the application, you can review React Native - Getting Started.\nSupported OS Application was tested using Android Emulators.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Tim Holman from the Noun Project.\n","id":150,"section":"posts","summary":"UdaciCards Summary For the UdaciCards project, you will build a mobile application (Android or iOS - or both) that allows users to study collections of flashcards. The app will allow users to create different categories of flashcards called \u0026ldquo;decks\u0026rdquo;, add flashcards to those decks, then take quizzes on those decks.\nInstallation You can install the project dependencies using:\ncd src/udacicards/ yarn The you can start the application:\nyarn start To troubleshoot the issues of the application, you can review React Native - Getting Started.","tags":["org:jrbeverly"],"title":"udacicards","uri":"/2018/04/jrbeverly-udacicards/","year":"2018"},{"content":"JollyBot Summary Prisoners' Dilemma A.I. bot performing an \u0026lsquo;Olive Branch\u0026rsquo; strategy focusing on attempting to cooperate whenever possible. The bot attempts to establish cooperation, even in cases where the opposing agent may appear hostile (e.g. always defect).\nDescription The iterated prisoners dilemma is a classic two-person game which consists of a number of rounds. In each round, each person can either defect by taking $1 from a (common) pile, or cooperate by giving $2 from the same pile to the other person. A purely rational agent will optimise over his expected long-term payoffs, possibly by averaging over his expectations of his opponents type (or strategy).\nJollyBot works by always attempting to cooperate with the opposing agent, attempting to establish mutual cooperation between the agents whenever reasonable. Even in the case of a more aggressive agent, JollyBot tries to remain jolly. The bot was developed as part of an assignment in an Artificial Intelligence course, with the goal of ultimately participating in a tournament against other submitted agents. JollyBot faced off against the other agents, emerging victorious by being a really swell fella.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Matt Brooks from the Noun Project.\n","id":151,"section":"posts","summary":"JollyBot Summary Prisoners' Dilemma A.I. bot performing an \u0026lsquo;Olive Branch\u0026rsquo; strategy focusing on attempting to cooperate whenever possible. The bot attempts to establish cooperation, even in cases where the opposing agent may appear hostile (e.g. always defect).\nDescription The iterated prisoners dilemma is a classic two-person game which consists of a number of rounds. In each round, each person can either defect by taking $1 from a (common) pile, or cooperate by giving $2 from the same pile to the other person.","tags":["org:jrbeverly"],"title":"jollybot","uri":"/2018/04/jrbeverly-jollybot/","year":"2018"},{"content":"Readable Udacity Nanodegree React Project\nInstalling Server  cd src/api npm install node start  Installing Client  cd src/client npm install npm start  API Server Information about the API server and how to use it can be found in its README file.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Srikanta from the Noun Project.\n","id":152,"section":"posts","summary":"Readable Udacity Nanodegree React Project\nInstalling Server  cd src/api npm install node start  Installing Client  cd src/client npm install npm start  API Server Information about the API server and how to use it can be found in its README file.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.","tags":["org:jrbeverly"],"title":"readable","uri":"/2018/01/jrbeverly-readable/","year":"2018"},{"content":"MyReads Project Summary A digital bookshelf app that allows you to select and categorize books you have read, are currently reading, or want to read.\nDescription MyReads is a digital bookshelf app that allows you to select and categorize books you have read, are currently reading, or want to read. It is built as a project for Udacity React Nanodegree.\nGetting Started The backend API uses a fixed set of cached search results and is limited to a particular set of search terms, which can be found in SEARCH_TERMS.md. That list of terms are the only terms that will work with the backend, so don\u0026rsquo;t be surprised if your searches for Basket Weaving or Bubble Wrap don\u0026rsquo;t come back with any results.\nBuild To start development with the project, you will need to run the following command:\nnpm install This will install all node modules that the project relies on. You can then start the web server using the following command;\nnpm start You can then open localhost:3000 to view the MyReads in the browser. The page will automatically reload if you make changes to the code.\nDependencies The project relies on the installation on Node.js.\nReact This project was bootstrapped with Create React App. You can find more information on how to perform common tasks here.\nBooks Web Service The MyReads application makes use of the Udacity React Books API. The service provides methods for searching a catalog of books based on a predefined list of terms. These terms are available at docs/SEARCH_TERMS.md.\nYou can read more about the web service at docs/API.md.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Marjon Siero from the Noun Project.\n","id":153,"section":"posts","summary":"MyReads Project Summary A digital bookshelf app that allows you to select and categorize books you have read, are currently reading, or want to read.\nDescription MyReads is a digital bookshelf app that allows you to select and categorize books you have read, are currently reading, or want to read. It is built as a project for Udacity React Nanodegree.\nGetting Started The backend API uses a fixed set of cached search results and is limited to a particular set of search terms, which can be found in SEARCH_TERMS.","tags":["org:jrbeverly"],"title":"myreads","uri":"/2017/11/jrbeverly-myreads/","year":"2017"},{"content":"Vagrant Docker Desktop Summary Provide a method of reproducible graphical development environments based on Linux. This repository provides a base Docker Desktop environment, sandboxed on your local computer.\nGetting Started You can use this locally with vagrant up, calling as such:\nvagrant --name=mydesktop up However It is recommended to use the script create.sh for the first run to ensure all necessary arguments are provided. The provided arguments creates a settings.yaml, storing the settings for the machine. You can create the machine by calling:\nsh create.sh -n mydesktop -d ubuntu If you want more information about the script create.sh, you can do so by calling:\nsh create.sh -h Parameters The parameters are used in the calling of vagrant up, primarily as vagrant [OPTIONS] up. After provisioning the environment, a settings file (setting.yaml) is created, which stores the provided parameters.\n   Name Type Description     name string Name of the provisioned desktop environment   desktop filename The name of the desktop provisioning script. These scripts are present in packaging/environments.    The vagrant environment is based on the bento/ubuntu images. If the timezone is not set, the provision script will attempt to auto-detect the timezone using tzupdate.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Maxi Koichi from the Noun Project.\n","id":154,"section":"posts","summary":"Vagrant Docker Desktop Summary Provide a method of reproducible graphical development environments based on Linux. This repository provides a base Docker Desktop environment, sandboxed on your local computer.\nGetting Started You can use this locally with vagrant up, calling as such:\nvagrant --name=mydesktop up However It is recommended to use the script create.sh for the first run to ensure all necessary arguments are provided. The provided arguments creates a settings.yaml, storing the settings for the machine.","tags":["org:cardboardci"],"title":"vagrant-desktop-docker","uri":"/2017/09/cardboardci-vagrant-desktop-docker/","year":"2017"},{"content":"Vagrant Homelab Desktop Summary Provide a method of reproducible graphical development environments based on Linux. This repository provides a base Homelab Desktop environment, sandboxed on your local computer.\nGetting Started You can use this locally with vagrant up, calling as such:\nvagrant --name=mydesktop up However It is recommended to use the script create.sh for the first run to ensure all necessary arguments are provided. The provided arguments creates a settings.yaml, storing the settings for the machine. You can create the machine by calling:\nsh create.sh -n mydesktop -d ubuntu If you want more information about the script create.sh, you can do so by calling:\nsh create.sh -h Parameters The parameters are used in the calling of vagrant up, primarily as vagrant [OPTIONS] up. After provisioning the environment, a settings file (setting.yaml) is created, which stores the provided parameters.\n   Name Type Description     name string Name of the provisioned desktop environment   desktop filename The name of the desktop provisioning script. These scripts are present in packaging/environments.    The vagrant environment is based on the bento/ubuntu images. If the timezone is not set, the provision script will attempt to auto-detect the timezone using tzupdate.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Maxi Koichi from the Noun Project.\n","id":155,"section":"posts","summary":"Vagrant Homelab Desktop Summary Provide a method of reproducible graphical development environments based on Linux. This repository provides a base Homelab Desktop environment, sandboxed on your local computer.\nGetting Started You can use this locally with vagrant up, calling as such:\nvagrant --name=mydesktop up However It is recommended to use the script create.sh for the first run to ensure all necessary arguments are provided. The provided arguments creates a settings.yaml, storing the settings for the machine.","tags":["org:devkitspaces"],"title":"vagrant-desktop-homelab","uri":"/2017/09/devkitspaces-vagrant-desktop-homelab/","year":"2017"},{"content":"Githooks Summary GitHooks provides a multi-hook framework for Git Hooks, along with a collection of scripts for the purposes of encouraging a commit policy, altering the project environment depending on the state of the repository, and implementing continuous integration workflows. The framework allows multi-script execution, you can use GitHooks to automate or optimize virtually any aspect of your development workflow.\nGetting Started Git Hooks are event-based scripts you can place in a hooks directory to trigger actions at certain points in gits execution. When you run certain git commands, the software will run the associated script within the git repository. GitHooks extends on this by enabling the installation of any arbitrary number of hooks for a command.\nInstallation You can install hooks in a git repository by two methods: unzipping then deleting, or manually copying in the scripts. Either option you will need to use the command line to setup your git repository, however it is recommended to use the unzip method as it is less likely to encounter mistakes (forgetting to set execution bit, missing a script, etc).\nUnzip githooks You can quickly install a set of hooks into the git repository by unzipping githooks.zip into the .git/hooks/ directory. To install all hooks and sub-hooks, you can do so with the following:\nunzip githooks.zip -d .git/hooks/ chmod +x .git/hooks/* You can then delete any hooks from the .git/hooks/ directory that you do not wish to use.\nManual Copying To manually install a git hook, you will need to start by copying the hook into the .git/hooks/ directory. The git hook, also known as the entrypoint hook, is responsible for executing scripts in a sub-directory. If you wish to install the commit-msg hook, you can do the following:\ncp commit-msg .git/hooks/commit-msg mkdir -p .git/hooks/commit-msg.d/ chmod +x .git/hooks/commit-msg After copying in the entrypoint hook, you will be able to copy hooks into a sub-directory named after the hook (e.g. commit-msg.d/). These hooks will be run by the entrypoint hook, commit-msg. To add a hook, you can do so with the following:\ncp 001-my-githook.sh .git/hooks/commit-msg.d/ This will install the hook 001-my-githook.sh into the commit-msg.d/ directory. When the entrypoint commit-msg is executed, it will call any scripts in the commit-msg.d/ directory. The entrypoint hook can be of the form of any supported git hook (applypatch-msg, commit-msg, post-update, pre-applypatch, pre-commit, etc).\nAcknowledgements The project icon is by Mario Gallego Adn from the Noun Project.\n","id":156,"section":"posts","summary":"Githooks Summary GitHooks provides a multi-hook framework for Git Hooks, along with a collection of scripts for the purposes of encouraging a commit policy, altering the project environment depending on the state of the repository, and implementing continuous integration workflows. The framework allows multi-script execution, you can use GitHooks to automate or optimize virtually any aspect of your development workflow.\nGetting Started Git Hooks are event-based scripts you can place in a hooks directory to trigger actions at certain points in gits execution.","tags":["org:jrbeverly"],"title":"githooks","uri":"/2017/09/jrbeverly-githooks/","year":"2017"},{"content":"Docker Alpine Base Summary A super small image with basic development libraries installed. The project icon is from cre.ativo mustard, HK from the Noun Project\nNOTE: This image is marked EOL, and use is discouraged.\nUsage You can use this image locally with docker run, calling sh to enter the container:\ndocker run -it -v /media/:/media/ jrbeverly/alpine:edge sh Gitlab You can setup a build job using .gitlab-ci.yml:\ncompile: image: jrbeverly/alpine:edge script: - echo \u0026#34;Hello world\u0026#34; Components Metadata Arguments Metadata build arguments used with the Label Schema Convention.\n   Variable Value Description     BUILD_DATE see metadata.variable The Date/Time the image was built.   VERSION see metadata.variable Release identifier for the contents of the image.   VCS_REF see metadata.variable Identifier for the version of the source code from which this image was built.    Build Arguments Build arguments used in the image.\n   Variable Value Description     USER see Makefile.options Sets the user to use when running the image.   DUID see user.variable The user id of the docker user.   DGID see user.variable The group id of the docker user\u0026rsquo;s group.    Volumes No volumes are exposed by the docker container. However, while running the image with limited permissions (baseimage), it is necessary to ensure that the docker user has permission to access mounted volumes. You will need to ensure that the docker user can read/write to the mounted volumes.\nThe working directory of the image is /media/.\nBuild Process To build the docker image, use the included Makefile. It is recommended to use the makefile to ensure all build arguments are provided.\nmake VERSION=\u0026lt;version\u0026gt; build You can view the build/README.md for more on using the Makefile to build the image.\nLabels The docker image follows the Label Schema Convention. Label Schema is a community project to provide a shared namespace for use by multiple tools, specifically org.label-schema. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.label-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/alpine Label Extension The label namespace org.doc-schema is an extension of org.label-schema. The namespace stores internal variables often used when interacting with the image. These variables will often be application versions or exposed internal variables. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/alpine Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Creative Commons By 3.0.\nThe project icon is by cre.ativo mustard from the Noun Project.\n","id":157,"section":"posts","summary":"Docker Alpine Base Summary A super small image with basic development libraries installed. The project icon is from cre.ativo mustard, HK from the Noun Project\nNOTE: This image is marked EOL, and use is discouraged.\nUsage You can use this image locally with docker run, calling sh to enter the container:\ndocker run -it -v /media/:/media/ jrbeverly/alpine:edge sh Gitlab You can setup a build job using .gitlab-ci.yml:\ncompile: image: jrbeverly/alpine:edge script: - echo \u0026#34;Hello world\u0026#34; Components Metadata Arguments Metadata build arguments used with the Label Schema Convention.","tags":["org:cardboardci"],"title":"ci-alpine","uri":"/2017/05/cardboardci-ci-alpine/","year":"2017"},{"content":"Dockerized pdf2htmlEX Summary A super small image with pdf2htmlEX installed. The project icon is from cre.ativo mustard, HK from the Noun Project\nNOTE: This image is marked EOL, and use is discouraged.\nUsage You can use this image locally with docker run, calling pdf2htmlEX:\ndocker run -v /media/:/media/ jrbeverly/pdf2htmlEX:privileged pdf2htmlEX report.pdf Gitlab You can setup a build job using .gitlab-ci.yml:\ncompile: image: jrbeverly/pdf2htmlEX:baseimage script: - pdf2htmlEX report.pdf artifacts: paths: - report.pdf Components Metadata Arguments Metadata build arguments used with the Label Schema Convention.\n   Variable Value Description     BUILD_DATE see metadata.variable The Date/Time the image was built.   VERSION see metadata.variable Release identifier for the contents of the image.   VCS_REF see metadata.variable Identifier for the version of the source code from which this image was built.    Build Arguments Build arguments used in the image.\n   Variable Value Description     USER see Makefile.options Sets the user to use when running the image.   DUID see user.variable The user id of the docker user.   DGID see user.variable The group id of the docker user\u0026rsquo;s group.    Volumes No volumes are exposed by the docker container. However, while running the image with limited permissions (baseimage), it is necessary to ensure that the docker user has permission to access mounted volumes. You will need to ensure that the docker user can read/write to the mounted volumes. (see User / Group Identifiers)\nThe working directory of the image is /media/.\nBuild Process To build the docker image, use the included Makefile. It is recommended to use the makefile to ensure all build arguments are provided.\nmake VERSION=\u0026lt;version\u0026gt; build You can view the build/README.md for more on using the Makefile to build the image.\nLabels The docker image follows the Label Schema Convention. Label Schema is a community project to provide a shared namespace for use by multiple tools, specifically org.label-schema. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.label-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/pdf2htmlEX Label Extension The label namespace org.doc-schema is an extension of org.label-schema. The namespace stores internal variables often used when interacting with the image. These variables will often be application versions or exposed internal variables. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/pdf2htmlEX User and Group Mapping All processes within the baseimage docker container will be run as the docker user, a non-root user. The docker user is created on build with the user id DUID and a member of a group with group id DGID.\nAny permissions on the host operating system (OS) associated with either the user (DUID) or group (DGID) will be associated with the docker user. The values of DUID and DGID are visible in the Build Arguments, and can be accessed by the commands:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.user\u0026#34; }}\u0026#39; jrbeverly/pdf2htmlEX:baseimage docker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.group\u0026#34; }}\u0026#39; jrbeverly/pdf2htmlEX:baseimage The notation of the build variables is short form for docker user id (DUID) and docker group id (DGID).\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Creative Commons By 3.0.\nThe project icon is by cre.ativo mustard from the Noun Project.\n","id":158,"section":"posts","summary":"Dockerized pdf2htmlEX Summary A super small image with pdf2htmlEX installed. The project icon is from cre.ativo mustard, HK from the Noun Project\nNOTE: This image is marked EOL, and use is discouraged.\nUsage You can use this image locally with docker run, calling pdf2htmlEX:\ndocker run -v /media/:/media/ jrbeverly/pdf2htmlEX:privileged pdf2htmlEX report.pdf Gitlab You can setup a build job using .gitlab-ci.yml:\ncompile: image: jrbeverly/pdf2htmlEX:baseimage script: - pdf2htmlEX report.pdf artifacts: paths: - report.","tags":["org:cardboardci"],"title":"ci-pdf2htmlex","uri":"/2017/05/cardboardci-ci-pdf2htmlex/","year":"2017"},{"content":"Dockerized GLibC Summary A super small image with glibc installed, to allow binaries compiled against glibc to work. The project icon is from cre.ativo mustard, HK from the Noun Project\nNOTE: This image is marked EOL, and use is discouraged.\nUsage You can use this image locally with docker run, calling sh to enter the container:\ndocker run -v /media/:/media/ jrbeverly/glibc:privileged echo \u0026#34;hello\u0026#34; Gitlab You can setup a build job using .gitlab-ci.yml:\ncompile: image: jrbeverly/glibc:baseimage script: - echo \u0026#34;hello\u0026#34; Components Metadata Arguments Metadata build arguments used with the Label Schema Convention.\n   Variable Value Description     BUILD_DATE see metadata.variable The Date/Time the image was built.   VERSION see metadata.variable Release identifier for the contents of the image.   VCS_REF see metadata.variable Identifier for the version of the source code from which this image was built.    Build Arguments Build arguments used in the image.\n   Variable Value Description     USER see Makefile.options Sets the user to use when running the image.   DUID see user.variable The user id of the docker user.   DGID see user.variable The group id of the docker user\u0026rsquo;s group.    Volumes No volumes are exposed by the docker container. However, while running the image with limited permissions (baseimage), it is necessary to ensure that the docker user has permission to access mounted volumes. You will need to ensure that the docker user can read/write to the mounted volumes. (see User / Group Identifiers)\nThe working directory of the image is /media/.\nBuild Process To build the docker image, use the included Makefile. It is recommended to use the makefile to ensure all build arguments are provided.\nmake VERSION=\u0026lt;version\u0026gt; build You can view the build/README.md for more on using the Makefile to build the image.\nLabels The docker image follows the Label Schema Convention. Label Schema is a community project to provide a shared namespace for use by multiple tools, specifically org.label-schema. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.label-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/glibc Label Extension The label namespace org.doc-schema is an extension of org.label-schema. The namespace stores internal variables often used when interacting with the image. These variables will often be application versions or exposed internal variables. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/glibc User and Group Mapping All processes within the baseimage docker container will be run as the docker user, a non-root user. The docker user is created on build with the user id DUID and a member of a group with group id DGID.\nAny permissions on the host operating system (OS) associated with either the user (DUID) or group (DGID) will be associated with the docker user. The values of DUID and DGID are visible in the Build Arguments, and can be accessed by the commands:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.user\u0026#34; }}\u0026#39; jrbeverly/glibc:baseimage docker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.group\u0026#34; }}\u0026#39; jrbeverly/glibc:baseimage The notation of the build variables is short form for docker user id (DUID) and docker group id (DGID).\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Creative Commons By 3.0.\nThe project icon is by cre.ativo mustard from the Noun Project.\n","id":159,"section":"posts","summary":"Dockerized GLibC Summary A super small image with glibc installed, to allow binaries compiled against glibc to work. The project icon is from cre.ativo mustard, HK from the Noun Project\nNOTE: This image is marked EOL, and use is discouraged.\nUsage You can use this image locally with docker run, calling sh to enter the container:\ndocker run -v /media/:/media/ jrbeverly/glibc:privileged echo \u0026#34;hello\u0026#34; Gitlab You can setup a build job using .","tags":["org:cardboardci"],"title":"ci-glibc","uri":"/2017/05/cardboardci-ci-glibc/","year":"2017"},{"content":"Dockerized X Window System Summary A super small image with X Window System development libraries installed. The project icon is from cre.ativo mustard, HK from the Noun Project\nNOTE: This image is marked EOL, and use is discouraged.\nUsage You can use this image locally with docker run, calling g++ to build X Window System applications:\ndocker run -v $(pwd):/media/ jrbeverly/xwindow:privileged g++ myxapp.cpp -o xapp Gitlab You can setup a build job using .gitlab-ci.yml:\nbuild: image: jrbeverly/xwindow:baseimage script: - g++ myxapp.cpp -o xapp artifacts: paths: - xapp Components Metadata Arguments Metadata build arguments used with the Label Schema Convention.\n   Variable Value Description     BUILD_DATE see metadata.variable The Date/Time the image was built.   VERSION see metadata.variable Release identifier for the contents of the image.   VCS_REF see metadata.variable Identifier for the version of the source code from which this image was built.    Build Arguments Build arguments used in the image.\n   Variable Value Description     USER see Makefile.options Sets the user to use when running the image.   DUID see user.variable The user id of the docker user.   DGID see user.variable The group id of the docker user\u0026rsquo;s group.    Volumes No volumes are exposed by the docker container. However, while running the image with limited permissions (baseimage), it is necessary to ensure that the docker user has permission to access mounted volumes. You will need to ensure that the docker user can read/write to the mounted volumes. (see User / Group Identifiers)\nThe working directory of the image is /media/.\nBuild Process To build the docker image, use the included Makefile. It is recommended to use the makefile to ensure all build arguments are provided.\nmake VERSION=\u0026lt;version\u0026gt; build You can view the build/README.md for more on using the Makefile to build the image.\nLabels The docker image follows the Label Schema Convention. Label Schema is a community project to provide a shared namespace for use by multiple tools, specifically org.label-schema. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.label-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/xwindow:\u0026lt;TAG\u0026gt; Label Extension The label namespace org.doc-schema is an extension of org.label-schema. The namespace stores internal variables often used when interacting with the image. These variables will often be application versions or exposed internal variables. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/xwindow:\u0026lt;TAG\u0026gt; User and Group Mapping All processes within the baseimage docker container will be run as the docker user, a non-root user. The docker user is created on build with the user id DUID and a member of a group with group id DGID.\nAny permissions on the host operating system (OS) associated with either the user (DUID) or group (DGID) will be associated with the docker user. The values of DUID and DGID are visible in the Build Arguments, and can be accessed by the commands:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.user\u0026#34; }}\u0026#39; jrbeverly/xwindow:baseimage docker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.group\u0026#34; }}\u0026#39; jrbeverly/xwindow:baseimage The notation of the build variables is short form for docker user id (DUID) and docker group id (DGID).\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Creative Commons By 3.0.\nThe project icon is by cre.ativo mustard from the Noun Project.\n","id":160,"section":"posts","summary":"Dockerized X Window System Summary A super small image with X Window System development libraries installed. The project icon is from cre.ativo mustard, HK from the Noun Project\nNOTE: This image is marked EOL, and use is discouraged.\nUsage You can use this image locally with docker run, calling g++ to build X Window System applications:\ndocker run -v $(pwd):/media/ jrbeverly/xwindow:privileged g++ myxapp.cpp -o xapp Gitlab You can setup a build job using .","tags":["org:xplatformer"],"title":"docker-xwindow","uri":"/2017/05/xplatformer-docker-xwindow/","year":"2017"},{"content":"Dockerized LaTeX Summary Comprehensive TeX document production system for use as a continuous integration image. The project icon is from cre.ativo mustard, HK from the Noun Project\nNOTE: This image is marked EOL, and use is discouraged.\nUsage You can use this image locally with docker run, calling latexmk as such:\ndocker run -v /media:/media jrbeverly/latex:full latexmk -pdf manual.tex Gitlab You can setup a build job using .gitlab-ci.yml:\ncompile_pdf: image: jrbeverly/latex:full script: - latexmk -pdf manual.tex artifacts: paths: - manual.pdf Components Metadata Arguments Metadata build arguments used with the Label Schema Convention.\n   Variable Value Description     BUILD_DATE see metadata.variable The Date/Time the image was built.   VERSION see metadata.variable Release identifier for the contents of the image.   VCS_REF see metadata.variable Identifier for the version of the source code from which this image was built.    Build Arguments Build arguments used in the image.\n   Variable Value Description     USER see Makefile.options Sets the user to use when running the image.   DUID see user.variable The user id of the docker user.   DGID see user.variable The group id of the docker user\u0026rsquo;s group.    Volumes No volumes are exposed by the docker container. However, while running the image with limited permissions (baseimage), it is necessary to ensure that the docker user has permission to access mounted volumes. You will need to ensure that the docker user can read/write to the mounted volumes. (see User / Group Identifiers)\nThe working directory of the image is /media/.\nBuild Process To build the docker image, use the included Makefile. It is recommended to use the makefile to ensure all build arguments are provided.\nmake VERSION=\u0026lt;version\u0026gt; build You can view the build/README.md for more on using the Makefile to build the image.\nLabels The docker image follows the Label Schema Convention. Label Schema is a community project to provide a shared namespace for use by multiple tools, specifically org.label-schema. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.label-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/latex Label Extension The label namespace org.doc-schema is an extension of org.label-schema. The namespace stores internal variables often used when interacting with the image. These variables will often be application versions or exposed internal variables. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/latex User and Group Mapping All processes within the baseimage docker container will be run as the docker user, a non-root user. The docker user is created on build with the user id DUID and a member of a group with group id DGID.\nAny permissions on the host operating system (OS) associated with either the user (DUID) or group (DGID) will be associated with the docker user. The values of DUID and DGID are visible in the Build Arguments, and can be accessed by the commands:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.user\u0026#34; }}\u0026#39; jrbeverly/latex:baseimage docker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.group\u0026#34; }}\u0026#39; jrbeverly/latex:baseimage The notation of the build variables is short form for docker user id (DUID) and docker group id (DGID).\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Creative Commons By 3.0.\nThe project icon is by cre.ativo mustard from the Noun Project.\n","id":161,"section":"posts","summary":"Dockerized LaTeX Summary Comprehensive TeX document production system for use as a continuous integration image. The project icon is from cre.ativo mustard, HK from the Noun Project\nNOTE: This image is marked EOL, and use is discouraged.\nUsage You can use this image locally with docker run, calling latexmk as such:\ndocker run -v /media:/media jrbeverly/latex:full latexmk -pdf manual.tex Gitlab You can setup a build job using .gitlab-ci.yml:\ncompile_pdf: image: jrbeverly/latex:full script: - latexmk -pdf manual.","tags":["org:cardboardci"],"title":"ci-latex","uri":"/2017/05/cardboardci-ci-latex/","year":"2017"},{"content":"Boilerplate for Docker Repository Summary A boilerplate template for specifying a docker image using the makefile build approach. The project icon is from cre.ativo mustard, HK from the Noun Project\nThis model of creating docker image has been deprecated.\nComponents Metadata Arguments Metadata build arguments used with the Label Schema Convention.\n   Variable Value Description     BUILD_DATE see metadata.variable The Date/Time the image was built.   VERSION see metadata.variable Release identifier for the contents of the image.   VCS_REF see metadata.variable Identifier for the version of the source code from which this image was built.    Build Arguments Build arguments used in the image.\n   Variable Value Description     USER see Makefile.options Sets the user to use when running the image.   DUID see user.variable The user id of the docker user.   DGID see user.variable The group id of the docker user\u0026rsquo;s group.    Volumes No volumes are exposed by the docker container. However, while running the image with limited permissions (baseimage), it is necessary to ensure that the docker user has permission to access mounted volumes. You will need to ensure that the docker user can read/write to the mounted volumes. (see User / Group Identifiers)\nThe working directory of the image is /media/.\nBuild Process To build the docker image, use the included Makefile. It is recommended to use the makefile to ensure all build arguments are provided.\nmake VERSION=\u0026lt;version\u0026gt; build You can view the build/README.md for more on using the Makefile to build the image.\nLabels The docker image follows the Label Schema Convention. Label Schema is a community project to provide a shared namespace for use by multiple tools, specifically org.label-schema. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.label-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; [IMAGE] Label Extension The label namespace org.doc-schema is an extension of org.label-schema. The namespace stores internal variables often used when interacting with the image. These variables will often be application versions or exposed internal variables. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; [IMAGE] User and Group Mapping All processes within the baseimage docker container will be run as the docker user, a non-root user. The docker user is created on build with the user id DUID and a member of a group with group id DGID.\nAny permissions on the host operating system (OS) associated with either the user (DUID) or group (DGID) will be associated with the docker user. The values of DUID and DGID are visible in the Build Arguments, and can be accessed by the commands:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.user\u0026#34; }}\u0026#39; [IMAGE] docker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.group\u0026#34; }}\u0026#39; [IMAGE] The notation of the build variables is short form for docker user id (DUID) and docker group id (DGID).\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Creative Commons By 3.0.\nThe project icon is by cre.ativo mustard from the Noun Project.\n","id":162,"section":"posts","summary":"Boilerplate for Docker Repository Summary A boilerplate template for specifying a docker image using the makefile build approach. The project icon is from cre.ativo mustard, HK from the Noun Project\nThis model of creating docker image has been deprecated.\nComponents Metadata Arguments Metadata build arguments used with the Label Schema Convention.\n   Variable Value Description     BUILD_DATE see metadata.variable The Date/Time the image was built.   VERSION see metadata.","tags":["org:cardboardci"],"title":"ci-boilerplate","uri":"/2017/05/cardboardci-ci-boilerplate/","year":"2017"},{"content":"Docker Baseimage Summary A super small image to act as a baseimage for continuous integration images. The project icon is from cre.ativo mustard, HK from the Noun Project\nNOTE: This image is marked EOL, and use is discouraged.\nUsage You can use this image locally with docker run, calling sh to enter the container:\ndocker run -it -v /media/:/media/ jrbeverly/baseimage:baseimage echo \u0026#34;hello\u0026#34; Gitlab You can setup a build job using .gitlab-ci.yml:\ncompile: image: jrbeverly/baseimage:alpine script: - echo \u0026#34;hello\u0026#34; Components Metadata Arguments Metadata build arguments used with the Label Schema Convention.\n   Variable Value Description     BUILD_DATE see metadata.variable The Date/Time the image was built.   VERSION see metadata.variable Release identifier for the contents of the image.   VCS_REF see metadata.variable Identifier for the version of the source code from which this image was built.    Build Arguments Build arguments used in the image.\n   Variable Value Description     USER see Makefile.options Sets the user to use when running the image.   DUID see user.variable The user id of the docker user.   DGID see user.variable The group id of the docker user\u0026rsquo;s group.    Volumes No volumes are exposed by the docker container. However, while running the image with limited permissions (baseimage), it is necessary to ensure that the docker user has permission to access mounted volumes. You will need to ensure that the docker user can read/write to the mounted volumes.\nThe working directory of the image is /media/.\nBuild Process To build the docker image, use the included Makefile. It is recommended to use the makefile to ensure all build arguments are provided.\nmake VERSION=\u0026lt;version\u0026gt; build You can view the build/README.md for more on using the Makefile to build the image.\nLabels The docker image follows the Label Schema Convention. Label Schema is a community project to provide a shared namespace for use by multiple tools, specifically org.label-schema. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.label-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/baseimage Label Extension The label namespace org.doc-schema is an extension of org.label-schema. The namespace stores internal variables often used when interacting with the image. These variables will often be application versions or exposed internal variables. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/baseimage Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Creative Commons By 3.0.\nThe project icon is by cre.ativo mustard from the Noun Project.\n","id":163,"section":"posts","summary":"Docker Baseimage Summary A super small image to act as a baseimage for continuous integration images. The project icon is from cre.ativo mustard, HK from the Noun Project\nNOTE: This image is marked EOL, and use is discouraged.\nUsage You can use this image locally with docker run, calling sh to enter the container:\ndocker run -it -v /media/:/media/ jrbeverly/baseimage:baseimage echo \u0026#34;hello\u0026#34; Gitlab You can setup a build job using .","tags":["org:cardboardci"],"title":"ci-baseimage","uri":"/2017/05/cardboardci-ci-baseimage/","year":"2017"},{"content":"Dockerized RSvg Summary A super small image with librsvg installed. The project icon is from cre.ativo mustard, HK from the Noun Project\nNOTE: This image is marked EOL, and use is discouraged.\nUsage You can use this image locally with docker run, calling rsvg-convert to rasterize an image:\ndocker run -v $(pwd):/media/ jrbeverly/rsvg:privileged rsvg-convert test.svg -o test.png Gitlab You can setup a build job using .gitlab-ci.yml:\nbuild: image: jrbeverly/rsvg:baseimage script: - rsvg-convert test.svg -o test.png artifacts: paths: - test.png Components Metadata Arguments Metadata build arguments used with the Label Schema Convention.\n   Variable Value Description     BUILD_DATE see metadata.variable The Date/Time the image was built.   VERSION see metadata.variable Release identifier for the contents of the image.   VCS_REF see metadata.variable Identifier for the version of the source code from which this image was built.    Build Arguments Build arguments used in the image.\n   Variable Value Description     USER see Makefile.options Sets the user to use when running the image.   DUID see user.variable The user id of the docker user.   DGID see user.variable The group id of the docker user\u0026rsquo;s group.    Volumes No volumes are exposed by the docker container. However, while running the image with limited permissions (baseimage), it is necessary to ensure that the docker user has permission to access mounted volumes. You will need to ensure that the docker user can read/write to the mounted volumes. (see User / Group Identifiers)\nThe working directory of the image is /media/.\nBuild Process To build the docker image, use the included Makefile. It is recommended to use the makefile to ensure all build arguments are provided.\nmake VERSION=\u0026lt;version\u0026gt; build You can view the build/README.md for more on using the Makefile to build the image.\nLabels The docker image follows the Label Schema Convention. Label Schema is a community project to provide a shared namespace for use by multiple tools, specifically org.label-schema. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.label-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/rsvg:\u0026lt;TAG\u0026gt; Label Extension The label namespace org.doc-schema is an extension of org.label-schema. The namespace stores internal variables often used when interacting with the image. These variables will often be application versions or exposed internal variables. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/rsvg:\u0026lt;TAG\u0026gt; User and Group Mapping All processes within the baseimage docker container will be run as the docker user, a non-root user. The docker user is created on build with the user id DUID and a member of a group with group id DGID.\nAny permissions on the host operating system (OS) associated with either the user (DUID) or group (DGID) will be associated with the docker user. The values of DUID and DGID are visible in the Build Arguments, and can be accessed by the commands:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.user\u0026#34; }}\u0026#39; jrbeverly/rsvg:baseimage docker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.group\u0026#34; }}\u0026#39; jrbeverly/rsvg:baseimage The notation of the build variables is short form for docker user id (DUID) and docker group id (DGID).\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Creative Commons By 3.0.\nThe project icon is by cre.ativo mustard from the Noun Project.\n","id":164,"section":"posts","summary":"Dockerized RSvg Summary A super small image with librsvg installed. The project icon is from cre.ativo mustard, HK from the Noun Project\nNOTE: This image is marked EOL, and use is discouraged.\nUsage You can use this image locally with docker run, calling rsvg-convert to rasterize an image:\ndocker run -v $(pwd):/media/ jrbeverly/rsvg:privileged rsvg-convert test.svg -o test.png Gitlab You can setup a build job using .gitlab-ci.yml:\nbuild: image: jrbeverly/rsvg:baseimage script: - rsvg-convert test.","tags":["org:cardboardci"],"title":"ci-rsvg","uri":"/2017/05/cardboardci-ci-rsvg/","year":"2017"},{"content":"Automated Windows Setup A script for setting up a Windows PC using BoxStarter and Chocolatey.\nGetting Started There are a few options for launching a BoxStarter script check out the offical documentation for all the various methods.\n\u0026gt; START http://boxstarter.org/package/nr/url?... Environments    Environment Description     basic A very simple box   standard A common environment that can be used on multiple dev machines   fullwin A .NET Developmenet environment with SQL Database tooling   dotnet A .NET Development environment   nodejs A rough outline of a nodejs environment    Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Ryzhkov Anton from the Noun Project.\n","id":165,"section":"posts","summary":"Automated Windows Setup A script for setting up a Windows PC using BoxStarter and Chocolatey.\nGetting Started There are a few options for launching a BoxStarter script check out the offical documentation for all the various methods.\n\u0026gt; START http://boxstarter.org/package/nr/url?... Environments    Environment Description     basic A very simple box   standard A common environment that can be used on multiple dev machines   fullwin A .","tags":["org:devkitspaces"],"title":"boxstarter-scripts","uri":"/2017/05/devkitspaces-boxstarter-scripts/","year":"2017"},{"content":"Homelab Summary A collection of templates and utility scripts used in my homelab. Most of these are just snippets or experiments.\nGetting Started As most of the scripts are self-contained, you can clone the repository:\ngit clone git://homelab/homelab And copy the relevant scripts into /usr/bin/ (or others) as necessary. You can also skip that, and just copy the contents of a file, then paste it into a fresh nano instance.\nDocker-compose Getting started is as simple as using docker-compose. You can do so as such:\ndocker-compose up -d If you wish to upgrade the container stack, you need to run the following commands:\ndocker-compose stop docker-compose rm -v docker-compose pull You can then start the docker environment.\ndocker-compose up -d Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Timothy Dilich from the Noun Project.\n","id":166,"section":"posts","summary":"Homelab Summary A collection of templates and utility scripts used in my homelab. Most of these are just snippets or experiments.\nGetting Started As most of the scripts are self-contained, you can clone the repository:\ngit clone git://homelab/homelab And copy the relevant scripts into /usr/bin/ (or others) as necessary. You can also skip that, and just copy the contents of a file, then paste it into a fresh nano instance.","tags":["org:jrbeverly"],"title":"homelab","uri":"/2017/05/jrbeverly-homelab/","year":"2017"},{"content":"Vagrant OpenGL Desktop Summary Provide a method of reproducible graphical development environments based on Linux. This repository provides a base OpenGL Desktop environment, sandboxed on your local computer. The repository is meant for experimenting with OpenGL related programming in a virtual machine.\nGetting Started You can use this locally with vagrant up, calling as such:\nvagrant --name=mydesktop up However It is recommended to use the script create.sh for the first run to ensure all necessary arguments are provided. The provided arguments creates a settings.yaml, storing the settings for the machine. You can create the machine by calling:\nsh create.sh -n mydesktop -d ubuntu If you want more information about the script create.sh, you can do so by calling:\nsh create.sh -h Parameters The parameters are used in the calling of vagrant up, primarily as vagrant [OPTIONS] up. After provisioning the environment, a settings file (setting.yaml) is created, which stores the provided parameters.\n   Name Type Description     name string Name of the provisioned desktop environment   desktop filename The name of the desktop provisioning script. These scripts are present in packaging/environments.    The vagrant environment is based on the bento/ubuntu images. If the timezone is not set, the provision script will attempt to auto-detect the timezone using tzupdate.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Maxi Koichi from the Noun Project.\n","id":167,"section":"posts","summary":"Vagrant OpenGL Desktop Summary Provide a method of reproducible graphical development environments based on Linux. This repository provides a base OpenGL Desktop environment, sandboxed on your local computer. The repository is meant for experimenting with OpenGL related programming in a virtual machine.\nGetting Started You can use this locally with vagrant up, calling as such:\nvagrant --name=mydesktop up However It is recommended to use the script create.sh for the first run to ensure all necessary arguments are provided.","tags":["org:blockycraft"],"title":"vagrant-desktop-opengl","uri":"/2017/05/blockycraft-vagrant-desktop-opengl/","year":"2017"},{"content":"Vagrant X11 Desktop Summary Provide a method of reproducible graphical development environments based on Linux. This repository provides a X11 Linux Desktop environment for the development of the XPlatformer project.\nGetting Started You can use this locally with vagrant up, calling as such:\nvagrant --name=mydesktop up However It is recommended to use the script create.sh for the first run to ensure all necessary arguments are provided. The provided arguments creates a settings.yaml, storing the settings for the machine. You can create the machine by calling:\nsh create.sh -n mydesktop -d ubuntu If you want more information about the script create.sh, you can do so by calling:\nsh create.sh -h Parameters The parameters are used in the calling of vagrant up, primarily as vagrant [OPTIONS] up. After provisioning the environment, a settings file (setting.yaml) is created, which stores the provided parameters.\n   Name Type Description     name string Name of the provisioned desktop environment   desktop filename The name of the desktop provisioning script. These scripts are present in packaging/environments.    The vagrant environment is based on the bento/ubuntu images. If the timezone is not set, the provision script will attempt to auto-detect the timezone using tzupdate.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Maxi Koichi from the Noun Project.\n","id":168,"section":"posts","summary":"Vagrant X11 Desktop Summary Provide a method of reproducible graphical development environments based on Linux. This repository provides a X11 Linux Desktop environment for the development of the XPlatformer project.\nGetting Started You can use this locally with vagrant up, calling as such:\nvagrant --name=mydesktop up However It is recommended to use the script create.sh for the first run to ensure all necessary arguments are provided. The provided arguments creates a settings.","tags":["org:xplatformer"],"title":"vagrant-desktop-x11","uri":"/2017/05/xplatformer-vagrant-desktop-x11/","year":"2017"},{"content":"Vagrant Linux Desktop Summary Provide a method of reproducible graphical development environments based on Linux. This repository provides a base Linux Desktop environment, sandboxed on your local computer.\nGetting Started You can use this locally with vagrant up, calling as such:\nvagrant --name=mydesktop --file=desktop.yaml up However It is recommended to use the script create.sh for the first run to ensure all necessary arguments are provided. The provided arguments requires a settings.yaml, storing the settings for the machine. You can see an example of one in tools/simple.yaml. You can create the machine by calling:\nsh create.sh -n mydesktop -d ubuntu If you want more information about the script create.sh, you can do so by calling:\nsh create.sh -h Parameters The parameters are used in the calling of vagrant up, primarily as vagrant [OPTIONS] up. After provisioning the environment, a settings file (setting.yaml) is created, which stores the provided parameters.\n   Name Type Description     name string Name of the provisioned desktop environment   desktop filename The name of the desktop provisioning script. These scripts are present in packaging/environments.    The vagrant environment is based on the bento/ubuntu images. If the timezone is not set, the provision script will attempt to auto-detect the timezone using tzupdate.\nSettings The following are arguments to the settings.yaml file:\n   Name Type Description     name string Name of the provisioned desktop environment   box vagrant-box The name of the underlying vagrant box   path dirname The path to the .vagrant directory   desktop string The name of the desktop provisioning script   logs dirname The directory to dump logs files   synced_folders (host: directory, guest: directory)[] A collection of syneced folders.    An example yaml is included below:\nname: lab box: ubuntu/trusty64 path: \u0026#34;.\u0026#34; desktop: ubuntu-minimal logs: \u0026#34;log_dir\u0026#34; synced_folders: - host: \u0026#34;../\u0026#34; guest: \u0026#34;/media/vagrant\u0026#34; Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Maxi Koichi from the Noun Project.\n","id":169,"section":"posts","summary":"Vagrant Linux Desktop Summary Provide a method of reproducible graphical development environments based on Linux. This repository provides a base Linux Desktop environment, sandboxed on your local computer.\nGetting Started You can use this locally with vagrant up, calling as such:\nvagrant --name=mydesktop --file=desktop.yaml up However It is recommended to use the script create.sh for the first run to ensure all necessary arguments are provided. The provided arguments requires a settings.","tags":["org:devkitspaces"],"title":"vagrant-desktop","uri":"/2017/05/devkitspaces-vagrant-desktop/","year":"2017"},{"content":"Blockycraft Manual Summary Blockycraft is an interactive graphics demo to create a Minecraft inspired demo. Blockycraft is focused on a series of graphics techniques create the graphical aethestics of a Minecraft world. This involves the usage of Perlin Noise for a dynamic world, lighting and shadows based on the positions of blocks in the world, and the aesthetic of the block world. The Blockycraft Manual explains the Blockycraft project in terms of interactive, compilation and technical components.\nBlockycraft Blockycraft is an interactive demo that uses standard first person controls to navigate through a block world. Blockycraft is a interactive graphics demo to create a Minecraft inspired demo which revolves around breaking and placing blocks. The game world is composed of rough cubes arranged in a fixed grid pattern and representing different materials, such as dirt, stone, and snow. The techniques used in the demo can be toggled using keyboard commands. The Blockycraft project involves an OpenGL C++ interaction demo that can be viewed at Blockycraft.\nDevelopment If you are developing on Windows, it is recommended to install the IDE Texmaker, the universal LaTeX editor. This will ensure that the necessary TexLive packages are installed (or can be installed). The IDE Texmaker is the default IDE used in development of the project.\nBuilding You can build the image using latexmk. To build with pdf2htmlEX, you can either install pdf2htmlEX in your environment, or make use of the docker image. To start the docker image, run the following:\nsh start.sh To build with latexmk, you can do the following:\nlatexmk -pdf manual.tex It is recommend to use the build scripts available in build/. These scripts are used in the build pipeline, ensuring that all arguments and attributes are set for compilation of the TeX project. The output pdf manual.pdf is available at the project root, while other build files are available in the output/ directory.\nsh build/compile.sh GitLab CI This project\u0026rsquo;s manual is built by GitLab CI, following the steps defined in .gitlab-ci.yml. The build scripts are available in build/, which are used to compile the TeX file.\nAcknowledgements The project icon is retrieved from kenney.nl. The original source material has been altered for the purposes of the project. The icon is used under the terms of the CC0 1.0 Universal.\nThe project icon uses assets by Kenney from kenney.nl/.\n","id":170,"section":"posts","summary":"Blockycraft Manual Summary Blockycraft is an interactive graphics demo to create a Minecraft inspired demo. Blockycraft is focused on a series of graphics techniques create the graphical aethestics of a Minecraft world. This involves the usage of Perlin Noise for a dynamic world, lighting and shadows based on the positions of blocks in the world, and the aesthetic of the block world. The Blockycraft Manual explains the Blockycraft project in terms of interactive, compilation and technical components.","tags":["org:blockycraft"],"title":"manual-classic","uri":"/2017/05/blockycraft-manual-classic/","year":"2017"},{"content":"XSamples Summary XSamples provides a collection of XWindows starter code that works with the XGameLib library. The XGameLib library powers the XPlatformer project. All the examples make use of the XLib API (XOrg) and focus on code that was developed in the XPlatformer project (or planned to be used). The point of the examples is to demonstrate how to use the XGameLib library.\nGetting Started To make (\u0026ldquo;compile and link\u0026rdquo;) an example, use the included makefile with the name of cpp file passed as a variable. For example, to make null.cpp:\nmake NAME=\u0026#34;null\u0026#34; Then, to run:\n./null Or you can even do it one step:\nmake run NAME=\u0026#34;null\u0026#34; For a full list of the examples, see docs/examples.md.\nAcknowledgements The project icon is retrieved from kenney.nl. The original source material has been altered for the purposes of the project. The icon is used under the terms of the CC0 1.0 Universal.\nThe project icon uses assets by Kenney from kenney.nl/.\n","id":171,"section":"posts","summary":"XSamples Summary XSamples provides a collection of XWindows starter code that works with the XGameLib library. The XGameLib library powers the XPlatformer project. All the examples make use of the XLib API (XOrg) and focus on code that was developed in the XPlatformer project (or planned to be used). The point of the examples is to demonstrate how to use the XGameLib library.\nGetting Started To make (\u0026ldquo;compile and link\u0026rdquo;) an example, use the included makefile with the name of cpp file passed as a variable.","tags":["org:xplatformer"],"title":"xsamples","uri":"/2017/05/xplatformer-xsamples/","year":"2017"},{"content":"XGameLib Summary XGameLib is a simple video game library used in the development of a classic side-scrolling arcade game, using the XLib API. The point of the game is to control a character through a terrain to meet an objective. The project makes use of the XLib API (XOrg) and focus on code that was developed to accomplish tasks for the assignment task.\nComponents    Component Filename Description     Spritesheet Spritesheet.h A uniform sheet of sprites that can be drawn individually.   Logger Logger.h Contains standard logging functionality and stored notifications.   KeyboardState KeyboardState.h Represents the state of keystrokes recorded by a keyboard input device.   MouseState MouseState.h Represents the state of a mouse input device, including mouse cursor position and buttons pressed.   Displayable Displayable.h Displayable is the base class for an object that can be updated/drawn to the screen.     Compile Instructions To make (\u0026ldquo;compile and link\u0026rdquo;) an example, use the included makefile with the name of cpp file passed as a variable.\nmake build Or you can even do it without specifying:\nmake Acknowledgements The project icon is retrieved from kenney.nl. The original source material has been altered for the purposes of the project. The icon is used under the terms of the CC0 1.0 Universal.\nThe project icon uses assets by Kenney from kenney.nl/.\n","id":172,"section":"posts","summary":"XGameLib Summary XGameLib is a simple video game library used in the development of a classic side-scrolling arcade game, using the XLib API. The point of the game is to control a character through a terrain to meet an objective. The project makes use of the XLib API (XOrg) and focus on code that was developed to accomplish tasks for the assignment task.\nComponents    Component Filename Description     Spritesheet Spritesheet.","tags":["org:xplatformer"],"title":"xgamelib","uri":"/2017/05/xplatformer-xgamelib/","year":"2017"},{"content":"XPlatformer Summary XPlatformer is a simple video game reminiscent of the classic side-scrolling arcade game, using the XLib API. The point of the game is to control a character through a terrain to meet an objective. The project makes use of the XLib API (XOrg) and focus on code that was developed to accomplish tasks for the assignment task.\nGetting Started To make (\u0026ldquo;compile and link\u0026rdquo;) an example, you can use bazel to build and run the project:\nbazel build //:xplatformer bazel run //:xplatformer Usage You can read more about how to interact with XPlatformer in the usage document.\nAcknowledgements The project icon is retrieved from kenney.nl. The original source material has been altered for the purposes of the project. The icon is used under the terms of the CC0 1.0 Universal.\nThe project icon uses assets by Kenney from kenney.nl/.\nResources All art assets were acquired from http://opengameart.org/ in particular from http://opengameart.org/users/kenney. Majority of art assets come from a particular package known as \u0026ldquo;Platformer Art Deluxe\u0026rdquo; available at http://opengameart.org/content/platformer-art-deluxe. If you would like to know more about these art assets, look into http://open.commonly.cc/ or the \u0026ldquo;Open Bundle\u0026rdquo; [See http://www.kenney.nl/]. The art assets are available with the Creative Commons License (CC0)\n","id":173,"section":"posts","summary":"XPlatformer Summary XPlatformer is a simple video game reminiscent of the classic side-scrolling arcade game, using the XLib API. The point of the game is to control a character through a terrain to meet an objective. The project makes use of the XLib API (XOrg) and focus on code that was developed to accomplish tasks for the assignment task.\nGetting Started To make (\u0026ldquo;compile and link\u0026rdquo;) an example, you can use bazel to build and run the project:","tags":["org:xplatformer"],"title":"xplatformer","uri":"/2017/05/xplatformer-xplatformer/","year":"2017"},{"content":"Friending Summary Friending is an online dating, friendship, and social networking mobile application that features user-created questionnaires. Friending has two primary features: joining groups to find people similar to you or signing up for events happening in your local area. Friending is a prototype built with the Proto.io application prototyping tool.\nThe Friending prototype was built as part of a requirements specification project. The project focused on the development of a user manual around a fictional matchmaking application called Friending. The application centered around user-created questionnaires that could be used to find relationship matches. The user manual was designed with the goal of deceiving the reader into believing that the application existed. A project vision document and set of user requirements guided the development of scenarios and use cases for the application. The fully-interactive high-fidelity prototype created for the requirements specification project is provided in this repository, available as a standalone HTML page.\nManual The Friending user manual provides info and tips to help you understand the mobile application. The requirements specification project involved the creation of a user manual for the fictional mobile application Friending. The Friending prototype is the actualization of a user vision and set of requirements to construct a matchmaking application. The vision and requirements were used to develop the expected behaviour of the prototype, although not all requirements were actualized into the interactive prototype. The prototype merely needed to present a faithful representation of the original vision.\nProto.io Proto.io is a web service to create fully-interactive high-fidelity prototypes that look and work exactly like your app should. No coding required. Using Proto.io you can quickly design and prototype design ideas. As the user manual was built around the fictional mobile application Friending, mockups were necessary to explain the application to the reader. Proto.io was selected for developing the mockups as they produced realistic mockups for a mobile application. Interaction was added to produce a fully-interactive prototype, which was useful in validating scenarios and use cases from the user manual. The visual design of the prototype was guided by the project vision document.\nGenerated The code provided in this repository is generated by Proto.io using the Export to HTML feature. Proto.io allows you to export your prototype so that it can be runnable as a standalone HTML page. When exported a zip package containing all necessary files to run and view the prototype is created. The code provided in this repository contains the project data files, and all the assets used in the project. As each exported prototype uses randomly generated file names for assets, this can mean that the versioning of assets is not always consistent, nor legible. The interaction code for a prototype is controlled by minimized javascript (scripts/ \u0026amp; libraries/), so it too is not legible.\nThe exported index.html file does not contain the rendered HTML pages of the prototype. The prototype pages are rendered by the Proto.io rendering engine (Javascript) which means that you should not modify any exported HTML code as this may break the presentation and functionality of the prototype. If you open the index.html file, then you will also see the respective prototypes device skin. The exported code has been altered slightly to switch the frame.html and index.html to make use of the device skin wrapper of the prototype.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Stefan Hartmann from the Noun Project.\n","id":174,"section":"posts","summary":"Friending Summary Friending is an online dating, friendship, and social networking mobile application that features user-created questionnaires. Friending has two primary features: joining groups to find people similar to you or signing up for events happening in your local area. Friending is a prototype built with the Proto.io application prototyping tool.\nThe Friending prototype was built as part of a requirements specification project. The project focused on the development of a user manual around a fictional matchmaking application called Friending.","tags":["org:thefriending"],"title":"friending","uri":"/2017/05/thefriending-friending/","year":"2017"},{"content":"Friending User Guide Summary The Friending user manual provided as a web resource generated from the user manual. The user guide provides info and tips to help you understand the mobile application as a web resource, instead of the standard PDF representation of the user manual. The method used to convert the user manual can be viewed in the build/ directory.\nFriending Friending is an online dating, friendship, and social networking mobile application that features user-created questionnaires. Friending has two primary features: joining groups to find people similar to you or signing up for events happening in your local area. Friending is a prototype built with the Proto.io application prototyping tool.\nThe Friending prototype was built as part of a requirements specification project. The project focused on the development of a user manual around a fictional matchmaking application called Friending. The application centered around user-created questionnaires that could be used to find relationship matches. The user manual was designed with the goal of deceiving the reader into believing that the application existed. A project vision document and set of user requirements guided the development of scenarios and use cases for the application. The user manual created for the requirements specification project is provided in this repository, available as a standalone HTML page.\nBuild You can build the standalone HTML page using pdf2htmlEX. To build with pdf2htmlEX, you can either install pdf2htmlEX in your environment, or make use of the docker image. To start the docker image, run the following:\nsh start.sh You can then use pdf2htmlEX as such:\npdf2htmlEX --zoom 1.5 --embed cfijo --dest-dir public/ index.pdf It is recommend to use the build scripts available in build/. These scripts are used in the build pipeline, ensuring that all arguments and attributes are set for compilation of the HTML page. The scripts handle the process of downloading the user manual from source, allowing it to be converted from PDF to HTML without losing text or format.\nsh compile.sh Build Parameters    Variable Default Description      ARTIFACT_URL see .gitlab-ci.yml The URL hosting the friending user manual.     Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Stefan Hartmann from the Noun Project.\n","id":175,"section":"posts","summary":"Friending User Guide Summary The Friending user manual provided as a web resource generated from the user manual. The user guide provides info and tips to help you understand the mobile application as a web resource, instead of the standard PDF representation of the user manual. The method used to convert the user manual can be viewed in the build/ directory.\nFriending Friending is an online dating, friendship, and social networking mobile application that features user-created questionnaires.","tags":["org:thefriending"],"title":"user-guide","uri":"/2017/05/thefriending-user-guide/","year":"2017"},{"content":"Friending Manual Summary The Friending user manual provides info and tips to help you understand the mobile application. The project is the actualization of a user vision and set of requirements to construct a matchmaking application. The User Manual encodes these requirements as scenarios and use cases, while the Friending prototype presents a high-fidelity vision of the matchmaking application.\nFriending Friending is an online dating, friendship, and social networking mobile application that features user-created questionnaires. Friending has two primary features: joining groups to find people similar to you or signing up for events happening in your local area. Friending is a prototype built with the Proto.io application prototyping tool.\nThe Friending prototype was built as part of a requirements specification project. The project focused on the development of a user manual around a fictional matchmaking application called Friending. The application centered around user-created questionnaires that could be used to find relationship matches. The user manual was designed with the goal of deceiving the reader into believing that the application existed. A project vision document and set of user requirements guided the development of scenarios and use cases for the application. The user manual created for the requirements specification project is provided in this repository, available as a PDF.\nDevelopment If you are developing on Windows, it is recommended to install the IDE Texmaker, the universal LaTeX editor. This will ensure that the necessary TexLive packages are installed (or can be installed). The IDE Texmaker is the default IDE used in development of the project.\nBuilding You can build the image using latexmk. To build with pdf2htmlEX, you can either install pdf2htmlEX in your environment, or make use of the docker image. To start the docker image, run the following:\nsh start.sh To build with latexmk, you can do the following:\nlatexmk -pdf manual.tex It is recommend to use the build scripts available in build/. These scripts are used in the build pipeline, ensuring that all arguments and attributes are set for compilation of the TeX project. The output pdf manual.pdf is available at the project root, while other build files are available in the output/ directory.\nsh build/compile.sh Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Stefan Hartmann from the Noun Project.\n","id":176,"section":"posts","summary":"Friending Manual Summary The Friending user manual provides info and tips to help you understand the mobile application. The project is the actualization of a user vision and set of requirements to construct a matchmaking application. The User Manual encodes these requirements as scenarios and use cases, while the Friending prototype presents a high-fidelity vision of the matchmaking application.\nFriending Friending is an online dating, friendship, and social networking mobile application that features user-created questionnaires.","tags":["org:thefriending"],"title":"manual","uri":"/2017/05/thefriending-manual/","year":"2017"},{"content":"Jotto Summary Jotto is a logic-oriented word game played with two players. Each player picks a secret word of five letters (that is in the dictionary), and the object of the game is to correctly guess the other player\u0026rsquo;s word first. Players take turns guessing and giving the number of Jots, or the number of letters that are in both the guessed word and the secret word.\nThe Jotto application is built with a single player, playing against a computer. The objective of the game is to correctly guess the secret word before the maximum number of guesses. The user interface provides feedback about the success of each guess, and the progress being made by the player. Each guess must be validated that it is present in the dictionary, of the proper length and contains repeated characters. After each guess the player will be provided feedback about the guess, such as the number of exact character matches and the number of partial character matches.\nDevelopment If you are developing on Windows, it is recommended to install the IDE IntelliJ IDEA. This will work with the existing jotto.iml present in the jotto/ directory. The output of the build process is available in the CI artifacts browser. The artifacts have an expiration period to ensure that old build artifacts are properly cleaned up.\nBuilding You can build the image using mvn or scripts in the build/ directory. To build with maven, you can either install maven in your environment, or make use of the docker image. To start the docker image, run the following:\nsh build/start.sh To build with mvn, you can do the following:\nmvn compile It is recommend to use the build scripts available in build/. These scripts are used in the build pipeline, ensuring that all arguments and attributes are set for compilation of the project. The output application jotto.jar is available at the project root, while other build files are available in the target/ directory.\nsh build/compile.sh Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by dnlhtz from the Noun Project.\n","id":177,"section":"posts","summary":"Jotto Summary Jotto is a logic-oriented word game played with two players. Each player picks a secret word of five letters (that is in the dictionary), and the object of the game is to correctly guess the other player\u0026rsquo;s word first. Players take turns guessing and giving the number of Jots, or the number of letters that are in both the guessed word and the secret word.\nThe Jotto application is built with a single player, playing against a computer.","tags":["org:jrbeverly"],"title":"jotto","uri":"/2017/05/jrbeverly-jotto/","year":"2017"}],"tags":[{"title":"blockycraft","uri":"/tags/blockycraft/"},{"title":"cloud-services","uri":"/tags/cloud-services/"},{"title":"continuous-integration","uri":"/tags/continuous-integration/"},{"title":"docker","uri":"/tags/docker/"},{"title":"index","uri":"/tags/index/"},{"title":"infrastructure-as-code","uri":"/tags/infrastructure-as-code/"},{"title":"mobile-social-network","uri":"/tags/mobile-social-network/"},{"title":"org:blockycraft","uri":"/tags/orgblockycraft/"},{"title":"org:cardboardci","uri":"/tags/orgcardboardci/"},{"title":"org:devkitspaces","uri":"/tags/orgdevkitspaces/"},{"title":"org:infraprints","uri":"/tags/orginfraprints/"},{"title":"org:jrbeverly","uri":"/tags/orgjrbeverly/"},{"title":"org:thefriending","uri":"/tags/orgthefriending/"},{"title":"org:xplatformer","uri":"/tags/orgxplatformer/"},{"title":"prototype","uri":"/tags/prototype/"},{"title":"terraform","uri":"/tags/terraform/"},{"title":"wisevault","uri":"/tags/wisevault/"}]}