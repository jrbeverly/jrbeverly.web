{"categories":[],"posts":[{"content":"   json with schema     Golang \u0026amp; OpenAPI Spec Fiddling with an idea of generating models from the OpenAPI spec for YAML.\nNotes  Bazel model works, but would want to formalize as a proper build system Model of pkg/apis/... is a good direction, version selecting will need to be a thing though What about YAML Support? Is swagger the expected way forward for this kind of thing? Should the generation of files be done as a bazel run or a bazel build? Generation should really commit against the repository, that way we can be more confident about the code results Works much better to have all components available  ","id":0,"section":"posts","summary":"Fiddling with an idea of generating models from the OpenAPI spec for YAML.","tags":["jrbeverly"],"title":"swagger-golang-bazelgen-exp","uri":"/2022/05/jrbeverly-swagger-golang-bazelgen-exp/","year":"2022"},{"content":"   Quick runthrough of gqlgen     Fiddling with gqlgen Experimenting a bit with gqlgen for generating GraphQL code from spec.\nNotes  Would prefer to move gqlgen.yml and schema.graphql into a spec/ directory (or other areas) Installation method with tools.go - not sure about this, static binary is preferrable for my usages Generated models are pretty solid, similar to swagger Resolvers is nice, but what about partial updates?  I\u0026rsquo;m not entirely sold on this pattern. Feel like I\u0026rsquo;d prefer more flexibility with how the models and various components are defined. Almost interested in the idea of instead of using annotations like json or yaml, leveraging generated code for rendering the JSON/YAML components. Then isolating the various elements of what the system is expected to do, things like:\n Data Serialization Patch \u0026amp; Diff Pattern for Models (e.g. receive patch, apply patch to model) Hexagonal architecture  Maybe this is a case where Roslyn is a better fit for specialized code generation with a hexagonal architecture in-mind? More research needed into this.\n","id":1,"section":"posts","summary":"Experimenting a bit with gqlgen for generating GraphQL code from spec.","tags":["jrbeverly"],"title":"graphql-golang-note-check","uri":"/2022/05/jrbeverly-graphql-golang-note-check/","year":"2022"},{"content":"   Using robot state machine for modal in React     State Machine for Confirmation Dialog Running through the workshop example of Build A Confirmation Modal in React with State Machines\nNotes  In principal, like the idea of representation this kinds of logic \u0026ldquo;Flows\u0026rdquo; Usage of strings for state is less than ideal, almost would want it to be objects Pattern of constructing a \u0026ldquo;flow\u0026rdquo; then making use of the \u0026ldquo;flow\u0026rdquo; Potential opportunities with systems like codegen Possible ideas for test evaluation with the states Not sure about this library, needs opportunities for isolation Feels like it doesn\u0026rsquo;t fit with the View -\u0026gt; ViewModel -\u0026gt; Model concept I was thinking of for State Machines The \u0026lsquo;dispatcher\u0026rsquo; works similar to what a wrapper over \u0026ldquo;Modal\u0026rdquo; would have, what benefits does it bring? Consider looking into eventing systems instead, they seem to better encapsulate this idea without strictness of the \u0026ldquo;machine structure\u0026rdquo; Maybe this could work better when combined with message passing + code generation?  ","id":2,"section":"posts","summary":"Running through the workshop example of \u003ca href=\"https://daveceddia.com/react-confirmation-modal-state-machine/\"\u003eBuild A Confirmation Modal in React with State Machines\u003c/a\u003e","tags":["jrbeverly"],"title":"react-xstate-machines","uri":"/2022/05/jrbeverly-react-xstate-machines/","year":"2022"},{"content":"   Experimenting with Pulumi for Lambda, S3 Website \u0026amp; Fargate     Experimenting with Pulumi Experimenting with the pulumi examples from https://github.com/pulumi/examples, and the options to have\nNotes  Enabling Bazel wtih this model encountered some pain points Pulumi is capable of performing operations like Docker Build / Referencing lambda binaries The change source pattern (git.dirty, git.author, etc) for volatile status is a nice pattern Building of artifacts should not be the responsibility of the deployment model Pulumi takes over the context element of deployments, and use that for deployments  Pulumi has some nice elements too it with respect to be able to use code to create the deployment, as well as a good web/console interface. However I\u0026rsquo;m not sold on the idea of these kind of \u0026ldquo;dynamic\u0026rdquo; systems, and instead prefer the style of an application model. This way a series of concerns can be baked into \u0026ldquo;Configuration\u0026rdquo; that is then acted upon, similar to Terraform.\nIdea on Notes Basics of this components are:\n Volatile Status - Properties about the change that aren\u0026rsquo;t really related to the software artifacts - Git SVC, Git Commit, Manifest Version, GitHub PR URL, etc Artifact Manifest - Software artifacts created as part of the build process Application Manifest(s) - What is the calver/semver of the application system Deployment Artifacts - Software artifacts created as part of the build process for deploying the service Application Model - Model of the application, and the various ways it can be assembled or connected to components (Graph relations with optional components and multiple vertex sets)  Ideally something like this:\nvolatile_status.json:\n{ \u0026#34;exec\u0026#34;: \u0026#34;cli\u0026#34;, \u0026#34;git\u0026#34;: { \u0026#34;author\u0026#34;: \u0026#34;Jonathan Beverly\u0026#34;, \u0026#34;author.email\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;committer\u0026#34;: \u0026#34;Jonathan Beverly\u0026#34;, \u0026#34;committer.email\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;dirty\u0026#34;: true, \u0026#34;head\u0026#34;: \u0026#34;ec3d5a5a63251dd884634e25b1f98c9be6b4b912\u0026#34;, \u0026#34;headName\u0026#34;: \u0026#34;refs/heads/main\u0026#34;, }, \u0026#34;vcs\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;github.com\u0026#34;, \u0026#34;owner\u0026#34;: \u0026#34;jrbeverly\u0026#34;, \u0026#34;repo\u0026#34;: \u0026#34;exp-pulumi-lambda\u0026#34; } } artifact.manifest.yaml:\napiVersion: openapplicationmodel/v1alphav1 kind: Manifest metadata: name: pulumi-lambda labels: app: pulumi service: xyz property: xyss spec: artifacts: - id: 946debe2-e213-11ec-8fea-0242ac120002 - id: e213ebe2-946d-8fea-11ec-200020242ac1 lambda.appmanifest.yaml:\napiVersion: openapplicationmodel/v1alphav1 kind: AppManifest metadata: name: pulumi-lambda labels: app: pulumi service: xyz property: xyss spec: id: 946debe2-e213-11ec-8fea-0242ac120002 filename: lambda.tar sha256: ba7816bf8f01cfea414140de5dae2223b00361a396177a9cb410ff61f20015ad annotations: app: lambda Then these artifact components will need some sort of spec for defining how to combine these various components. Additionally this spec should be oriented as something that tools can generate, which would allow for dynamic deployment processes by simply generating the spec, then publishing it to a service for performing the deployment. Similar to how pulumi is doing the deployment, but instead requiring it to resolve all properties \u0026amp; actions it would perform (e.g. terraform plan), then require it to execute the plan.\n","id":3,"section":"posts","summary":"Experimenting with the pulumi examples from \u003ca href=\"https://github.com/pulumi/examples,\"\u003ehttps://github.com/pulumi/examples,\u003c/a\u003e and the options to have","tags":["jrbeverly"],"title":"exp-pulumi-lambda","uri":"/2022/05/jrbeverly-exp-pulumi-lambda/","year":"2022"},{"content":"   Fiddling with Golang \u0026amp; Bazel for a microservice gin app     Golang Gin \u0026amp; Gitpod Fiddling with the Dev experience of Golang gin within Gitpod\nBased on gitpod-io/go-gin-app\nNotes  Prefer the pattern of combining this with cobra Server rendered HTML is pretty straightforward Returning simple JSON pretty straightforward Can be combined with GRPC + Proto What about swagger for generating the modules? Considerations about how to organize the elements (cli, server, routes, models, database, etc) What about logging for this? Gitpod has occassionally issues with the import of gin References for things like templates/ at the top level is nice Log while running is excellent  Overall positive.\n","id":4,"section":"posts","summary":"Fiddling with the Dev experience of Golang gin within Gitpod Based on \u003ca href=\"https://github.com/gitpod-io/go-gin-app\"\u003egitpod-io/go-gin-app\u003c/a\u003e","tags":["jrbeverly"],"title":"golang-gin-gitpod","uri":"/2022/05/jrbeverly-golang-gin-gitpod/","year":"2022"},{"content":"   Playing around with MirageJS     MirageJS Tutorial Running through the (MirageJS Tutorial)[https://github.com/miragejs/tutorial] case\nNotes  If generated from a schema, the system is essentially  Backend (proper) Backend Mock (in-memory) Backend JS Client (proper) Backend JS Client Mock (in-memory - integrated with MirageJS)   How could this be integrated with code generation from a specification? Does this assist with local development? Test scenarios in an pseudo-E2E case?  ","id":5,"section":"posts","summary":"Running through the (MirageJS Tutorial)[https://github.com/miragejs/tutorial] case","tags":["jrbeverly"],"title":"reminders-miragejs","uri":"/2022/05/jrbeverly-reminders-miragejs/","year":"2022"},{"content":"   Fiddling with video generation by code     MAnim Experimentation with Video Generation Experimenting with the MAnim library for generating video animations.\nNotes  Solid library that works really well at what it does Scenes feel very \u0026ldquo;on-track\u0026rdquo; in that it is a sequence of steps one followed by the next Python allows for re-use to create common elements like an isometric file Works great for scenes where the emphasis is on the core \u0026ldquo;shapes\u0026rdquo; and animation  Don\u0026rsquo;t think this fits with the use-case I\u0026rsquo;m after. Would make more sense to explore as a unity \u0026ldquo;game\u0026rdquo; that is really just an interactive cutscene. As that would allow for elements of the scene to still remain dynamic (animated) while the core \u0026ldquo;state\u0026rdquo; of the animation remains at a fixed point in time.\n","id":6,"section":"posts","summary":"Experimenting with the MAnim library for generating video animations.","tags":["jrbeverly"],"title":"manim-exp-video-generation","uri":"/2022/05/jrbeverly-manim-exp-video-generation/","year":"2022"},{"content":"   Minimal react webassembly case with Webpack, React     React Webpack with Rust WebAssembly Fiddling around with an opinionated example for Webpack builds with WebAssembly.\nSource code for fractal is based on https://dev.to/brightdevs/using-webassembly-with-react-1led, and the repository templated by https://github.com/Fallenstedt/wasm-react-webpack-template.\nNotes  WASM Build works pretty well with rust Rust is a solid option for getting webassembly integrated Golang was considered, but previous experiments weren\u0026rsquo;t as desired Makefile as an entrypoint is preferrable than using yarn ... Embedding the generated packages within the front (e.g. under pkg/) allows for local refs When does the build perform certain targets? E.g. resolution of paths vs copying of static files? Searching for options of individual target execution didn\u0026rsquo;t yield much (using wrong search terms?) Webpacks plugin model isn\u0026rsquo;t really the desired execution model, prefer graph-driven like buck/bazel Webpack plugin dumping the files just by directory path isn\u0026rsquo;t ideal, would prefer to ref (e.g. //pkg/fractal) Loading WebAssembly requires a bit of pre-amble, could probably be made into a \u0026lt;project\u0026gt;-commons for UIs responsible for loading it Usages of them are pretty nice, opportunities for generated API/Schemas/State Machines/State Models  I\u0026rsquo;m not sure on this one. I get the benefits that come with the webpack ecosystem, but had multiple issues with the way its goes about it in comparison to patterns like Bazel/Buck. I like this pattern more for combining various languages to create the final result, such as having a rust library for certain functions or components. Splitting it away from the frontend makes it so they can be designed in more of a \u0026ldquo;unit-test\u0026rdquo; way. Possible options might be to encode a state machine within one of these libraries, then have the web interface act on this, like a View =\u0026gt; ViewModel =\u0026gt; {Model} where the ViewModel performs queries into the state machine {Model}.\n","id":7,"section":"posts","summary":"Fiddling around with an opinionated example for Webpack builds with WebAssembly. Source code for fractal is based on \u003ca href=\"https://dev.to/brightdevs/using-webassembly-with-react-1led,\"\u003ehttps://dev.to/brightdevs/using-webassembly-with-react-1led,\u003c/a\u003e and the repository templated by \u003ca href=\"https://github.com/Fallenstedt/wasm-react-webpack-template\"\u003ehttps://github.com/Fallenstedt/wasm-react-webpack-template\u003c/a\u003e.","tags":["jrbeverly"],"title":"react-wasm-babel","uri":"/2022/05/jrbeverly-react-wasm-babel/","year":"2022"},{"content":"   Vulnerability disclosure, based on the dioterms     Vulnerability Disclosure Policy from Dioterms Exploring leveraging dioterms and policymaker for creating vulnerability disclore policies for a website.\nNotes  DNS is related for the deployment of the website (_security) Entry within the /.well-known/ root of the domain (example.com/.well-known/security.txt) Security entry for the domain (example.com/security) If the application is located within example.com/app/... (e.g. index.html), then the top level domain elements can be \u0026ldquo;procedural\u0026rdquo; Construct the webpage into a bundle (website.wbn), publish it to the \u0026ldquo;deployer\u0026rdquo;, which can then handle the top level elements References can still exist within the app (/security, /.well-known/...), known to the website manifest Website manifest allow it to enforce expectations about required top-level components Distributable/Sharable webpages can combine/merge these components (e.g. website.wbn, website.manifest, website.policy) with organization (or overwrite)  Acknowledgements:\n https://jacobian.org/2021/jul/8/appsec-pagnis/ https://github.com/disclose/dioterms https://policymaker.disclose.io/policymaker  ","id":8,"section":"posts","summary":"Exploring leveraging \u003ca href=\"https://github.com/disclose/dioterms\"\u003edioterms\u003c/a\u003e and \u003ca href=\"https://policymaker.disclose.io/policymaker\"\u003epolicymaker\u003c/a\u003e for creating vulnerability disclore policies for a website.","tags":["jrbeverly"],"title":"vuln-disclosure-policy","uri":"/2022/05/jrbeverly-vuln-disclosure-policy/","year":"2022"},{"content":"   Experiments with conftest, terraform workflow     Experimenting with ConfTest, Terraform \u0026amp; Bazel Experimenting with using Bazel to handle the build \u0026amp; execution of Terraform files, while providing means of writing tests against the terraform with conftest. Fiddling with the idea of having local tests against the configuration, as well as tests against the terraform plan.\nThe intention is that Bazel would be responsible for constructing Terraform deployable tarballs, which contains all resolved modules \u0026amp; providers. These would be executed to perform apply, plan and other commands.\nNotes  Terraform init should be treated as a repository rule, based off the .terraform.lock.hcl Modules can exist in any directory, and be substituted into other packages as its using the pkg_tar under the hood Using a provider to keep track of \u0026ldquo;runpaths\u0026rdquo; for any of the commands, making it so these properties can be shared across rules Substitutions in command arguments through {variable} and kwargs works great Standard pattern for \u0026ldquo;chaining\u0026rdquo; commands is needed, so that running one will run the others Should it always be the case that Terraform does a plan then apply? How should chained commands perform some degree of caching so they aren\u0026rsquo;t re-running every single time? Terraform modules need to exist in multiple forms: local modules \u0026amp; externally sourced modules (e.g. how to integrate with http_archive - or similar) Using the pattern of custom rules under bazel/rules/rules_xyz is really nice, the deps.bzl can then just load them Rego/OPA are missing vscode extensions for use in gitpod (does vscode have them in devcontainers?) Adding the opa toolchain along with the testing process went really well Long term rules_toolchains should be external, and rely on a pre-installed tool like toolchains or termtools, that can download these How do these rules integrate with different kinds of rules? Is the API solid enough that something like rules_terraform_ext could be made to provide common helpers? Should the terraform_workspace really exist? Or should it just be terraform_command with something like rules_terraform_ext filling in the ease-of-use? Makefile with bazel can be great for the make help addition  ","id":9,"section":"posts","summary":"Experimenting with using Bazel to handle the build \u0026amp; execution of Terraform files, while providing means of writing tests against the terraform with conftest. Fiddling with the idea of having local tests against the configuration, as well as tests against the terraform plan. The intention is that Bazel would be responsible for constructing Terraform deployable tarballs, which contains all resolved modules \u0026amp; providers. These would be executed to perform \u003ccode\u003eapply\u003c/code\u003e, \u003ccode\u003eplan\u003c/code\u003e and other commands.","tags":["jrbeverly"],"title":"bazel-terraform-conftest-experiments","uri":"/2022/05/jrbeverly-bazel-terraform-conftest-experiments/","year":"2022"},{"content":"   Julia \u0026amp; Jupyter     Julia Jupyter Notebook Experiments with working with Julia \u0026amp; Jupyter Notebooks.\nNotes  Installation of Jupyter, Conda + Julia (+ packages) onto a gitpod image Makefile for common actions working with the notebooks Validating some basic cases working with Julia for generating plots \u0026amp; other bits for the visualization  ","id":10,"section":"posts","summary":"Experiments with working with Julia \u0026amp; Jupyter Notebooks.","tags":["jrbeverly"],"title":"julia-with-jupyter-notebook","uri":"/2022/04/jrbeverly-julia-with-jupyter-notebook/","year":"2022"},{"content":"   Validation of running notebooks in gitpod     GitPod Jupyter Notebook Validating working with Jupyter notebooks in a GitPod environments\nNotes  Support for preview or open in browser mode Initial provisioning of the image takes a bit, pre-baked likely would help in this area Jupyter lab is another option Makefile is a good option for acting as an entrypoint for common commands Outstanding questions still exist for multi-notebook repositories like the learning repo  ","id":11,"section":"posts","summary":"Validating working with Jupyter notebooks in a GitPod environments","tags":["jrbeverly"],"title":"gitpod-jupyter-notebook","uri":"/2022/03/jrbeverly-gitpod-jupyter-notebook/","year":"2022"},{"content":"   Docker-compose case for Pushgateway experiments     Pushgateway Compose Setup Simple code setup for spinning up Pushgateway, Prometheus \u0026amp; Grafana for validating lifecycle pushgateway metrrics.\nNotes  Metrics published to pushgateway are collected by Prometheus Prometheus is enabled in Grafana for queries Grafana datasources \u0026amp; dashboards are configured from the provisioning directory The script publish.sh can be used to publish the metrics into the system Dashboards in grafana/dashboards can be configured with other tools for construction Dashboards could be standardized, then shared into other sources Tools can be configured for publishing in these scenarios  ","id":12,"section":"posts","summary":"Simple code setup for spinning up Pushgateway, Prometheus \u0026amp; Grafana for validating lifecycle pushgateway metrrics.","tags":["jrbeverly"],"title":"pushgateway-compose-setup","uri":"/2022/03/jrbeverly-pushgateway-compose-setup/","year":"2022"},{"content":"   Experimenting with AWS SCPs \u0026amp; Organization Units     AWS Organization Structure Experiments - Mirrored Organizations Experiments with AWS Organization structure and potential SCP policies.\nNotes  The entire organization unit hierarchy shouldn\u0026rsquo;t be a single entity for mirroring. Makes it difficult to evaluate in \u0026ldquo;isolation\u0026rdquo; Entire organization mirrors can work with the SCPs, but internal permissions (e.g. S3 Bucket) still might have issues Organizations should include a uniqueness component to allow for constructing a new version (\u0026amp; prototyping) SCPs seem like they would benefit in cases where there is a sort of \u0026ldquo;State machine\u0026rdquo; in the SCP State machine examples are \u0026ldquo;During provisioning of account, need to create IAM Users, but from then on no users should be created\u0026rdquo; Account boundaries for services as a way of strictly locking things makes it easier to have DenyKMS and other such policies Region denies only apply after provisioning, as we need to purge the \u0026ldquo;default VPCs\u0026rdquo; created when an AWS Account is created (+ any other \u0026ldquo;default\u0026rdquo; resources) AWS Password Policy / AWS IAM Account Name / Etc are all good examples of something that should only need to be provisioned \u0026ldquo;once\u0026rdquo; SCPs give a potential idea for the concept of \u0026ldquo;Immutable AWS Account Infrastructure\u0026rdquo;, that require you to create a new AWS Account (+ migrate resources) rather than edit them Sandbox/Staging organizations can contain the developer workloads that are for sandbox/development Developer workloads should be contained within accoounts that can be created/decommissioned on a release schedule (see ubuntu - Bionic Beaver, Focal Fossa, Xenial Xerus)  More investigation is needed into this idea, as the exact \u0026ldquo;concern\u0026rdquo; that this kind of structure \u0026amp; SCP policy layout will handle is kind of vague. Although grouping AWS Accounts and associating tags with them can be useful for things like data residency / storage compliance, its not immediately clear how this design maps to the \u0026ldquo;problem\u0026rdquo; itself.\nSCPs seem like they would be a good guardrail, but have concerns that it would encounter issues in cases with the rule being enforced at all times, vs a more state machine concept (e.g. [Provisioning (allowed) =\u0026gt; Running (not allowed)])\n","id":13,"section":"posts","summary":"Experiments with AWS Organization structure and potential SCP policies.","tags":["jrbeverly"],"title":"aws-exp-organizations-policy","uri":"/2022/03/jrbeverly-aws-exp-organizations-policy/","year":"2022"},{"content":"   GitHub Actions cronjob responsible for mirroring github releases     Release Mirror for GitHub Releases Lightweight experiment for mirroring GitHub releases into a file store system like Minio or AWS S3.\nGetting Started packages/ Contains the tool definition files for each of the mirror repositories (tool.ini), and the computed checksums for the downloaded files. In an actual use case it would be better to store the checksums externally from this repository, allowing this one to act purely as an executor, and the other repository acting as a \u0026ldquo;record\u0026rdquo; of the known-good commit SHAs (as well as .sign/.ack files to chain \u0026ldquo;trust\u0026rdquo;)\ncmd/ Contains the scripts that are responsible for performing the lookup \u0026amp; download of toolchains from each of the potential sources. Only source for now is GitHub.com.\nNotes  Could other tools like asdf be leveraged instead to download the core binaries, then checksum/bundle that way? What about concerns running any old git repos? Do we wish to mirror the GitHub releases, or the binaries themselves? Or is it multiple things we wish to run? Are there alternative options for having this kind of mirror? Could this benefit from some \u0026ldquo;import into content addressable storage\u0026rdquo; model Commiting against the same repo isn\u0026rsquo;t great, as it can muddle the commit history of changes to the executor Likely want the ability to work off not just the \u0026ldquo;latest\u0026rdquo;, but any release created for these Revocation of builds with known CVEs should be its own separate thing (not integrated into this) gh (GitHub CLI) is pretty effective for downloading these kinds of binaries  ","id":14,"section":"posts","summary":"Lightweight experiment for mirroring GitHub releases into a file store system like Minio or AWS S3.","tags":["jrbeverly"],"title":"github-pullthrough-mirror","uri":"/2022/03/jrbeverly-github-pullthrough-mirror/","year":"2022"},{"content":"   Fiddling with the github folder conventions     GitHub Configuration in Code Fiddling with the configuration options available for GitHub, while encoding the properties in the .github directory.\nGetting Started This repository\nThe concept is to create an almost \u0026ldquo;self-contained\u0026rdquo; repository, that includes within hidden directories like .github/.aws/.azure/.gcp/etc that represent almost \u0026ldquo;interfaces\u0026rdquo; between the repository and external services that would act on it. This way rather than having the repository rely on assumptions about how it is configured, it is instead providing all the baseline elements for any supporting infrastructure (e.g. operators) to provide these integrations themselves.\nExamples of these include:\n GitHub Labels GitHub Secrets from External Stores (e.g. source from aws) Dependabot CodeQL Repository Settings License Data  To get the full list of all \u0026ldquo;initial\u0026rdquo; baseline constraints for this, I explored the use of https://github.com/ossf/scorecard to identify potential concerns that would exist in the repository.\nNotes A collection of \u0026ldquo;best practices\u0026rdquo; were collected at: https://bestpractices.coreinfrastructure.org/en/criteria. These can be summarized roughly as the following, with the full list contained under docs/CRITERIA.md:\n Use fuzzing (fuzz testing) for programs (see https://github.com/google/oss-fuzz) Published process for reporting vulnerabilities in the repository Provide a working build system that can automatically rebuild the software from source code General policy that tests will be added to an automated test suite (new functionality) Apply at least one static code analysis tool (beyond compiler warnings and safe language modes) Provide should contain licensing configuration Provide documentation in the form of API Reference, Examples \u0026amp; \u0026ldquo;basics\u0026rdquo; Project should support some means of organizing for discussion To enable collaborative review, the project\u0026rsquo;s source repository MUST include interim versions for review between releases; it MUST NOT include only final releases. Impose a versioning system on the release of artifacts (do not pin latest) Provide release notes with changes and so on  Some of these practices are setting a baseline, which I think can be useful, but don\u0026rsquo;t exactly map perfectly to how I\u0026rsquo;d think about this. However the overall idea of a \u0026ldquo;Standard\u0026rdquo; with clear set of criteria (programmatic if possible) sounds like a positive option for these kinds of repositories.\nOverall think this is a worthwhile idea to continue with, having repositories be \u0026ldquo;self-contained\u0026rdquo;. I think some of the components could be split out of .github into other standards, then leveraging automation to populate the approach components in the .github/ directory. Examples could include things like:\n Having the \u0026ldquo;artifacts\u0026rdquo; published by a repository defined in repository Having the \u0026ldquo;secrets\u0026rdquo; needed for the build process encoded in repository Using Content/Unique Addressable Storage for dependencies/tools to avoid specifying where artifacts must be sourced (so they can pull from authoritative or mirror) CODEOWNERS leveraging an RBAC model for the repository, rather than inheriting the implementation of the service (e.g. how GitHub teams are \u0026ldquo;done\u0026rdquo;) IDE Related components working to externalize the dependency installation into repository pre-baked images  ","id":15,"section":"posts","summary":"Fiddling with the configuration options available for GitHub, while encoding the properties in the \u003ccode\u003e.github\u003c/code\u003e directory.","tags":["jrbeverly"],"title":"github-config-in-code","uri":"/2022/03/jrbeverly-github-config-in-code/","year":"2022"},{"content":"   Fiddling with GitHub Applications for automated commits     GitHub App for Generated Commits Running through a bunch of things to be done with this\nNotes  Is it possible to push empty commits, which would need to be handled Commits should be crafted first, then attempts being made to apply the change Would need to have better visibility into the \u0026ldquo;crafted\u0026rdquo; files that will be committed Does this really require a GitHub Application? Feels like this could be decoupled Need to ensure that requests with the API go through the proper retry processes Need to ensure that the configuration is split from auth, so its easier to rotate the auth without fiddling with the config Should the app really be responsible for the work-component of crafting the change? Should all changes be made as a pull request + request to merge from the GitHub App? This would avoid conflicts as much, and let GitHub be responsible for changes?  Does this kind of thing really need a GitHub Application? Feels like this is something that can be split away from GitHub as a kind of entity that is responsible for the following actions:\n Pull the original source from the git repositories Apply the processes on the source files (regular text processing) Apply the change, and publish using standard git procedures  This wouldn\u0026rsquo;t require any additional overhead complexity with communicating with the GitHub repository. Or could this be something that is abstracted into a common application that is responsible for something like\n Receive a set of \u0026ldquo;Changes\u0026rdquo; as proposed within git Perform the change on behave of this system  This would allow commits to be \u0026ldquo;queued\u0026rdquo; and reduce a lot of the headache with dealing with this processes within the tool, and instead allow other tools to just communicate with the \u0026ldquo;Git Operations\u0026rdquo; API, which is much more fault tolerant.\n","id":16,"section":"posts","summary":"Running through a bunch of things to be done with this","tags":["jrbeverly"],"title":"github-app-for-code-change","uri":"/2022/02/jrbeverly-github-app-for-code-change/","year":"2022"},{"content":"   Minor experiment with a shell script for signing artifacts that would be generated from a build process.     GPG Artifact Sign Experiment Minor experiment with a shell script for signing artifacts that would be generated from a build process.\nNotes  Build tooling can support multiple checksum algorithms (sha256/sha1/md5/sha512/etc) Docker Container Trust (DCT) didn\u0026rsquo;t fit with the usecase/portability desired GPG is the standard way for doing this (can this be packaged into something more portable?) Design should aim to be agnostic of GitHub Releases or any other platform Build tooling likely doesn\u0026rsquo;t need to understand the concept of \u0026ldquo;signing\u0026rdquo; (or should it?) If build tools understand signing, does that mean there need to be \u0026lsquo;Developer Keys\u0026rsquo;?  Is this really just as just a matter of familiarity?   The CI/Build process is responsible for signing files to assert that \u0026ldquo;it\u0026rdquo; was responsible for the build (e.g. not just dev artifacts published to S3)  ","id":17,"section":"posts","summary":"Minor experiment with a shell script for signing artifacts that would be generated from a build process.","tags":["jrbeverly"],"title":"gpg-artifact-sign-exp","uri":"/2022/01/jrbeverly-gpg-artifact-sign-exp/","year":"2022"},{"content":"   Fiddling with one of the Bevy examples for provisioning a window with Bevy.     Rusy Bevy Baseline Fiddling with one of the Bevy examples for provisioning a window with Bevy.\nNotes  Initialize setup required installation of libraries like: libasound2-dev libwebkit2gtk-4.0 libudev-dev mingw-w64 Cross-compilation works, but associated GitHub Issues seemed conflicting Earlier version of bevy had issue with missing audio driver (devcontainer) failing the build  Don\u0026rsquo;t think will continue with this for now, maybe investigate later.\nComponents for the chess board sourced from https://caballerocoll.com/blog/bevy-chess-tutorial/\n","id":18,"section":"posts","summary":"Fiddling with one of the Bevy examples for provisioning a window with Bevy.","tags":["jrbeverly"],"title":"bevy-rustlang-example-window","uri":"/2022/01/jrbeverly-bevy-rustlang-example-window/","year":"2022"},{"content":"   Checking exception case with syscall/js     Experiment - WebAssembly Golang + Bazel Experimenting with some issues encountering with WebAssembly, Golang \u0026amp; Bazel\nNotes  Confirmed issue with syscall/js in the basecase with using goos and goarch (toolchains passed as orgs better option?) Using a genrule sufficient to workaround the case Base case with a simple calculator, using just base HTML js.Value conversions, framework wrapper to exist to handle the interop? Directory layout of cmd/ and app feels a bit off, but does help keep the bits separate  ","id":19,"section":"posts","summary":"Experimenting with some issues encountering with WebAssembly, Golang \u0026amp; Bazel","tags":["jrbeverly"],"title":"exp-webassembly-golang-bazel","uri":"/2022/01/jrbeverly-exp-webassembly-golang-bazel/","year":"2022"},{"content":"   Fiddling with overwritten MOTD     Packer Overwrite MOTD Overwriting the MOTD of pre-baked AMIs using Packer\nNotes  Message files are located in /etc/update-motd.d/ Existing ones can be purged and replaced with a fixed one Likely want to keep the status components (source from existing, overwrite) Scripts need to be executable, and include shebang Current packer requires AWS for EC2 create/snapshot, can we avoid?  This works well for setting up a baseline for the AMIs. Is there a way we can perform this Packer construction without needing to be connected to AWS? As at the moment this more resembles \u0026ldquo;Assembly\u0026rdquo; than \u0026ldquo;Build\u0026rdquo;. Having a daemon (virtualbox/hyper-v) is less optimal, but if it can be solely run on any machine with the installed tools, then that would be preferable.\nPotential option is to use Virtualbox (or similar) to create an OVF file, upload to S3 and then import as an AMI using that model. That would require the build machines to have VirtualBox, but that can be worked around. This could make it so that a single server image is compatible with multiple clouds/execution environments.\nWhat about Cloud-specific optimizations or toolchains? E.g. AWS has the CLI, Session Manager and such? Could just be a post-build, like a platform/arch specific compilation for systems.\n","id":20,"section":"posts","summary":"Overwriting the MOTD of pre-baked AMIs using Packer","tags":["jrbeverly"],"title":"packer-overwrite-motd","uri":"/2022/01/jrbeverly-packer-overwrite-motd/","year":"2022"},{"content":"   Experimenting with cobra docs generation     CobraCMD with GenMarkdownTree Experiment with the GenMarkdownTree method available with cobrago.\nActions The command line utility can be executed by running:\nbazel run //cmd/cobradocs version The markdown tree for the docs can be generated by running:\nbazel run //docs/cobradocs -- --dir $PWD Notes  Markdown or YAML generated from the docs works fairly nice Better fit would be YAML, then providing it to a markdown templator (or alternative tools) Feels like having an OpenAPI spec for the CLI would make this process easier (build docs from the OpenAPI spec) Need to have a reference to the command object for the CLI Files are written into a directory, or dumped to a buffer \u0026amp; stdout Can be good for getting some docs out, but is this the ideal direction?  ","id":21,"section":"posts","summary":"Experiment with the GenMarkdownTree method available with cobrago.","tags":["jrbeverly"],"title":"cobra-cmd-with-docs","uri":"/2022/01/jrbeverly-cobra-cmd-with-docs/","year":"2022"},{"content":"   Experiment using Golang WebAssembly for a terminal in-browser     XTerm for Terminal as Browser Experimenting with the idea of a minimum environment for running terminal applications in browser. In essence, allowing a user to navigate to example.com/terminal to view a terminal version of the sites API. With the appropriate token \u0026amp; other bits provided from the browser session tokens.\nNotes  WebAssembly for Golang can be used in combination with this Browser token can be used to authenticate with the service, allowing for commands to exec against it Using something like the cobra yaml export (or deriving actions from OpenAPI-like spec), the JS interface can be generated Validation would be performed within the cmd, although a common \u0026lsquo;OpenAPI-like\u0026rsquo; spec would help reduce the complexity on this front (have better interacted with UIs) Leveraging other wrapper frameworks for xTerm.js would probably suit better for any syntax highlighting that would be desired / color schemas / etc Integration with local machine state for browsers would need to be something handled by the tool itself Better option might just be a browser \u0026ldquo;terminal\u0026rdquo; that understands an OpenAPI-like spec \u0026amp; appropriate JS/WASM bindings  ","id":22,"section":"posts","summary":"Experimenting with the idea of a minimum environment for running terminal applications in browser. In essence, allowing a user to navigate to \u003ccode\u003eexample.com/terminal\u003c/code\u003e to view a terminal version of the sites API. With the appropriate token \u0026amp; other bits provided from the browser session tokens.","tags":["jrbeverly"],"title":"xterm-for-cmd-as-site","uri":"/2022/01/jrbeverly-xterm-for-cmd-as-site/","year":"2022"},{"content":"   Explore using aws iam assume role with cert     AWS AssumeRole with Certificate for CI Exploring the concept of using AWS IoT Certificates for authenticating with AWS.\nThis came up while working with minio, which supports authentication with certificates:\n MinIO provides a custom STS API that allows authentication with client X.509 / TLS certificates.\n Getting Started The commands boostrap.sh and cert.sh are provided for working with the IoT devices. Bootstrap is responsible for provisioning the certificate, IoT resources, and the permissionless IAM role. Assuming that AWS credentials are configured, a test can be started by running the following:\nbash scripts/bootstrap.sh github-my-repository-name This will create the following resources:\n An IAM Role with a trust policy for IoT credentials An IoT Thing \u0026amp; ThingType An IoT Certificate An IoT Role Alias, Policy \u0026amp; associations with the ceritifcate \u0026amp; thing  The output of this command will be located in the output directory under the name provided above (e.g. github-my-repository-name). This will contain the certificate, private/public keys and auth.yaml. The auth.yaml file is intended to work similar to a kubeconfig file, allowing the command get-aws-credentials.bash to retrieve credentials from AWS over HTTPS.\nThis can be done by then running:\nbash scripts/get-aws-credentials.bash github-my-repository-name Notes This has some potential. Rather than leveraging Terraform, I wanted to use an API oriented approach to this, as that is likely how the certificates + associated components would be provisioned. They would be created by an operator-like service that identifies repositories that are requesting authentication to the artifact publishing services. The credentials would be populated into the Continuous Integration/Delivery platform (like GitHub Actions), with rotation and revocation happening base on how the operator is interacted with.\nIt may be possible with this model to support using the \u0026ldquo;generic\u0026rdquo; S3 API for publishing, rather than relying on the AWS CLI. This wouldn\u0026rsquo;t decoupled the CI/CD system from AWS credentials \u0026amp; publishing, but it does create a case where we are no longer relying on AWS_* environment variables, but instead on things like X.509 certificates, and standard-based publishers (e.g. minio - mc)\n","id":23,"section":"posts","summary":"Exploring the concept of using AWS IoT Certificates for authenticating with AWS. This came up while working with minio, which supports authentication with certificates: \u0026gt; MinIO provides a custom STS API that allows authentication with client X.509 / TLS certificates.","tags":["jrbeverly"],"title":"aws-assumerole-with-cert","uri":"/2022/01/jrbeverly-aws-assumerole-with-cert/","year":"2022"},{"content":"   Website in AWS S3 with authenticated access     AWSS3 \u0026amp; AWS Cognito Fiddling with AWS S3 Websites leveraging AWS Cognito for authentication\nTerraform is based on the tutorial https://transcend.io/blog/restrict-access-to-internal-websites-with-beyondcorp/ and the public repository: https://github.com/transcend-io/beyondcorp-cloudfront\nNotes  This approach isn\u0026rsquo;t really something I think is great The lack of ease configuration for Lambdas, and the need to either embed configuration in the lambda zip, or through AWS SSM is not ideal Either a single monolith terraform module, or split between multiple. Adds a lot of overhead which is less than ideal AWS S3 just doesn\u0026rsquo;t feel like a good source for storing the S3 artifacts - requires bucket policy + \u0026ldquo;public\u0026rdquo; - Don\u0026rsquo;t like Seems more preferable to have something like K8s + Nginx pod - Management of secrets/dns/etc can be externalized then Docker images gives the opportunity to sign with something like cosign, ability to \u0026ldquo;extend\u0026rdquo; with URL rewrites/etc through nginx itself Authentication can be fronted as part of existing K8s services used by backends Entire system can be validated with a local installation of K8s Alternative caching layers seem more ideal (e.g. Cloudflare pages / etc), thing that can go infront of the primary service Could a k8s-ingress served entity just synchronize with other CDNs? Cloudfront \u0026ldquo;deploying\u0026rdquo; wait times are kind of in the way for rapid changes (exists in k8s with pods, but cloudfront just feels slower?)  Overall not really liking this approach, as it doesn\u0026rsquo;t simplify the system as much as I was hoping it would.\n","id":24,"section":"posts","summary":"Fiddling with AWS S3 Websites leveraging AWS Cognito for authentication Terraform is based on the tutorial \u003ca href=\"https://transcend.io/blog/restrict-access-to-internal-websites-with-beyondcorp/\"\u003ehttps://transcend.io/blog/restrict-access-to-internal-websites-with-beyondcorp/\u003c/a\u003e and the public repository: \u003ca href=\"https://github.com/transcend-io/beyondcorp-cloudfront\"\u003ehttps://github.com/transcend-io/beyondcorp-cloudfront\u003c/a\u003e","tags":["jrbeverly"],"title":"cloudfront-cognito-private-auth","uri":"/2022/01/jrbeverly-cloudfront-cognito-private-auth/","year":"2022"},{"content":"   Experimenting with the EntityModel for interface boundaries     EntityModel Dapper Experiment Case Exported case of experimenting with using Postgres Functions, Dapper \u0026amp; Entity.Model.\nNotes Experimenting a bit more with the entity model of splitting out the \u0026ldquo;database\u0026rdquo; components (e.g. ID) from the model objects, with an aim towards designing a strict interface for working with the database, that could be generated from a baseline specification. This would allow for things like ReadOnly entities, better field filtering \u0026amp; potential for \u0026ldquo;programmatic\u0026rdquo; improvements to how the database handles searches.\nContinuing with the direction of leveraging functions/stored procedures for databases. This simplifies the internals of the API, letting the clients be only consumers, and having external sources be responsible for the provisioning of the database schema.\nPotential areas of interest:\n Constructing a \u0026ldquo;Database\u0026rdquo; type schema bundle, that can be provisioned on-demand by services (allows DB provisioning to be external \u0026ldquo;infrastructure\u0026rdquo;) \u0026ldquo;Models\u0026rdquo; can generated the DB \u0026amp; interface components for services, as well as CLIs for easy interaction Schema bundles can be combined for \u0026ldquo;migration\u0026rdquo; updates \u0026amp; verification policies (tests / \u0026lsquo;DB Policy\u0026rsquo; - similar to a \u0026ldquo;IAM Policy\u0026rdquo;) Common classes for interacting with the database, with tools to construct the CommandDefinitions to send messages to database How to handle the idea of \u0026ldquo;chaining\u0026rdquo;, e.g. X =\u0026gt; Data, then use that data to get Y.  Options for tools that work well in devcontainers for performing postman-like queries against a server for rapid development.\n","id":25,"section":"posts","summary":"Exported case of experimenting with using Postgres Functions, Dapper \u0026amp; Entity.Model.","tags":["jrbeverly"],"title":"dapper-with-entity-model","uri":"/2021/10/jrbeverly-dapper-with-entity-model/","year":"2021"},{"content":"   Experimenting with generating code from Cuelang definitions     Cuelang with SchemaGen Experimenting with using Cuelang for the purposes of representing a schema, then generating associated files from the original source of truth\nNotes  Schema validation is nice, the base case is straightforward Combining data with this allows for connecting enum/datasets What about stubbing of datasets (e.g. restrict this to \u0026lsquo;Dataset\u0026rsquo; that isn\u0026rsquo;t locally defined?) Text templating isn\u0026rsquo;t really what I want to do with this kind of tool Seems like I need to write more \u0026ldquo;representation/composition\u0026rdquo; than I originally hoped Doesn\u0026rsquo;t seem to support the kind of \u0026ldquo;intentions\u0026rdquo; workflow I was hoping for  This isn\u0026rsquo;t handling the case I\u0026rsquo;m interested in with schema validation \u0026amp; generation. Although it provides a lot of the base essentials out of the box, I\u0026rsquo;m looking more for a cross between schema+terraform. A system that allows the tracking of version drift, and reconcilation as part of its design.\nClarifying my exact intended use case is probably a better direction than fiddlign with this more.\n","id":26,"section":"posts","summary":"Experimenting with using Cuelang for the purposes of representing a schema, then generating associated files from the original source of truth","tags":["jrbeverly"],"title":"cue-for-schema-gen","uri":"/2021/10/jrbeverly-cue-for-schema-gen/","year":"2021"},{"content":"   Experimenting with using bazel to package+test glue shell scripts     Bazel Bash Packaged Experimenting with using Bazel \u0026amp; Bats in container images for writing up tests for shell scripts.\nNotes Sometimes while developing things it can be useful to have a small shell script that performs a simple action or operation, that in the short-term makes sense as a shell script. In this case, it can be useful to add some simple tests to ensure that:\n The script is runnable Simple transformations work as expected Any environment variable or file references work as intended  This isn\u0026rsquo;t intended for cases where a shell script is increasing in complexity, but rather where it acts as \u0026lsquo;Glue\u0026rsquo; responsible for filling in gaps that might exist in a system.\nSome minor notes about this process are included below:\n Using docker allows avoiding requiring the expected toolchains to be installed locally (or defined in bazel) The Bats image is used instead of installed bats onto an image. Installing bats onto an existing image might be a better option Container tests can check other properties of the executable shell scripts (as it should be packed the same as it would be for other targets)  ","id":27,"section":"posts","summary":"Experimenting with using Bazel \u0026amp; Bats in container images for writing up tests for shell scripts.","tags":["jrbeverly"],"title":"bazel-bash-packaged","uri":"/2021/10/jrbeverly-bazel-bash-packaged/","year":"2021"},{"content":"   A Joos programming language compiler, written in Java.     JCompiler Summary A Joos programming language compiler, written in Java.\nGetting Started The project is currently not maintained or kept in runnable order. You may be able to open the project in Eclipse, but at this time the code is only here as readonly.\nNotes The application was written as part of the UWaterloo CS444 Compiler Course.\nGroup Members:\n seanmk sxyuan  Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Stephen Copinger from the Noun Project.\n","id":28,"section":"posts","summary":"A Joos programming language compiler, written in Java.     JCompiler Summary A Joos programming language compiler, written in Java.\nGetting Started The project is currently not maintained or kept in runnable order. You may be able to open the project in Eclipse, but at this time the code is only here as readonly.\nNotes The application was written as part of the UWaterloo CS444 Compiler Course.","tags":["jrbeverly"],"title":"jcompiler","uri":"/2021/10/jrbeverly-jcompiler/","year":"2021"},{"content":"   Exploring bazel toolchains mirrored in AWS S3     bazel-toolchain-from-s3 Experimenting with setting up Bazek toolchains, when the tools are mirrored into an AWS S3 bucket.\nThis builds off previous work done in jrbeverly/bazel-external-toolchain-rule for creating toolchains from files.\nNotes  Implementation of s3_archive uses that same model as http_archive The repository_rule does not use toolchains like rules, meaning bootstrap rules must be downloaded by repository_ctx Repository rules can convert labels to paths using the repository_ctx.path method  To get a fully managed system that doesn\u0026rsquo;t require tools to be installed on the local system, would likely require a bootstrap rule using a tool that is publicly available. This way it could be downloaded using the download_and_extract rule of Bazel, then combined with the other rules for perform the act of downloading binaries. The process as follows:\n Download \u0026amp; configure the cloudio tool for downloading binaries from a cloud source (AWS S3/Azure RM/GCP/etc) Make this tool available to the other repository rules (aspect? label? ?) Repository rule uses this binary to run the download commands  This may have an additional benefit in that the cloudio tool could behave in a content-addressable manner, removing the need for URLs (primary/mirrors/etc) for tools. Instead leaving it up to the cloudio to download the toolchain from the appropriately known registries.\nDesign Jots  Using something like an awsrc file (aim to be similar to netrc) to determine which AWS Profile authentication to use for buckets (is this worthwhile? - Maybe - Our default AWS_PROFILE may not always have artifact access - think isolated AWS accounts like \u0026lsquo;malware\u0026rsquo;) Requires a sha256sum checker, which is only really available on linux (MacOS can install, but default is shasum ), and Windows is PowerShell. AWS CLI is working on parts of the environment like AWS_PROFILE (which may run tools through credential_process)  ","id":29,"section":"posts","summary":"Experimenting with setting up Bazek toolchains, when the tools are mirrored into an AWS S3 bucket. This builds off previous work done in jrbeverly/bazel-external-toolchain-rule for creating toolchains from files.","tags":["jrbeverly"],"title":"bazel-toolchain-from-s3","uri":"/2021/09/jrbeverly-bazel-toolchain-from-s3/","year":"2021"},{"content":"   Exploring enabling bazel toolchains with an external \u0026lsquo;toolchain\u0026rsquo; file     bazel-external-toolchain-rules Experimenting with setting up Bazel toolchains using an externally managed .toolchain file, that is responsible for defining properties such as:\n System compatibility Integrity Checks Tool retrieval locations  Notes This idea came out of the idea of having Bazel rules that were not aware of how toolchains were defined (or what systems they are compatible with), and instead being entirely based on the lock file (.toolchain) available in the caller environment. This means that the bazel rules can in essence just be extensions of the command line specification.\nRepositories would have a collection of .toolchain files, that could be used by other services (e.g. pre-baked GitPod/Codespace/DevContainers, local installs, etc). When used by Bazel, these files would be read by Bazel and converted into the appropriate rules for downloading \u0026amp; setting up the toolchain.\nThe basic idea scratched out:\nload(\u0026#34;//bazel/macros:load_all.bzl\u0026#34;, \u0026#34;register_external_toolchains\u0026#34;) register_external_toolchains( name = \u0026#34;external_toolchains\u0026#34;, toolchains = { \u0026#34;//bazel/toolchains:helm.toolchain\u0026#34;: \u0026#34;bazel_toolchain_helm\u0026#34;, \u0026#34;//bazel/toolchains:yq.toolchain\u0026#34;: \u0026#34;bazel_toolchain_yq\u0026#34;, }, ) Local or imported rules can be specified in the string field (bazel_toolchain_helm), making it simple to map the toolchain definition to the toolchain use in Bazel.\nName=yq Version=4.13.2 [linux_amd64] OS=linux CPU=x86_64 Executable=yq_linux_amd64 URL=https://github.com/mikefarah/yq/releases/download/v4.13.2/yq_linux_amd64.tar.gz Sha256Sum=b462478cfee8fb02b1b6bbee87b2b1d2f0ef4f0b693a95c04308006f04cc525e [darwin_amd64] OS=osx CPU=x86_64 Executable=yq_darwin_amd64 URL=https://github.com/mikefarah/yq/releases/download/v4.13.2/yq_darwin_amd64.tar.gz Sha256Sum=cf0cbf49a423d515d69879c08af4bab64af09f29b949545aa8e3d771a94a3db7 [windows_amd64] OS=windows CPU=x86_64 Executable=yq_windows_amd64.exe URL=https://github.com/mikefarah/yq/releases/download/v4.13.2/yq_windows_amd64.zip Sha256Sum=0b08de85f4de4b55e98fcd69c707bea52f91478603cc4221c024ddd80cfd6141 Custom Tool Stores These can then be combined with custom storages for tools, rather than just relying on the http_archive. An example would be an s3_archive rule that can retrieve these tools from an AWS S3 Bucket responsible for storing these binaries.\nThe aim of this would be to enable the above rules (register_external_toolchain(s)) to support tools or systems that augment the processing of the read toolchain file. If done right, it would allow something like mirroring all toolchain dependencies to an internal store with minimal manual intervention:\n Run a command to download all the toolchains to a custom local directory (toolchains download --directory \u0026lt;xyz\u0026gt;) Upload this directory to a minio or hosted file store (aws s3 sync --recursive \u0026lt;xyz\u0026gt;/ s3://...) Map the toolchains to the now vendored path (toolchains vendor s3 s3://...) In bazel, modify the register_external_toolchains to add an adaptor/affix/aspect/etc that supports downloading from S3.  The above is the scenario this idea was explored, but the hope would be letting the .toolchain (\u0026amp; potentially supporting files) act as the source of truth about toolchains, and letting Bazel interpret it.\nDesign Jots  Instead of URLs for downloading the toolchains, what if instead they were content-addressable? Rather than using bazel to perform the analysis, would it be possible to have a tool generate the entire compatibility-layer? Thus allowing us to only define the necessary \u0026ldquo;version\u0026rdquo; components in a bazel compatible way? Would a system like gazelle make more sense? Having a tool read the .toolchain file, then generate all the repository_rule bindings (\u0026amp; use netrc instead of custom rules?)  ","id":30,"section":"posts","summary":"Experimenting with setting up Bazel toolchains using an externally managed \u003ccode\u003e.toolchain\u003c/code\u003e file, that is responsible for defining properties such as: - System compatibility - Integrity Checks - Tool retrieval locations","tags":["jrbeverly"],"title":"bazel-external-toolchain-rules","uri":"/2021/09/jrbeverly-bazel-external-toolchain-rules/","year":"2021"},{"content":"   Experimenting with using bazel \u0026amp; jsonett to generate data from configuration files     Bazel \u0026amp; Jsonnet Templates Generating files from base configuration files using Jsonnet.\nNotes Exploring the idea of leveraging jsonnet with Bazel to create a series of templates sourced from configuration files. The basic principle of this is how to built in-repository the idea of   Configuration Files { template([inputs]) =\u0026gt; rendered }.\nThe usage of libsonnet lets the lib/ directory contain all of the models, utilities and other means of representing the data structures. Having a structure form to the data is in comparison to other templating systems like gomplate that typically just work off the idea of a text template. Having the structure means that we can leverage transforms to manipulate the data is a logical way, then render it as the expected output text.\nSupport for \u0026ldquo;aspect\u0026rdquo; style transformations of the data is fairly useful as well, as it gives the opportunity to apply things like tags to every resource.\nAlthough this makes the generation of templated data better, it feels like just a more involved templating process, that as the system becomes sufficiently complex, it will eventually be seen as better to transition to a language/DSL better suited to the specialzied task (e.g. terraform, skylark, hcl)\n","id":31,"section":"posts","summary":"Generating files from base configuration files using Jsonnet.","tags":["jrbeverly"],"title":"bazel-jsonnett-templates","uri":"/2021/09/jrbeverly-bazel-jsonnett-templates/","year":"2021"},{"content":"   Experimenting with laying out the licensing stamp for a closed/internal source repository     internal-reserved-license-repo Experimenting with laying out the licensing stamp for a closed/internal source repository\nNotes Experimenting with the idea of what license annotations would look like on an internal repository that is not intended for public distribution. This can seem odd, as the source code files of the project should not be distributed, so the only individuals viewing the license should be those with pre-approved access (i.e. contributors/employees).\nThe intention of this is to explore ideas around having tools \u0026amp; systems be aware of the licensing \u0026ldquo;intentions\u0026rdquo; of the source code. By that I mean, if an automated tool attempted to set a GitHub Repository to \u0026lsquo;Public\u0026rsquo; or migrate it out to the organizations OSS/Public GitHub Organization, the license.spdx file would be noted as UNLICENSED (or the equivalent term) and refuse the action on the grounds that is not permit. Or just not show it as meeting the minimum requirement to be flagged as \u0026lsquo;Open Source\u0026rsquo;.\nI don\u0026rsquo;t think designing everything to have consider what a license.spdx file would be the direction, but instead curious how an \u0026lsquo;affix\u0026rsquo; or \u0026lsquo;aspect\u0026rsquo; oriented style could work with this. This requires further investigation about intentions around annotating resources with external non-technical considerations like licensing/distribution.\n","id":32,"section":"posts","summary":"Experimenting with laying out the licensing stamp for a closed/internal source repository","tags":["jrbeverly"],"title":"internal-reserved-license-repo","uri":"/2021/09/jrbeverly-internal-reserved-license-repo/","year":"2021"},{"content":"   Testing publishing to Dropbox programatically     github-actions-dbx-upload Publishing to Dropbox programmatically from GitHub Actions with the intentions to mirror the model of AWS S3 publishing.\nNotes Exploring how one might leverage Dropbox as an artifact storage source, with an authentication model that is similar to AWS. Setting up the initial Dropbox Application to get the tokens for publishing was a bit more involved than useful, and I suspect the process of rotating these tokens might be a pain too.\nWhen the system is setting up and going, it works fairly well. The user interface \u0026amp; sharing features of Dropbox fit well with an artifact storage system. The fixed billing plans (\u0026amp; reasonable free limit) make it an appealing option for use in comparison to an IaaS like AWS/GCloud/Azure.\nThe authentication \u0026amp; publishing process for this is similar to other options (like AWS S3), it would be possible to use it in a way that creates a common interface that allows easy switching between the two as artifact publishing sources like so:\n- name: Configure Dropbox Credentials uses: ./.github/actions/configure-dropbox-credentials with: dropbox-access-key-id: ${{ secrets.DROPBOX_ACCESS_KEY_ID }} dropbox-secret-access-key: ${{ secrets.DROPBOX_SECRET_ACCESS_KEY }} dropbox-session-token: ${{ secrets.DROPBOX_SESSION_TOKEN }}  - name: Upload to artifacts uses: ./.github/actions/upload-artifacts-to-dropbox with: path: packages/ folder: artifacts/blockycraft/${GITHUB_SHA} vs\n- name: Configure AWS Credentials uses: aws-actions/configure-aws-credentials@v1 with: aws-access-key-id: ${{ secrets.DROPBOX_ACCESS_KEY_ID }} aws-access-key-id: ${{ secrets.DROPBOX_SECRET_ACCESS_KEY }} aws-session-token: ${{ secrets.DROPBOX_SESSION_TOKEN }}  - name: Upload to artifacts uses: aws-actions/upload-artifacts-to-s3 # Doesn\u0026#39;t exist, but using as stub with: path: packages/ folder: artifacts/blockycraft/${GITHUB_SHA} ","id":33,"section":"posts","summary":"Publishing to Dropbox programmatically from GitHub Actions with the intentions to mirror the model of AWS S3 publishing.","tags":["jrbeverly"],"title":"github-actions-dbx-upload","uri":"/2021/08/jrbeverly-github-actions-dbx-upload/","year":"2021"},{"content":"   Terraform runner in CodePipeline     Terraform AWS CodePipline Terraform Executor Terraform executor leveraging the CodePipeline functionality in AWS, for a fully serverless model of executing terraform in AWS.\nGetting Started The primary environment is configured in env/, with a single main file. This provisions the CodePipeline, CodeBuild components, as well as some stub S3 buckets that represent both incoming sources, artifacts. The artifacts bucket is treated as a stand-in for the Terraform state environment variable, but the pipeline itself does not attempt to configure the backend for S3.\nNotes This likely is only really suited to simple pipelines, as it seems clunky to work with. Although the AWS built-in option can be beneficial, this doesn\u0026rsquo;t really make Terraform an appealing option, and instead makes more sense to leverage something like CloudFormation. Cases where this might need more strict access controls, or the ability to have appropriate guardrails seems like it wouldn\u0026rsquo;t work out long term.\nNotes:\n Simple triggers work off the arrival of notifications in Cloudwatch for S3 publishes Can have a single file act as the sort of \u0026ldquo;GitOps\u0026rdquo; authoritative state for the infrastructure Multi-component terraform deploys (e.g. terragrunt) are possilbe, but feel like they wouldn\u0026rsquo;t fit well with the system Terraform for configuring the build pipeline doesn\u0026rsquo;t feel ideal, as opposed to a more \u0026ldquo;configuration\u0026rdquo; based management option like GitHub Actions / Gitlab CI S3 Storage (\u0026amp; dynamodb locking table) should be provisioned externally from the primary provision terraform  Not really sure about this one, as I feel CloudFormation is a better option for that \u0026ldquo;AWS built-in\u0026rdquo; approach, or using an external service responsible for the terraform executor.\n","id":34,"section":"posts","summary":"Terraform executor leveraging the CodePipeline functionality in AWS, for a fully serverless model of executing terraform in AWS.","tags":["jrbeverly"],"title":"terraform-aws-codepipeline-terraform-runner","uri":"/2021/08/jrbeverly-terraform-aws-codepipeline-terraform-runner/","year":"2021"},{"content":"   Experimenting with Golang Analyzers with Bazel     Bazel Golang Inline Analyzer Experimenting with having analyzers locally defined to a repository, rather than externally defined.\nNotes  Requires using go_tool_library instead of go_library due to a dependency change issue (must also use go_tool_library of deps) Baked natively into nogo, so it can be pretty straightforward to test Names of types aren\u0026rsquo;t as simple as package.Type, but instead include other components (using HasSuffix) (What options are there?) Change in \u0026lsquo;internal/cobrago/storage.go@ListFilesInStorage\u0026rsquo; can be removed as a test case for the errors The tools/ directory probably isn\u0026rsquo;t the best path. Want something that we can spin-out/externalize as these evolve with the code  Overall this is a pretty good way to start prototyping mechanisms for code analysis that is right next to the code, which can then later be spun out into their own generic analyzers. As it runs automatically with nogo, there is a natural way of enabling an analyzer in a tiered manner (warning =\u0026gt; error).\nWhat would be necessary to get these setup in as lightweight as possible way to ensure very simple constraints? E.g. Don't use 'XYZ' type while in 'ABC' module. The main goal of having these would be essentially creating \u0026lsquo;Tests\u0026rsquo; for the code to ensure that code is being built in a manner idiomatic to the codebase.\n","id":35,"section":"posts","summary":"Experimenting with having analyzers locally defined to a repository, rather than externally defined.","tags":["jrbeverly"],"title":"golang-analyzer-inline-bazel","uri":"/2021/08/jrbeverly-golang-analyzer-inline-bazel/","year":"2021"},{"content":"   Experiments with GitHub App written in Golang     GitHub App in Golang Prototyping GitHub App written in Golang with the AWS \u0026amp; GitHub integrations split away, to try and encode the core \u0026lsquo;concepts\u0026rsquo; solely into the lib/ component\nNotes  Development (\u0026amp; Testing) should support a non-smee way of development How would integration/infrastructure/e2e tests work with this kind of system? Would it make more sense to have an OpenAPI system, with the GitHub integration performing the interface? Is this similar to Hubot, in that having an interface layer/service makes more sense, as it avoids the requirement for direct interface? (This can be within the system itself)  ","id":36,"section":"posts","summary":"GitHub App written in Golang with the AWS \u0026amp; GitHub integrations split away, to try and encode the core \u0026lsquo;concepts\u0026rsquo; solely into the lib/ component","tags":["jrbeverly"],"title":"github-app-golang","uri":"/2021/07/jrbeverly-github-app-golang/","year":"2021"},{"content":"   Validating that K3s in internal HomeLab is viable     K3s In HomeLab Proof of Concept Determining how viable it would to be switch from using docker-compose to using K3s to run my internal homelab environment.\nGetting Started The installation process assumes that you have a freshly imaged ubuntu machine, connected to the internal network, and with a minimum of password-based SSH access.\nInstalling dependencies The tools FluxCD \u0026amp; K3sup can be installed on the host machine using the script setup.sh (scripts/setup.bash). This can be done like so:\nbash scripts/setup.bash When this is completed, you should have K3sup and flux installed in your environment.\nSetting up SSH keys for K3sup To setup SSH keys for K3sup, you can run the script ssh-for-fresh.sh (scripts/ssh-for-fresh.sh), that will create an SSH key pair, copy it to the machine, and adjust the SSH configurations (both on host+machine) to expect SSH keys for connections.\nYou only need this for the install of k3sup, and can allow password-based login again after install.\nbash scripts/ssh-for-fresh.sh \u0026lt;ip\u0026gt; \u0026lt;user\u0026gt; Installing K3s on the machine You can perform the installation for the machine by running the script k3s-install.sh (scripts/k3s-install.sh). You can do that like so:\nbash scripts/k3s-install.sh \u0026lt;ip\u0026gt; \u0026lt;user\u0026gt; Installing FluxCD on the cluster You can perform the installation for the cluster by running the script fluxcd-install.sh (scripts/fluxcd-install.sh). You can do that like so:\nbash scripts/k3s-install.sh \u0026lt;name of cluster\u0026gt; \u0026lt;github username\u0026gt; \u0026lt;github repository name (to be created)\u0026gt; This will create the github repository with the basic fluxcd configuration setup.\nSetting up the configuration Follow the steps in the config/ directory to get your config+secrets configured in the cluster.\nCopying in the Kustomizations Copy in all of the Kustomization yaml files located in this directory under clusters/homelab/. These setup the cluster with a series of namespaces/CRD/networking/certs. These are all configured with the jrbeverly.dev domain for prototyping.\nFor the domain resolution to work, you\u0026rsquo;ll need to manually create the domains as they exist in the YAML in Cloudflare.\nNotes  GitPod installation did not work as desired Setting up services like require cross-talk in terms of filesystems would likely be high overhead Layout of directories could use some improvement (same with naming) Secrets management could be simplified, but what is the best way to handle this? GitHub private repositories are supported by leveraging FluxCD Bootstrap instead of manually setting it up Using kubectl get secrets you can find the Opaque secret created for the Deploy Key with FluxCD Bootstrap (\u0026amp; rotate/change it) Initial setup for this requires a GitHub Personal Access Token (PAT) to be created, but can be removed when Deploy Keys are configured If the deploy keys disappear from GitHub, you can retrieve them from the secret in K3s (kubectl get secrets), then upload to GitHub.  ","id":37,"section":"posts","summary":"Determining how viable it would to be switch from using docker-compose to using K3s to run my internal homelab environment.","tags":["jrbeverly"],"title":"k3s-at-home-poc","uri":"/2021/07/jrbeverly-k3s-at-home-poc/","year":"2021"},{"content":"   Experimenting with using GitPod for Golang CLI apps with Cobra     GitPod Golang CLI Leveraging GitPod for prototyping out a golang cli that interfaces with AWS.\nNotes  The .gitpod.yml file must exist in the root directory Dockerfile(s) for the environment can be specified in its own directory (.gitpod) Commands can be run on start-up, ensuring that the build is working as expected GitHub Permissions required to make changes to GitHub Actions workflows GitHub Permissions required for a series of commit/pull request based actions Workspaces can be provisioned/stopped/cleaned up on-demand Docker works on the provisioned nodes Extensions \u0026amp; Other components are defined by the .gitpod.yml file  In comparison, the code-server approach is to bake all the dependencies into the same image that is running code-server itself. This means that extensions/settings/etc can be baked onto the image that are common across projects. If working in Docker, it requires a bit more process to work with other docker processes (running on same docker network, matching file system paths, etc).\nGitPod lacks the Progress Web App (PWA) experience that code-server has, which is unfortunate as its very helpful to treat it as if it were its own application.\n","id":38,"section":"posts","summary":"Leveraging GitPod for prototyping out a golang cli that interfaces with AWS.","tags":["jrbeverly"],"title":"gitpod-cobra-golang","uri":"/2021/07/jrbeverly-gitpod-cobra-golang/","year":"2021"},{"content":"   Experimenting with some aspects of the Rust language     Rust Language Checks Experimenting with aspects of Rustlang for working with database, and immutable data structures.\nNotes  How might the Entity.Model apply to this? Where we split the id from the data struct? Immutable hashmap appears to need additional crates (cursory look) The /src \u0026amp; /lib standard directories is a nice component Using .env to reference the database URL Requires some additional components for PSQL \u0026amp; Diesel  ","id":39,"section":"posts","summary":"Experimenting with aspects of Rustlang for working with database, and immutable data structures.","tags":["jrbeverly"],"title":"rust-lang-checks","uri":"/2021/07/jrbeverly-rust-lang-checks/","year":"2021"},{"content":"   Creating AMIs from a common packaged environment in AWS     Packer Bakery with AWS Native Creating pre-baked AMIs using Packer within AWS Native resources (Codepipeline / CodeBuild).\nNotes  Tuning minimum permissions can be a bit difficult with the CodePipeline/CodeBuild error messages Artifacts bucket should store only temporary/cache files, and should be destroyable Logs from CodePipeline \u0026amp; CodeBuild can be restricted to a specific log group Unique \u0026lsquo;key\u0026rsquo; identifier allow single-use of module within a common key-scope Events on-complete require additional resources/overhead Image is amazon pre-built, installing Packer on-the-fly  Although nice to leverage IAM solely for this, the benefits don\u0026rsquo;t really outway the issues with leveraging CodePipeline for this kind of build. Splitting this CI/artifact process is less than ideal, but granting credentials to external providers that can spin up any EC2 \u0026amp; run arbitrary scripts has its concern points.\n","id":40,"section":"posts","summary":"Creating pre-baked AMIs using Packer within AWS Native resources (Codepipeline / CodeBuild).","tags":["jrbeverly"],"title":"packer-bake-with-aws-native","uri":"/2021/05/jrbeverly-packer-bake-with-aws-native/","year":"2021"},{"content":"   Prototyping ideas with using Bazel and AWS Cloud Development Kit to create cloudformation templates     bazel-and-aws-cdk Prototyping ideas with using Bazel and AWS Cloud Development Kit to create cloudformation templates\n","id":41,"section":"posts","summary":"Prototyping ideas with using Bazel and AWS Cloud Development Kit to create cloudformation templates","tags":["jrbeverly"],"title":"bazel-and-aws-cdk","uri":"/2021/04/jrbeverly-bazel-and-aws-cdk/","year":"2021"},{"content":"   Repository templating \u0026amp; code automation experiments     Repository Templating \u0026amp; File Automation Experimenting with a model of building a lightweight cron+bash system for performing templating\u0026amp;file modification to multiple repositories.\nIdea The basics of this repository was the idea of leveraging the GitHub CLI (gh) to automate the creation of pull requests that were intended to facilitate common chore work in repositories. This would reduce the need to handle things like setting up license files, formatting, metadata, GitHub Actions, etc.\nRather than continuously scanning all of the repositories for anything that might not be compliant with this, this instead works based off a work-item model, where an entry in created in the database (which is just a directory tasks in this repository). On a schedule items would be pulled from the work queue, executing the process until at least one \u0026ldquo;change\u0026rdquo; was made. If everything was up to spec, then all of the available tasks would be removed from the directory.\nThe task executions are staggered to avoid triggering a mass change in all repositories, as to avoid having to cleanup a mess of invalid changes / PRs that need to be declined.\nAn example of a work item would be:\n{ \u0026#34;branch\u0026#34;: \u0026#34;ensure-workflows-are-linted\u0026#34;, \u0026#34;script\u0026#34;: \u0026#34;actions/workflow-lint.sh\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;docs(build): Workflow linting process\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;Body of the pull request\u0026#34;, \u0026#34;labels\u0026#34;: \u0026#34;documentation,chore\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;jrbeverly/repository-template-file-invoke-prototype\u0026#34; } Notes  This rough model works suprisingly well for getting some \u0026lsquo;base\u0026rsquo; elements going Merge queue, or some form of automated approvals makes sense for some of these changes (as they are additive only changes) GitHub Apps (or some managed \u0026ldquo;service-cron\u0026rdquo;) would be a better approach rather this rough \u0026ldquo;bash\u0026rdquo; process Often times the templates \u0026amp; modifications that wish to be applied to repositories would be better as a \u0026lsquo;fit-for-purpose\u0026rsquo; code/service Validating the scripts against multiple repositories is a bit of a pain Using Git as a \u0026ldquo;database\u0026rdquo; for a work queue that only pulls a single digit number of times per day works suprisingly well Staggering the changes works great to avoid a sudden huge set of changes all at once This kind of system should be split out from the CI processes, as coupling it introduces some pain points with regards to the \u0026lsquo;CRONJOB\u0026rsquo; nature of this  ","id":42,"section":"posts","summary":"Experimenting with a model of building a lightweight cron+bash system for performing templating\u0026amp;file modification to multiple repositories.","tags":["jrbeverly"],"title":"repository-template-file-invoke-prototype","uri":"/2021/03/jrbeverly-repository-template-file-invoke-prototype/","year":"2021"},{"content":"   A set of vim, zsh, git, and configuration files.     dotfiles A set of vim, zsh, git, and configuration files.\n","id":43,"section":"posts","summary":"A set of vim, zsh, git, and configuration files.","tags":["jrbeverly"],"title":"dotfiles","uri":"/2021/03/jrbeverly-dotfiles/","year":"2021"},{"content":"   Machine Learning Lab     Machine Learning Lab A repository for aggregating my machine learning exercises, practices and learning labs. The projects included in this repository are based on the coursework for Udacity\u0026rsquo;s Deep Learning Nanodegree Foundations. These are primarily from working on the Machine Learning Nanodegree offered by Udacity.\nThe project files are built using Jupyter Book into a web-accessible form.\nGetting Started The conda environment for working with all of the Jupyter Notebooks is provided as environment.yml. This environment is used for all testing by Github Actions and can be setup by:\nThe environment can be setup by running the following:\nconda env create -f environment.yml conda activate ml-learning-lab The Jupyter book can interacted with using the following command:\njb build notebooks/ ","id":44,"section":"posts","summary":"A repository for aggregating my machine learning exercises, practices and learning labs. The projects included in this repository are based on the coursework for Udacity\u0026rsquo;s Deep Learning Nanodegree Foundations. These are primarily from working on the Machine Learning Nanodegree offered by Udacity. The project files are built using Jupyter Book into a web-accessible form.","tags":["jrbeverly"],"title":"ml-learning-lab","uri":"/2021/03/jrbeverly-ml-learning-lab/","year":"2021"},{"content":"   Prebuilt, development environment in the browser  powered by VS Code     Codespace Prebuilt, development environment in the browser  powered by VS Code.\nThis image acts as a catch-all image for doing full-stack development in a polyglot type environment. The running container makes use of the host docker service to allow for docker builds.\n","id":45,"section":"posts","summary":"Prebuilt, development environment in the browser  powered by VS Code. This image acts as a catch-all image for doing full-stack development in a polyglot type environment. The running container makes use of the host docker service to allow for docker builds.","tags":["jrbeverly"],"title":"codespace","uri":"/2021/03/jrbeverly-codespace/","year":"2021"},{"content":"   Ansible descriptions of internal home network     HomeLab - Internal Tooling Ansible playbooks for configuring services running within my internal home cloud.\nGetting Started DevContainers The DevContainer environment can be started by opening the repository in VSCode and installing the \u0026lsquo;Remote - Containers\u0026rsquo; extension. When started, the prompt will build and image and configure the container.\nThe deployments to any of the environment can be triggered by running any of the helper scripts available in /opt/bin. To deploy the codelab environment, you can run codelab.\nLocal Environment The local environment can be setup by running the command:\nsource .devcontainer/local/setup.bash Deployments for the environments can then be triggered by running the scripts available local/. This requires docker on the machine to run.\nConfiguration MediaLab Images The medialab environment has the requirement of specifying all server images as images.json, as most of the projects do not have official images, and this lets the images change in a jq-friendly manner.\nThe sample of these look like so:\n{ \u0026#34;emby\u0026#34;: \u0026#34;ghcr.io/linuxserver/...\u0026#34;, \u0026#34;indexer\u0026#34;: \u0026#34;ghcr.io/linuxserver/...\u0026#34;, \u0026#34;downloader\u0026#34;: \u0026#34;ghcr.io/linuxserver/...\u0026#34;, \u0026#34;movie_manager\u0026#34;: \u0026#34;ghcr.io/linuxserver/...\u0026#34;, \u0026#34;tv_manager\u0026#34;: \u0026#34;ghcr.io/linuxserver/...\u0026#34; } DNS Secrets The secrets for use in DNS and VPN connectivity are specified as environment variables, and the environment file defining them is available in LastPass under HomeLab. These secrets are necessary for all web-enabled machines.\nThe sample of these look like so:\nexport NAMECHEAP_TOKEN=\u0026#39;xyz\u0026#39; export OPENVPN_USERNAME=\u0026#39;pXX123\u0026#39; export OPENVPN_PASSWORD=\u0026#39;abc\u0026#39; Structure All docker related outputs are dumped to the /srv directory. THis acts as the root for the docker containers running on any of the servers. The top level contains the docker-compose YAML files that define what is running on the machine. This is split into different kinds of components (nginx, server, utility) to make it easier to share concepts between machines.\nThe intended directory structure is like so:\n\u0026gt; srv/ \u0026gt; etc/ : secrets, environment variables and configuration data \u0026gt; data/ : data shared between the deployed server \u0026gt; tmp/ : temporary data (or data that can be destroyed + recreated) The code uses copy-over-ref for the YAML configuration to allow each machine to be slightly unique in its configuration if the need presents itself.\nNotes I\u0026rsquo;ve been running variations of my home network for a while now, with varying degrees of complexity. Some times running a full suite of software development tooling, or just running a couple of simple websites for hosting home media/photos. The biggest issue I\u0026rsquo;ve had is with configuration getting out of date.\nEven when tracking the docker-compose files in GitHub, they tended to get out-of-sync as the way of making changes was to scp+docker-compose up or git pull+docker-compose up. Eventually they would get in a bad state, remain in that state for a while due to time constraints, and just be a pain.\nSwitching to ansible for this, along with this design structure is intended to remove this annoying burden, by making ansible the primary way to make changes. Any changes that exist within /srv that is infrastructure related that is not managed by ansible should be blown away.\nThis hopefully will make issues related to the docker images less of a problem in the long term for running this infrastructure.\n","id":46,"section":"posts","summary":"Ansible playbooks for configuring services running within my internal home cloud.","tags":["jrbeverly"],"title":"home","uri":"/2020/11/jrbeverly-home/","year":"2020"},{"content":"   code-server running on AWS Lightsail     Running code-server on AWS Lightsail Summary Run VS Code on an AWS Lightsail instance with auto-generated password and static IP. Early experiments with cloud-driven development environments configured on-demand using terraform.\nInitial exploratory work for seeing what changes exist in the workflows, and any issues that may arise as a result of working in Lightsail.\nNotes Below are some quick points noted while experimenting with this:\nAWS Access Keys In comparison to running this on ECS or EC2, AWS access keys need to be generated and supplied if you wish to run AWS commands.\nThis is nice to some degree if I rely on aws sso to have temporary credentials. Each instance can then focus on being configured for aws sso (or related sso tooling) to get keys.\nPulling data from the instance would be an issue, and make cross-cutting concerns (like backups / logging) more difficult.\nBaked Images Unlike ECS or EC2, it doesn\u0026rsquo;t offer the options of pre-baking an image. That would be ideal, as it would allow for layering (base -\u0026gt; developer -\u0026gt; devops -\u0026gt; admin) that has the minimum necessary tools and recommended specs. Branching could also be an option depending on the type of workloads (perf, load, cross-browser validation).\nProvisioning with a single shell script works fine for the demo, but a fully featured environment would need something like ansible.\nPorts Terraform doesn\u0026rsquo;t support defining the ports at this time, and requires a workaround mechanism. This isn\u0026rsquo;t ideal, and makes it difficult to have a more flexible model.\nManual modification of the firewall is less than ideal, as the desired intent is to be completely automated. In this way running in an ECS/EC2 environment is superior as it allows for restricting incoming traffic to just the VPN. Ports can then be exposed in blocks without risking open internet access.\nOverall Thoughts AWS Lightsail doesn\u0026rsquo;t really scale to the desired solution. I\u0026rsquo;d like something that allows public access but can be locked behind sufficient guardrails (such as a VPN + SSO).\nWorking with terraform to provision the environment is nice, and with a gitops approach could make provisioning dev environments on demand really easy.\nDefining the environments may encounter some problems, as I\u0026rsquo;d like to avoid YAML sprawl when figuring out what should be on the instance.\n","id":47,"section":"posts","summary":"Run VS Code on an AWS Lightsail instance with auto-generated password and static IP. Early experiments with cloud-driven development environments configured on-demand using terraform. Initial exploratory work for seeing what changes exist in the workflows, and any issues that may arise as a result of working in Lightsail.","tags":["jrbeverly"],"title":"aws-lightsail-codespaces","uri":"/2020/08/jrbeverly-aws-lightsail-codespaces/","year":"2020"},{"content":"   Experimenting with lambda as on-demand app     Express in Deployments A simple Express application built with the intent to test an Express server running in different environments (local, docker, lambda).\nUsage The available make commands can be listed with make help:\nUsage: make \u0026lt;target\u0026gt; help This help text. Serverless deploy Deploy the lambda with serverless remove Destroys the instructure Express local Locally run the app Docker docker Build and run in a docker container Notes Simple service with the intent to be used for some AWS work involving cloud costing, cold starts and on-demand provisioning of services. There are a couple projects that I\u0026rsquo;m looking at that don\u0026rsquo;t need a server with 100% uptime. An on-demand server would be ideal, as it significantly reduces the costs associated with the server.\nAdditionally if it is running in a FaaS infrastructure, then there isn\u0026rsquo;t a need to worry about any real infrastructure. Anything that is a bit complex can be addressed with serverless or terraform (if necessary).\nIt does raise some concerns about how to perform monitoring and balance concerns (costing / on-demand approach).\nOn Usages In terms of project layout, the Makefile works to act as a coordinater for all the components. It is less than an ideal, but I aimed to keep the structure as simple as possible. With it I can pretty easily test some use cases with docker and a server app. The split off app/ avoids the over-reliance on serverless or AWS lock-in.\n","id":48,"section":"posts","summary":"A simple Express application built with the intent to test an Express server running in different environments (local, docker, lambda).","tags":["jrbeverly"],"title":"aws-lambda-simple-service","uri":"/2020/08/jrbeverly-aws-lambda-simple-service/","year":"2020"},{"content":"   Repository mapping to the infrastructure of jrbeverly.     jrbeverly Represents the infrastructure resources of \u0026lsquo;jrbeverly\u0026rsquo;, keeping track of infrastructure components, assets and other resources that are needed for components.\nDirectory Mappings The directory layout is modelled after the Linux directories (/var, /etc/, /root, /srv, /opt, etc). This is intended to handle cases as they are adopted into the standards.\n/srv Represents the website resources for any hosted domain.\n","id":49,"section":"posts","summary":"Represents the infrastructure resources of \u0026lsquo;jrbeverly\u0026rsquo;, keeping track of infrastructure components, assets and other resources that are needed for components.","tags":["jrbeverly"],"title":"jrbeverly.web","uri":"/2020/06/jrbeverly-jrbeverly.web/","year":"2020"},{"content":"   Blockycraft is a Minecraft inspired Block Engine.     blockycraft Summary Blockycraft is a Minecraft inspired Block Engine written in Unity3D and built using GitHub Actions. The intent of this project is to better learn Unity, and discover some new use cases with GitHub Actions.\nThe project is available as binary releases for different operating systems, and includes a hosted WebGL version that can be played in supported browsers:\nblockycraft.jrbeverly.dev/play\nThere is no formal feature list or any intentions to carry the project long-term into a fully featured block engine.\nBuild Pipeline The build pipeline is done using GitHub Actions and the unity-builder actions. To avoid over-using the runners, the builds only trigger for pull requests, web deploys and releases. Using only GitHub services, it removes the need to manage deploy keys and infrastructure.\nA playable WebGL copy is available at blockycraft.jrbeverly.dev/play, and the binaries for the project are available on the releases tab.\nAcknowledgements The project assets were created by kenney.nl and available for download by others. The game assets are used under CC0 1.0 Universal.\nThe project icon uses assets by Kenney from kenney.nl/. The game assets have been combined together to produce the output.\n","id":50,"section":"posts","summary":"Blockycraft is a Minecraft inspired Block Engine written in Unity3D and built using GitHub Actions. The intent of this project is to better learn Unity, and discover some new use cases with GitHub Actions. The project is available as binary releases for different operating systems, and includes a hosted WebGL version that can be played in supported browsers: \u003ca href=\"https://blockycraft.jrbeverly.dev/play\"\u003eblockycraft.jrbeverly.dev/play\u003c/a\u003e \u003cimg src=\"./docs/screenshots/world.png\" alt=\"blockycraft world\" title=\"Blockycraft\"\u003e There is no formal feature list or any intentions to carry the project long-term into a fully featured block engine.","tags":["blockycraft"],"title":"blockycraft","uri":"/2020/05/blockycraft-blockycraft/","year":"2020"},{"content":"   README of the Blockycraft organization     Blockycraft Summary Blockycraft is a Minecraft inspired Block Engine written in Unity3D and built using GitHub Actions. The intent of this project is to better learn Unity, and discover some new use cases with GitHub Actions.\nThe project is available as binary releases for different operating systems, and includes a hosted WebGL version that can be played in supported browsers:\nblockycraft.jrbeverly.dev/play\nThere is no formal feature list or any intentions to carry the project long-term into a fully featured block engine.\nHistory The Blockycraft project was first developed for a University of Waterloo graphics course, and this codebase is available as blockycraft-classic. The first codebase went through some revisions in an attempt to address the technical debt incurred in the original development cycle for the demo. While these did make some improvements, they still left the codebase in a less than ideal state.\nWith the introduced of GitHub Actions, and a recent spark of interest in using Unity, I looked to rebuild parts of the project in Unity. The project isn\u0026rsquo;t intended to be feature complete with the original blockycraft, but instead be a enjoyable project for tinkering with unity.\n","id":51,"section":"posts","summary":"Blockycraft is a Minecraft inspired Block Engine written in Unity3D and built using GitHub Actions. The intent of this project is to better learn Unity, and discover some new use cases with GitHub Actions. The project is available as binary releases for different operating systems, and includes a hosted WebGL version that can be played in supported browsers: \u003ca href=\"https://blockycraft.jrbeverly.dev/play\"\u003eblockycraft.jrbeverly.dev/play\u003c/a\u003e \u003cimg src=\"./screenshots/hut-with-mine.png\" alt=\"blockycraft world\" title=\"Blockycraft\"\u003e There is no formal feature list or any intentions to carry the project long-term into a fully featured block engine.","tags":["blockycraft"],"title":"readme","uri":"/2020/05/blockycraft-readme/","year":"2020"},{"content":"   Dockerfiles for CardboardCI images     cardboardci-dockerfiles Dockerfiles for CardboardCI\u0026rsquo;s Docker images.\n","id":52,"section":"posts","summary":"Dockerfiles for \u003ca href=\"https://hub.docker.com/r/cardboardci\"\u003eCardboardCI\u0026rsquo;s Docker images\u003c/a\u003e.","tags":["cardboardci"],"title":"dockerfiles","uri":"/2020/04/cardboardci-dockerfiles/","year":"2020"},{"content":"   README for the Friending Community     Friending Summary Friending is an online dating, friendship, and social networking mobile application that features user-created questionnaires. Friending has two primary features: joining groups to find people similar to you or signing up for events happening in your local area. Friending is a prototype built with the Proto.io application prototyping tool.\nThe Friending prototype was built as part of a requirements specification project. The project focused on the development of a user manual around a fictional matchmaking application called Friending. The application centered around user-created questionnaires that could be used to find relationship matches. The user manual was designed with the goal of deceiving the reader into believing that the application existed. A project vision document and set of user requirements guided the development of scenarios and use cases for the application. The fully-interactive high-fidelity prototype created for the requirements specification project is provided in this repository, available as a standalone HTML page.\nManual The Friending user manual provides info and tips to help you understand the mobile application. The requirements specification project involved the creation of a user manual for the fictional mobile application Friending. The Friending prototype is the actualization of a user vision and set of requirements to construct a matchmaking application. The vision and requirements were used to develop the expected behaviour of the prototype, although not all requirements were actualized into the interactive prototype. The prototype merely needed to present a faithful representation of the original vision.\n","id":53,"section":"posts","summary":"README for the Friending Community     Friending Summary Friending is an online dating, friendship, and social networking mobile application that features user-created questionnaires. Friending has two primary features: joining groups to find people similar to you or signing up for events happening in your local area. Friending is a prototype built with the Proto.io application prototyping tool.\nThe Friending prototype was built as part of a requirements specification project.","tags":["thefriending"],"title":"readme","uri":"/2020/04/thefriending-readme/","year":"2020"},{"content":"   README for the XPlatformer Community     XPlatformer Summary XPlatformer is a simple video game reminiscent of the classic side-scrolling arcade game, using the XLib API. The point of the game is to control a character through a terrain to meet an objective. The project makes use of the XLib API (XOrg) and focus on code that was developed to accomplish tasks for the assignment task.\nAcknowledgements The project icon is retrieved from kenney.nl. The original source material has been altered for the purposes of the project. The icon is used under the terms of the CC0 1.0 Universal.\nThe project icon uses assets by Kenney from kenney.nl/.\nResources All art assets were acquired from http://opengameart.org/ in particular from http://opengameart.org/users/kenney. Majority of art assets come from a particular package known as \u0026ldquo;Platformer Art Deluxe\u0026rdquo; available at http://opengameart.org/content/platformer-art-deluxe. If you would like to know more about these art assets, look into http://open.commonly.cc/ or the \u0026ldquo;Open Bundle\u0026rdquo; [See http://www.kenney.nl/]. The art assets are available with the Creative Commons License (CC0)\n","id":54,"section":"posts","summary":"README for the XPlatformer Community     XPlatformer Summary XPlatformer is a simple video game reminiscent of the classic side-scrolling arcade game, using the XLib API. The point of the game is to control a character through a terrain to meet an objective. The project makes use of the XLib API (XOrg) and focus on code that was developed to accomplish tasks for the assignment task.","tags":["xplatformer"],"title":"readme","uri":"/2020/04/xplatformer-readme/","year":"2020"},{"content":"   Leverage a few basic machine learning concepts to assist you and a client with finding the best selling price for their home.     Predicting Boston Housing Prices Evaluate the performance and predictive power of a model that has been trained and tested on data collected from homes in suburbs of Boston, Massachusetts. A model trained on this data that is seen as a good fit could then be used to make certain predictions about a home  in particular, its monetary value. This model would prove to be invaluable for someone like a real estate agent who could make use of such information on a daily basis.\nGetting Started You can spin up the environment using docker by running the following commands:\nbash docker.bash # inside docker container bash .build/conda.bash Alternatively you can manually install Python and the following Python libraries installed:\n NumPy Pandas matplotlib scikit-learn  You will also need to have software installed to run and execute a Jupyter Notebook\nData The modified Boston housing dataset consists of 489 data points, with each datapoint having 3 features. This dataset is a modified version of the Boston Housing dataset found on the UCI Machine Learning Repository.\nFeatures  RM: average number of rooms per dwelling LSTAT: percentage of population considered lower status PTRATIO: pupil-teacher ratio by town MEDV: median value of owner-occupied homes (Target Variable)  ","id":55,"section":"posts","summary":"Evaluate the performance and predictive power of a model that has been trained and tested on data collected from homes in suburbs of Boston, Massachusetts. A model trained on this data that is seen as a good fit could then be used to make certain predictions about a home  in particular, its monetary value. This model would prove to be invaluable for someone like a real estate agent who could make use of such information on a daily basis.","tags":["jrbeverly"],"title":"boston-housing","uri":"/2020/02/jrbeverly-boston-housing/","year":"2020"},{"content":"   CharityML is a fictitious charity organization that was established to provide financial support for people eager to learn machine learning.     Finding Donors for CharityML Employ several supervised algorithms to accurately model individuals' income using data collected from the 1994 U.S. Census. From the best candidate algorithm from preliminary results and further optimize this algorithm to best model the data. Construct a model that accurately predicts whether an individual makes more than $50,000. This sort of task can arise in a non-profit setting, where organizations survive on donations. Understanding an individual\u0026rsquo;s income can help a non-profit better understand how large of a donation to request, or whether or not they should reach out to begin with. While it can be difficult to determine an individual\u0026rsquo;s general income bracket directly from public sources, we can (as we will see) infer this value from other publically available features.\nGetting Started You can spin up the environment using docker by running the following commands:\nbash docker.bash # inside docker container bash .build/conda.bash Alternatively you can manually install Python and the following Python libraries installed:\n NumPy Pandas matplotlib scikit-learn  You will also need to have software installed to run and execute a Jupyter Notebook\nData The modified census dataset consists of approximately 32,000 data points, with each datapoint having 13 features. This dataset is a modified version of the dataset published in the paper \u0026ldquo;Scaling Up the Accuracy of Naive-Bayes Classifiers: a Decision-Tree Hybrid\u0026rdquo;, by Ron Kohavi. You may find this paper online, with the original dataset hosted on UCI.\nFeatures  age: Age workclass: Working Class (Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked) education_level: Level of Education (Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool) education-num: Number of educational years completed marital-status: Marital status (Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse) occupation: Work Occupation (Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces) relationship: Relationship Status (Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried) race: Race (White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black) sex: Sex (Female, Male) capital-gain: Monetary Capital Gains capital-loss: Monetary Capital Losses hours-per-week: Average Hours Per Week Worked native-country: Native Country (United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad\u0026amp;Tobago, Peru, Hong, Holand-Netherlands) income: Income Class (\u0026lt;=50K, \u0026gt;50K) (Target Variable)  ","id":56,"section":"posts","summary":"Employ several supervised algorithms to accurately model individuals' income using data collected from the 1994 U.S. Census. From the best candidate algorithm from preliminary results and further optimize this algorithm to best model the data. Construct a model that accurately predicts whether an individual makes more than $50,000. This sort of task can arise in a non-profit setting, where organizations survive on donations. Understanding an individual\u0026rsquo;s income can help a non-profit better understand how large of a donation to request, or whether or not they should reach out to begin with. While it can be difficult to determine an individual\u0026rsquo;s general income bracket directly from public sources, we can (as we will see) infer this value from other publically available features.","tags":["jrbeverly"],"title":"charityml","uri":"/2020/02/jrbeverly-charityml/","year":"2020"},{"content":"   Applying unsupervised learning techniques on product spending data to identify customer segments hidden in the data.     Creating Customer Segments Analyze a dataset containing data on various customers' annual spending amounts (reported in monetary units) of diverse product categories for internal structure. One goal of this project is to best describe the variation in the different types of customers that a wholesale distributor interacts with. Doing so would equip the distributor with insight into how to best structure their delivery service to meet the needs of each customer.\nGetting Started You can spin up the environment using docker by running the following commands:\nbash docker.bash # inside docker container bash .build/conda.bash Alternatively you can manually install Python and the following Python libraries installed:\n NumPy Pandas matplotlib scikit-learn  You will also need to have software installed to run and execute a Jupyter Notebook\nData The customer segments data is included as a selection of 440 data points collected on data found from clients of a wholesale distributor in Lisbon, Portugal. More information can be found on the UCI Machine Learning Repository.\nNote (m.u.) is shorthand for monetary units.\nFeatures  Fresh: annual spending (m.u.) on fresh products (Continuous); Milk: annual spending (m.u.) on milk products (Continuous); Grocery: annual spending (m.u.) on grocery products (Continuous); Frozen: annual spending (m.u.) on frozen products (Continuous); Detergents_Paper: annual spending (m.u.) on detergents and paper products (Continuous); Delicatessen: annual spending (m.u.) on and delicatessen products (Continuous); Channel: {Hotel/Restaurant/Cafe - 1, Retail - 2} (Nominal) Region: {Lisbon - 1, Oporto - 2, or Other - 3} (Nominal)  ","id":57,"section":"posts","summary":"Analyze a dataset containing data on various customers' annual spending amounts (reported in monetary units) of diverse product categories for internal structure. One goal of this project is to best describe the variation in the different types of customers that a wholesale distributor interacts with. Doing so would equip the distributor with insight into how to best structure their delivery service to meet the needs of each customer.","tags":["jrbeverly"],"title":"customer-segments","uri":"/2020/02/jrbeverly-customer-segments/","year":"2020"},{"content":"   Blockycraft is a Minecraft inspired Block Engine.     Summary Blockycraft is a interactive graphics demo to create a Minecraft inspired demo which revolves around breaking and placing blocks. The game world is composed of rough cubes arranged in a fixed grid pattern and representing different materials, such as dirt, stone, and snow. The techniques used in the demo can be toggled using keyboard commands. The Blockycraft project is written using C++ and OpenGL.\nNotes The project was developed for a University of Waterloo Graphics course in Summer 2016. The focus was on graphics techniques (e.g. transparency, ambient occlusion) or gameplay.\nThe original blockycraft project was a hack \u0026amp; slack project built under a pretty short timeline (~month). After completing it, the project went through a second round of refactoring to break up the main.cpp file which stored almost all of the codebase. During this refactoring, improvements were pulled in, or code from external projects was adopted to improve the overall codebase.\nDue to the difficulty of building the project, I recently recorded video of the project to go along with the screenshots.\nAcknowledgements I would like to take a moment to acknowledge worked that has been included or incorporated into the refactored version of Blockycraft:\n Perlin Noise - Replaced previous implementation with a modified version of sol-prog\u0026rsquo;s improved perlin noise implementation fogleman/Craft - Improved rendering mechanism and faster means of performing lookups of Chunk data (originally implemented was significantly slower) CS488 Course Assets - The project is built within the course assets of CS488. This has not changed in the refactoring  Kenney.nl The project icon is retrieved from kenney.nl. The original source material has been altered for the purposes of the project. The icon is used under the terms of the CC0 1.0 Universal.\nThe project uses assets by Kenney from kenney.nl/, and the icon is built from these assets.\n","id":58,"section":"posts","summary":"Blockycraft is a interactive graphics demo to create a Minecraft inspired demo which revolves around breaking and placing blocks. The game world is composed of rough cubes arranged in a fixed grid pattern and representing different materials, such as dirt, stone, and snow. The techniques used in the demo can be toggled using keyboard commands. The Blockycraft project is written using C++ and OpenGL. \u003cimg src=\"./docs/screenshots/world.png\" alt=\"blockycraft world\" title=\"Blockycraft\"\u003e","tags":["blockycraft"],"title":"blockycraft-classic","uri":"/2020/01/blockycraft-blockycraft-classic/","year":"2020"},{"content":"   Identify a canine breed from an image.     Classification of Dogs My implementation of the Convolutional Neural Networks (CNN) algorithm for identifying a canines breed from an image. Additionally, it supply the resembled dog breed if provided an image of a human.\nClassification You can see an example classification for the German Shepherd picture below:\nAnd an example of a misclassification for a rotweiler.\nNotes You can see my full analysis of the classifier in the notebook, but a snippet is included below\nThe output result is about where I expected it to be. ResNet50 is very good with large image data, and I provided minimal layers to the algorithm, I expected a good base performance. However the model has only achieved 81% score, which would not work very well in a production environment (app or SaaS).\nPotential improvements that are available for this model are:\n Reduce overfitting with usages of dropout and batch_normalization layers Add batch_normalization to reduce covariate shift in the calculation process Change the optimizer to another type, to find a better optimizer fit for my problem (adagrad or adam)  None of the above are guaranteed to produce a better result, just potential directions that I could pursue to improve the performance of the machine.\nProject This project was submitted by Jonathan Beverly as part of the Nanodegree At Udacity. The source was originally pulled from https://github.com/udacity/dog-project.git.\n","id":59,"section":"posts","summary":"My implementation of the Convolutional Neural Networks (CNN) algorithm for identifying a canines breed from an image. Additionally, it supply the resembled dog breed if provided an image of a human.","tags":["jrbeverly"],"title":"dog-project","uri":"/2020/01/jrbeverly-dog-project/","year":"2020"},{"content":"   A quadcopter flying agent that learns to take off using reinforcement learning.     Quadcopter using Reinforcement Learning My implementation of the DDPG reinforcement learning algorithm to solve the problem of a quadcopter taking flight.\nI have included a reference to the DDPG paper used in the development of the flying agent:\n Continuous control with deep reinforcement learning Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra\nWe adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.\n Flight Path You can see the best take-off that was produced from my reward function and DDPG implementation below:\nAlthough I am partial to this series of flight paths that occurred during the development phase of this project.\nNotes You can see my full analysis of my quadcopter in the notebook, but a snippet is included below\nI went with take-off in the end, although I did try landing at first. I was having a lot of difficulty getting started with landing, so I switched over to take-off. I think one of the issues with my landing approach was that my reward function was a distance based one, which meant there was no attempt to setup an \u0026lsquo;approach\u0026rsquo;.\nI went through a couple of iterations with my reward function, each time implementing a new factor. I split the function into the computation of two components: reward and penalty. I wanted to incentivize the agent into certain behaviours, while discouraging results that were not optimal. I describe it this way, because I wished to reflect these ideas:\n If you are stable, you have a lower penalty. If you are reckless (flying sideways), you have a higher penalty. The closer you are, the lower the penalty. The further you are, the higher the penalty. You are rewarded for proximity (cumulative) You are rewarded for still flying  These ideas didn\u0026rsquo;t translate well in the end, and in the end only a weighted distance was used for computing the reward. You can see that reward=0 =\u0026gt; abs(0-penalty) = penalty for the function. This means that most of the code in the get_reward function is not actually needed. The best decision was to ensure that the reward function was normalized, as that made the simulation a lot more consistent between runs.\n","id":60,"section":"posts","summary":"My implementation of the DDPG reinforcement learning algorithm to solve the problem of a quadcopter taking flight. I have included a reference to the DDPG paper used in the development of the flying agent: \u0026gt; Continuous control with deep reinforcement learning \u0026gt; Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra \u0026gt; \u0026gt; We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.","tags":["jrbeverly"],"title":"quadcopter","uri":"/2020/01/jrbeverly-quadcopter/","year":"2020"},{"content":"   IAM-less AWS API access for humans.     BMX BMX grants you API access to your AWS accounts, based on Okta credentials that you already own.\nIt uses your Okta identity to create short-term AWS STS tokens, as an alternative to long-term IAM access keys. BMX manages your STS tokens with the following commands:\n bmx print writes your short-term tokens to stdout as AWS environment variables. You can execute bmx print\u0026rsquo;s output to make the environment variables available to your shell. bmx write writes your short-term tokens to ~/.aws/credentials.  BMX prints detailed usage information when you run bmx -h or bmx \u0026lt;cmd\u0026gt; -h.\nBMX was developed by D2L (Brightspace/bmx), and modifications have been made to the project by Arctic Wolf.\nFeatures  BMX is multi-platform: it runs on Linux, Windows, and Mac. BMX maintains your Okta session for 12 hours: you enter your Okta password once a day, and BMX takes care of the rest. Project scoped configurations BMX supports Web and SMS MFA.  Installation Available versions of BMX are available on the releases page.\nGetting Started To authenticate and obtain a session via the command line, run the following:\nbmx login This will prompt you for your Okta organization and credentials. When you have successfully connected, you can run the following to get a set of IAM STS credentials for use with the AWS API:\nbmx print The command will print a series of environment set commands, that can be used to set the environment variables of the current shell session:\nexport AWS_SESSION_TOKEN=... export AWS_ACCESS_KEY_ID=... export AWS_SECRET_ACCESS_KEY=... # Run AWSCLI using environment variables for credentials aws sts get-caller-identity If you\u0026rsquo;d like to learn about the ways BMX assists with authenticating to AWS, you can review in the getting started documentation.\nVersioning BMX is maintained under the Semantic Versioning guidelines.\nGetting Involved See CONTRIBUTING.md for guidelines.\n","id":61,"section":"posts","summary":"BMX grants you API access to your AWS accounts, based on Okta credentials that you already own. It uses your Okta identity to create short-term AWS STS tokens, as an alternative to long-term IAM access keys. BMX manages your STS tokens with the following commands: 1. \u003ccode\u003ebmx print\u003c/code\u003e writes your short-term tokens to \u003ccode\u003estdout\u003c/code\u003e as AWS environment variables. You can execute \u003ccode\u003ebmx print\u003c/code\u003e\u0026rsquo;s output to make the environment variables available to your shell. 1. \u003ccode\u003ebmx write\u003c/code\u003e writes your short-term tokens to \u003ccode\u003e~/.aws/credentials\u003c/code\u003e. BMX prints detailed usage information when you run \u003ccode\u003ebmx -h\u003c/code\u003e or \u003ccode\u003ebmx \u0026lt;cmd\u0026gt; -h\u003c/code\u003e. BMX was developed by D2L (\u003ca href=\"https://github.com/Brightspace/bmx/\"\u003eBrightspace/bmx\u003c/a\u003e), and modifications have been made to the project by Arctic Wolf.","tags":["jrbeverly"],"title":"bmx","uri":"/2020/01/jrbeverly-bmx/","year":"2020"},{"content":"   Adjust the time of a series of commits in a git repository     Git Timeline Allows bulk modification of the commit dates of a repository, changing the history of a repository.\nUsage # Creates the demo repository ./git-timeline.bash clone # Copies the demo repository to the working environment ./git-timeline.bash working # Exports the history of the git repository to files ./git-timeline.bash history You can then edit the dates of the three files emitted:\n FIRST - The first commit to the repository HISTORY - The commit history LATEST - The most recent commit to the repository  After you have done this, you can then run apply and show:\n./git-timeline.bash apply ./git-timeline.bash show If you are finding it difficult to get the right timelines (or just working with the scripts), you can use cycle to start fresh, and re-apply. This does not alter the modified time files.\n","id":62,"section":"posts","summary":"Allows bulk modification of the commit dates of a repository, changing the history of a repository.","tags":["jrbeverly"],"title":"git-timeline","uri":"/2020/01/jrbeverly-git-timeline/","year":"2020"},{"content":"   Prototype running of New-PSSession in an AWS PowerShell Lambda     AWS Lambda PowerShell Example A simple Lambda function written in PowerShell to validate if New-PSSession can be leveraged from an AWS Lambda.\nThe objective of this repository was to determine if it was possible to use New-PSSession from an AWS Lambda. This would allow for modification of services like Office365 using remote sessions from AWS without requiring an ECS/EC2 container. Without a custom lambda runtime (or some way of enabling WSMan), it would be difficult to do this with a vanilla lambda execution environment. The error encountered during runtime is included below:\nThis parameter set requires WSMan, and no supported WSMan client library was found. WSMan is either not installed or unavailable for this system These errors lead to the following github issues for PowerShell: PowerShell/5561 \u0026amp; PowerShell/11159.\nThe lambda function can be published by Publish-AWSPowerShellLambda -ScriptPath .\\RemoteSession.ps1 -Name RemoteSession -Region us-east-1.\n","id":63,"section":"posts","summary":"A simple Lambda function written in PowerShell to validate if \u003ccode\u003eNew-PSSession\u003c/code\u003e can be leveraged from an AWS Lambda.","tags":["jrbeverly"],"title":"aws-lambda-remote-session","uri":"/2020/01/jrbeverly-aws-lambda-remote-session/","year":"2020"},{"content":"   The source for the landing page of Friending     Friending Landing Page This repository contains the source for the webpage friending.jrbeverly.dev.\nThe webpage is built off the Hugo template by by themefisher used under CC by 3.0 - Alterations to theme have been made to the underlying template.\nThose changes are made available in the themes/vex directory.\n","id":64,"section":"posts","summary":"\u003cp\u003eThis repository contains the source for the webpage \u003ca href=\"https://friending.jrbeverly.dev/\"\u003efriending.jrbeverly.dev\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe webpage is built off the Hugo template by by \u003ca href=\"http://www.Themefisher.com\"\u003ethemefisher\u003c/a\u003e used under \u003ca href=\"https://creativecommons.org/licenses/by/3.0/\"\u003eCC by 3.0\u003c/a\u003e - Alterations to theme have been made to the underlying template.\u003c/p\u003e\n\u003cp\u003eThose changes are made available in the \u003ccode\u003ethemes/vex\u003c/code\u003e directory.\u003c/p\u003e\n","tags":["thefriending"],"title":"website","uri":"/2020/01/thefriending-website/","year":"2020"},{"content":"   SAM Application for a simple chat application based on WebSockets in API Gateway     AWS Chat App SAM Application for a simple chat application based on API Gateways new WebSocket API feature. This was originally developed as an experiment to see how viable running a chat-bot in a fully serverless environment, as opposed to just running on a container in ECS.\nThis repository is based on Announcing WebSocket APIs in Amazon API Gateway, with the cloudformation and lambdas from simple-websockets-chat-app.\nUsage A build-harness created with make is available for the repository. This harness simplifies the command necessary to build and deploy the project. You can see the available targets by running make help:\nUsage: make \u0026lt;target\u0026gt; help This help text. Deploy pack Package the yaml files as a single package Package an AWS SAM application. deploy Deploy an AWS SAM application. publish Re-package and deploy an AWS SAM application. Cloudformation describe Describes the deployed cloudformation stack. outputs Describes the outputs of the deployed cloudformation stack. Docker docker Runs AWS SAM in docker. Tests chat Login to one of the chat sessions. stop Halt the existing chat services. logs Dump the logs from the docker image. The build-harness does a bit more in this case, as I experimented with a couple different tools while working on this. Notably:\n AWS SAM for packaging and deploying the code cfpack.js for bundling multiple cloudformation templates into a single one docker-compose for testing the service with wscat awscli for describing and working with the deployed cloudformation stack  Deploy Using docker, you can build and deploy the sample using the following steps:\nmake docker make pack make package make deploy You can get the URL for the websocket by calling make outputs. This can then be provided to the docker-compose template in docker/.\nmake outputs echo \u0026#34;WSCAT_URI=wss://\u0026lt;id\u0026gt;.execute-api.\u0026lt;region\u0026gt;.amazonaws.com/\u0026lt;env\u0026gt;\u0026#34; \u0026gt; docker/.env make chat # wscat -c $WSCAT_URI # {\u0026#34;message\u0026#34;:\u0026#34;sendmessage\u0026#34;, \u0026#34;data\u0026#34;:\u0026#34;hello world\u0026#34;} Notes I built this while I was looking into serverless chat-ops. At the time I was tinkering with ideas about chat-ops like services that could be used to manage key rotation in external services. This lead to some experiments with Hubot and AWS Lambda. I thought this example would offer sufficient complexity for tinkering with the idea of a lambda-based application for working with different services. The ideas were as follows:\n If each service is its own lambda, permissions to service tokens (GitHub / CircleCI / TravisCI / Jenkins) can be locked down at the route level Each service token could have its own secret store, allowing granular access permissions Rotations would occur on set intervals, which meant the service didn\u0026rsquo;t need to remain running at all times Trigger actions (revocations / deletes / freezes) could be trigger by API Routes Authentication could potentially leverage existing AWS services (useful for a secure service)  Terraform I had originally intended to write this in terraform, however at time of development (\u0026amp; writing), Terraform does not have support for the new version of API Gateway (V2). I have included CloudFormation in Terraform prototype in the directory tests/ for future reference. Rather than trying to get this working, I looked for ways to setup a terraform-like experience in cloudformation.\nCfPack I used cfpack.js to split the cloudformation template into smaller templates. Although this added another dependency (\u0026amp; build step), I found it a lot easier to work with multiple smaller templates as opposed to one big one. I did some early experiments with nested stacks, but personally found it like juggling.\n cfpack is a small CLI tool that can help you to deal with huge CloudFormation templates by splitting it into multiple smaller templates. Using this tool you can also build sharable drop-in templates that you can share across your projects.\n ","id":65,"section":"posts","summary":"SAM Application for a simple chat application based on API Gateways new WebSocket API feature. This was originally developed as an experiment to see how viable running a chat-bot in a fully serverless environment, as opposed to just running on a container in ECS. This repository is based on \u003ca href=\"https://aws.amazon.com/blogs/compute/announcing-websocket-apis-in-amazon-api-gateway/\"\u003eAnnouncing WebSocket APIs in Amazon API Gateway\u003c/a\u003e, with the cloudformation and lambdas from \u003ca href=\"https://github.com/aws-samples/simple-websockets-chat-app\"\u003esimple-websockets-chat-app\u003c/a\u003e.","tags":["jrbeverly"],"title":"aws-chat-app","uri":"/2019/12/jrbeverly-aws-chat-app/","year":"2019"},{"content":"   Hubot in AWS on ECS Fargate     Hubot in AWS ECS Hubot deployment in AWS using AWS ECS Fargate. This was prototyped out while I was evaluating ChatOps strategies that could be used to wrap existing web interfaces or require minimal overhead.\nUsage A build-harness created with make is available for the repository. This harness simplifies the commands necessary to build and deploy the project. You can see the available targets by running make help:\nUsage: make \u0026lt;target\u0026gt; help This help text. Terraform ecr Deploy the ECR for the Hubot image ecs Deploy the Hubot ECS service destroy Destroys the terraform instructure Hubot launch Launches the hubot application in docker Docker docker Build the hubot image for deployment deploy Deploys the hubot image to ECR Deploy Using docker, you can build and deploy the sample using the following steps:\nmake docker make ecr make deploy make ecs make launch Notes At the time I was building this I was trying to figure out a nice way to have a ChatOps style interface for a web service. The reason for ChatOps is that I was after something that would:\n Allow me to execute commands easily while on the go (e.g. slack + phone) Offer multiple operations for diagnostic and configuration Require minimal knowledge of how the system might work (e.g. \u0026lt;tool\u0026gt; help)  I do enjoy what Hubot offers but it did not exactly fit the target needs I was going for:\n I didn\u0026rsquo;t need the more advanced input format Slash commands were sufficient for the interface I didn\u0026rsquo;t wish to develop the app within the context of the Hubot application Additional infrastructure (ECS Service) was worrying, as I was aiming for high reliability  ","id":66,"section":"posts","summary":"Hubot deployment in AWS using AWS ECS Fargate. This was prototyped out while I was evaluating ChatOps strategies that could be used to wrap existing web interfaces or require minimal overhead.","tags":["jrbeverly"],"title":"hubot-in-aws","uri":"/2019/12/jrbeverly-hubot-in-aws/","year":"2019"},{"content":"   Building the AWSCLI Docker image in Bazel     Docker AWSCLI Built with Bazel  The AWS Command Line Interface (CLI) is a unified tool to manage your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts.\n CardboardCI aims to create a collection of docker images that can be used in continuous integration. These images will have all dependencies pinned, to ensure that any commit will produce the exact same image (or as close to as possible). The aim of this repository is to build docker-awscli in Bazel, to evaluate whether it would help fit those goals.\nAdditionally it helps to test whether Bazel could work under GitHub Actions, or would require another service to build.\nNotes If I were to use this for CardboardCI, I\u0026rsquo;d probably need to develop/use packer manager rules for the following:\n GitHub Releases NPM RubyGems AptGet  And potentially some edge cases with LaTeX and Lua (luarocks).\nPackage Manager Rules For the package managers that would need rules, they would follow the format of the existing download_pkgs (f(deps[]) =\u0026gt; tarball). Most of the package managers used in CardboardCI have some way to download but not install the packages. Here are some examples of the rule usages:\ndownload_npm( name = \u0026#34;pkgs\u0026#34;, image_tar = \u0026#34;@ubuntu//image\u0026#34;, packages = [ \u0026#34;surge:0.0.1\u0026#34;, ], # Could we read all of the packages from a file? # packages_file = \u0026#34;:file\u0026#34; ) If the rule could support a way of providing a file, then the same automated process as now could be used for upgrading the dependencies. The big problem I see with this is that if only 1 dependency changes, we must re-download all of the other dependencies. The option of bazel-ifying each dependency as a rule would create problems with updating (e.g. how to update version). This isn\u0026rsquo;t too bad if there exists a file like so:\nload(\u0026#34;@io_bazel_rules_docker//docker/package_managers:download_npm.bzl\u0026#34;, \u0026#34;download_npm\u0026#34;) download_npm( name = \u0026#34;pkg_surge\u0026#34;, image_tar = \u0026#34;@ubuntu//image\u0026#34;, src = \u0026#34;:surge.dep\u0026#34; ) With the .dep file looking something like this:\nname=\u0026#34;surge\u0026#34; version=\u0026#34;0.0.1\u0026#34; Conclusions Below I have included jot notes for my conclusions on using Bazel to build docker images for CardboardCI:\n Most of the images can be built using Bazel Some images may require bazel-ifying the source projects Windows support is lacking, and the resulting errors are difficult to investigate The errors are sometimes opaque (e.g. /tmp/installer OCI runtime error) Bazel rules would be needed for other package managers (npm, rubygems, luarocks, etc) container_run_and_commit allows for half-in half-out development Having each image defined with bazel could have some benefits if it was all-in  I don\u0026rsquo;t feel that there is a compelling reason to try and implement the docker images in Bazel, and more than enough reasons why it isn\u0026rsquo;t worthwhile at the moment.\n","id":67,"section":"posts","summary":"\u003cblockquote\u003e\n\u003cp\u003eThe AWS Command Line Interface (CLI) is a unified tool to manage your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts. CardboardCI aims to create a collection of docker images that can be used in continuous integration. These images will have all dependencies pinned, to ensure that any commit will produce the exact same image (or as close to as possible). The aim of this repository is to build \u003ca href=\"https://github.com/cardboardci/docker-awscli\"\u003edocker-awscli\u003c/a\u003e in Bazel, to evaluate whether it would help fit those goals. Additionally it helps to test whether Bazel could work under GitHub Actions, or would require another service to build.\u003c/p\u003e\n\u003c/blockquote\u003e\n","tags":["cardboardci"],"title":"bazel-docker-awscli","uri":"/2019/11/cardboardci-bazel-docker-awscli/","year":"2019"},{"content":"   Experimenting with using GitHub to host a powershell library     Powershell Library on GitHub Summary A powershell library that is installed from GitHub, rather than from Powershellgallery.\nUsage Downloading from GitHub:\n# Enable installing from github Install-Module -Name InstallModuleFromGitHub # Install the module Install-ModuleFromGitHub -GitHubRepo jrbeverly/pwsh-from-github # Perform Actions Write-Hello -Name \u0026#34;World\u0026#34; Write-World -Message \u0026#34;LFG\u0026#34; Notes Experimenting with using InstallModuleFromGitHub, instead of using Powershell gallery.\nI have noticed that this requires all of the scripts be at the root of the repository, rather than using a folder structure like so:\n\u0026gt; lib/ \u0026gt; Namespace1/ \u0026gt; Something1-1.ps1 \u0026gt; Something1-2.ps1 \u0026gt; Namespace2/ \u0026gt; Something2-2.ps1 \u0026gt; Main.ps1 ","id":68,"section":"posts","summary":"A powershell library that is installed from GitHub, rather than from Powershellgallery.","tags":["jrbeverly"],"title":"pwsh-from-github","uri":"/2019/11/jrbeverly-pwsh-from-github/","year":"2019"},{"content":"   Describes IAM resources for delegating access to external services.     AWS IAM External Role Terraform module for a continuous integration user-role pairing.\nThese types of resources are supported:\n IAM Role  Usage module \u0026#34;cicd_setup\u0026#34; { source = \u0026#34;git::https://gitlab.com/infraprints/modules/aws/iam-ci-role\u0026#34; username = \u0026#34;infraprints-iam-ci-role-basic\u0026#34; role_name = \u0026#34;infraprints-iam-ci-role-basic\u0026#34; environment_variable = { s3_bucket = \u0026#34;infraprints-bucket-example\u0026#34; hello_world = \u0026#34;hello world\u0026#34; } } Examples  Basic Example  Notes  Environment variables are prefixed with ENV_ to prevent them  Inputs    Name Description Type Default Required     environment_variable Times map \u0026lt;map\u0026gt; no   labels  map \u0026lt;map\u0026gt; no   length The length of the external id desired. string \u0026quot;16\u0026quot; no   path  string \u0026quot;ci\u0026quot; no   period  string \u0026quot;32400\u0026quot; no   role_name The name of the role. string n/a yes   service  string \u0026quot;GitLab\u0026quot; no   tags Key-value mapping of tags for the IAM role. map \u0026lt;map\u0026gt; no   username The name of the user. string n/a yes    Outputs    Name Description     arn The Amazon Resource Name (ARN) specifying the role.   create_date The Amazon Resource Name (ARN) specifying the role.   unique_id The Amazon Resource Name (ARN) specifying the role.    ","id":69,"section":"posts","summary":"Terraform module for a continuous integration user-role pairing. These types of resources are supported: - \u003ca href=\"https://www.terraform.io/docs/providers/aws/r/iam_role.html\"\u003eIAM Role\u003c/a\u003e","tags":["infraprints"],"title":"terraform-aws-iam-ci-role","uri":"/2019/11/infraprints-terraform-aws-iam-ci-role/","year":"2019"},{"content":"   Describes an S3 Object representing a terraform outputs file.     AWS S3 Terraform State Output Terraform module which creates an S3 Object containing terraform outputs.\nThese types of resources are supported:\n S3 Bucket Object Template  Usage locals { topics = [\u0026#34;aws\u0026#34;, \u0026#34;s3\u0026#34;, \u0026#34;terraform\u0026#34;] tags = { name = \u0026#34;infraprints\u0026#34;, description = \u0026#34;Infrastructure as Code.\u0026#34; } } module \u0026#34;output_resources\u0026#34; { source = \u0026#34;git::https://gitlab.com/infraprints/modules/aws/terraform-state-output\u0026#34; bucket = \u0026#34;infraprints-terraform-state-output\u0026#34; key = \u0026#34;aws/infraprints/project/outputs.tf\u0026#34; terraform_output = [ { key = \u0026#34;aws_account_id\u0026#34; value = \u0026#34;123412341234\u0026#34; }, { key = \u0026#34;topics\u0026#34; value = jsonencode(local.topics) }, { key = \u0026#34;tags\u0026#34; value = jsonencode(local.tags) }, ] } With using the output of this module here:\nmodule \u0026#34;example\u0026#34; { source = \u0026#34;s3::https://s3.amazonaws.com/infraprints-terraform-state-output/aws/infraprints/project\u0026#34; } output \u0026#34;aws_account_id\u0026#34; { value = module.example.aws_account_id } output \u0026#34;topics\u0026#34; { value = module.example.topics } output \u0026#34;tags\u0026#34; { value = module.example.tags } Examples  Basic Example Arrays Example Maps Example  Notes  S3 Versioning should be enabled  Inputs    Name Description Type Default Required     bucket The name of the bucket to put the file in. string n/a yes   key The name of the object once it is in the bucket. Should end with the .tf file extension. string n/a yes   tags A mapping of tags to assign to the object. map \u0026lt;map\u0026gt; no   terraform_output A set of terraform outputs to make available. list n/a yes    Outputs    Name Description     etag The ETag generated for the object (an MD5 sum of the object content). For plaintext objects or objects encrypted with an AWS-managed key, the hash is an MD5 digest of the object data. For objects encrypted with a KMS key or objects created by either the Multipart Upload or Part Copy operation, the hash is not an MD5 digest, regardless of the method of encryption. More information on possible values can be found on Common Response Headers.   id The key of the resource supplied above.   rendered The final rendered template.   version_id A unique version ID value for the object, if bucket versioning is enabled.    ","id":70,"section":"posts","summary":"Terraform module which creates an S3 Object containing terraform outputs. These types of resources are supported: bazel BUILD.bazel docs icon outputs README.md scripts srv WORKSPACE \u003ca href=\"https://www.terraform.io/docs/providers/aws/r/s3_bucket_object.html\"\u003eS3 Bucket Object\u003c/a\u003e bazel BUILD.bazel docs icon outputs README.md scripts srv WORKSPACE \u003ca href=\"https://www.terraform.io/docs/providers/template/d/file.html\"\u003eTemplate\u003c/a\u003e","tags":["infraprints"],"title":"terraform-aws-terraform-state-output","uri":"/2019/11/infraprints-terraform-aws-terraform-state-output/","year":"2019"},{"content":"   Represents a top level domain that uses external hosted zones for DNS management.     terraform-aws-acm-certificate ","id":71,"section":"posts","summary":"   Represents a top level domain that uses external hosted zones for DNS management.     terraform-aws-acm-certificate ","tags":["infraprints"],"title":"terraform-aws-route53-subdomains","uri":"/2019/11/infraprints-terraform-aws-route53-subdomains/","year":"2019"},{"content":"   Provision storage and locking mechanisms for Terraform.     AWS Terraform Remote State with Lock Table Terraform module which creates a terraform remote state storage in S3 with a DynamoDB lock table.\nThese types of resources are supported:\n S3 Bucket DynamoDB Table  Usage module \u0026#34;remote_state\u0026#34; { source = \u0026#34;git::https://gitlab.com/infraprints/modules/aws/terraform-remote-state\u0026#34; bucket = \u0026#34;infraprints-terraform-remote-state\u0026#34; table = \u0026#34;infraprints-terraform-lock-table\u0026#34; } resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;terraform\u0026#34; { name = \u0026#34;infraprints-terraform\u0026#34; assume_role_policy = \u0026lt;\u0026lt;EOF { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;ec2.amazonaws.com\u0026#34; }, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Sid\u0026#34;: \u0026#34;\u0026#34; } ] } EOF } resource \u0026#34;aws_iam_role_policy\u0026#34; \u0026#34;terraform_policy\u0026#34; { name = \u0026#34;TerraformRemoteState\u0026#34; role = aws_iam_role.terraform.id policy = module.remote_state.write_policy } Examples  Basic Example  Notes  Only SSL access is permitted to the remote state storage Only encrypted objects can be uploaded to the remote state storage The lock table is not encrypted  Inputs Placeholder.\nOutputs Placeholder.\n","id":72,"section":"posts","summary":"Terraform module which creates a terraform remote state storage in S3 with a DynamoDB lock table. These types of resources are supported: bazel BUILD.bazel docs icon outputs README.md scripts srv WORKSPACE \u003ca href=\"https://www.terraform.io/docs/providers/aws/r/s3_bucket.html\"\u003eS3 Bucket\u003c/a\u003e bazel BUILD.bazel docs icon outputs README.md scripts srv WORKSPACE \u003ca href=\"https://www.terraform.io/docs/providers/aws/r/dynamodb_table.html\"\u003eDynamoDB Table\u003c/a\u003e","tags":["infraprints"],"title":"terraform-aws-terraform-remote-state","uri":"/2019/11/infraprints-terraform-aws-terraform-remote-state/","year":"2019"},{"content":"   Provision a bucket for storing build artifacts in S3.     AWS S3 Terraform State Output Terraform module for an tiered storage S3 bucket with eventual object expiration. Primary use key is for a build artifacts storage.\nThese types of resources are supported:\n S3 Bucket  Usage module \u0026#34;build_artifacts\u0026#34; { source = \u0026#34;git::https://gitlab.com/infraprints/modules/aws/s3-artifacts\u0026#34; bucket = \u0026#34;infraprints-s3-artifacts\u0026#34; standard_transition_days = 10 glacier_transition_days = 30 expiration_days = 365 tags = { Longevity = \u0026#34;Yearly\u0026#34; Expiration = \u0026#34;True\u0026#34; } } Examples  Basic Example Adjusted Example  Notes  With the default configuration, all objects in the S3 bucket will expire in 90 days. The S3 bucket uses tiered storage with eventual expiration. This bucket is not designed for long term persistence.  Inputs    Name Description Type Default Required     bucket The name of the bucket. string n/a yes   expiration_days Number of days until objects are expunged. string \u0026quot;90\u0026quot; no   force_destroy A boolean that indicates all objects should be deleted from the bucket so that the bucket can be destroyed without error. These objects are not recoverable. string \u0026quot;false\u0026quot; no   glacier_transition_days Number of days until objects are transitioned to the GLACIER storage class. string \u0026quot;60\u0026quot; no   region If specified, the AWS region this bucket should reside in. Otherwise, the region used by the callee. string \u0026quot;\u0026quot; no   standard_transition_days Number of days until objects are transitioned to the STANDARD_IA storage class. string \u0026quot;30\u0026quot; no   tags A mapping of tags to assign to the bucket. map \u0026lt;map\u0026gt; no    Outputs    Name Description     arn The ARN of the bucket. Will be of format arn:aws:s3:::bucketname.   bucket The name of the bucket.   bucket_domain_name The bucket domain name. Will be of format bucketname.s3.amazonaws.com.   id The name of the bucket.   region The AWS region this bucket resides in.    ","id":73,"section":"posts","summary":"Terraform module for an tiered storage S3 bucket with eventual object expiration. Primary use key is for a build artifacts storage. These types of resources are supported: - \u003ca href=\"https://www.terraform.io/docs/providers/aws/r/s3_bucket.html\"\u003eS3 Bucket\u003c/a\u003e","tags":["infraprints"],"title":"terraform-aws-s3-artifacts","uri":"/2019/11/infraprints-terraform-aws-s3-artifacts/","year":"2019"},{"content":"   Provision a deployment pipeline for executing Terraform scripts.     terraform-aws-acm-certificate ","id":74,"section":"posts","summary":"   Provision a deployment pipeline for executing Terraform scripts.     terraform-aws-acm-certificate ","tags":["infraprints"],"title":"terraform-aws-codepipeline-terraform","uri":"/2019/11/infraprints-terraform-aws-codepipeline-terraform/","year":"2019"},{"content":"   Provision a certificate for a domain, verified through DNS.     AWS ACM DNS Validated Certificate Terraform module for provisioning a DNS validated certificate, along with the required validation records. The module will wait for validation to complete.\nThese types of resources are supported:\n ACM Certificate ACM Certificate Validation  Usage module \u0026#34;certificate\u0026#34; { source = \u0026#34;git::https://gitlab.com/infraprints/modules/aws/acm-certificate\u0026#34; zone_id = \u0026#34;${data.aws_route53_zone.zone.id}\u0026#34; domain_name = \u0026#34;infraprints.io\u0026#34; subject_alternative_names = [ \u0026#34;api.infraprints.io\u0026#34;, \u0026#34;dev.infraprints.io\u0026#34;, ] } data \u0026#34;aws_route53_zone\u0026#34; \u0026#34;zone\u0026#34; { name = \u0026#34;infraprints.io\u0026#34; } Examples  Basic Example Multiple Records Example  Notes  The module deploys the required validation records and wait for validation to complete, which can take upwards to 30 minutes.  Inputs    Name Description Type Default Required     domain_name A domain name for which the certificate should be issued string n/a yes   subject_alternative_names A list of domains that should be SANs in the issued certificate list \u0026lt;list\u0026gt; no   ttl The TTL of the validation record(s). string \u0026quot;60\u0026quot; no   zone_id The ID of the hosted zone to contain the validation record(s). string n/a yes    Outputs    Name Description     arn The ARN of the certificate that is being validated.   domain_name The domain name for which the certificate is issued.   fqdn FQDN built using the zone domain and name.   id The ARN of the certificate.   validation_record_fqdns List of FQDNs that implement the validation.    ","id":75,"section":"posts","summary":"Terraform module for provisioning a DNS validated certificate, along with the required validation records. The module will wait for validation to complete. These types of resources are supported: - \u003ca href=\"https://www.terraform.io/docs/providers/aws/r/acm_certificate.html\"\u003eACM Certificate\u003c/a\u003e - \u003ca href=\"https://www.terraform.io/docs/providers/aws/r/acm_certificate_validation.html\"\u003eACM Certificate Validation\u003c/a\u003e","tags":["infraprints"],"title":"terraform-aws-acm-certificate","uri":"/2019/11/infraprints-terraform-aws-acm-certificate/","year":"2019"},{"content":"   Describes an IAM role for delegating cross-account access.     AWS IAM External Role Terraform module for describing an IAM role responsible for delegating cross-account access.\nThese types of resources are supported:\n IAM Role  Usage module \u0026#34;example\u0026#34; { source = \u0026#34;git::https://gitlab.com/infraprints/modules/aws/iam-external-role\u0026#34; name = \u0026#34;infraprints-iam-external-role\u0026#34; external_id = \u0026#34;TXAiS9rfgQghzWW2\u0026#34; role_arn = [\u0026#34;${aws_iam_role.default.arn}\u0026#34;] count = \u0026#34;1\u0026#34; } resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;default\u0026#34; { name = \u0026#34;infraprints-ec2-role\u0026#34; assume_role_policy = \u0026lt;\u0026lt;EOF { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;ec2.amazonaws.com\u0026#34; }, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, } ] } EOF } Examples  Basic Example ExternalID Example Multiple roles Example  Notes  The count property is required as a constant as a workaround to a Terraform issue.  Inputs    Name Description Type Default Required     count The number of principal entities. string n/a yes   description The description of the role. string \u0026quot;\u0026quot; no   external_id External Identifier set on the role. string \u0026quot;\u0026quot; no   force_detach_policies Specifies to force detaching any policies the role has before destroying it. Defaults to false. string \u0026quot;true\u0026quot; no   max_session_duration The maximum session duration (in seconds) that you want to set for the specified role. If you do not specify a value for this setting, the default maximum of one hour is applied. This setting can have a value from 1 hour to 12 hours. string \u0026quot;3600\u0026quot; no   name The name of the role. string n/a yes   path The path to the role. See IAM Identifiers for more information. string \u0026quot;/external/\u0026quot; no   permissions_boundary The ARN of the policy that is used to set the permissions boundary for the role. string \u0026quot;\u0026quot; no   role_arn The list of principal entities that is allowed to assume the role. list n/a yes   tags Key-value mapping of tags for the IAM role. map \u0026lt;map\u0026gt; no    Outputs    Name Description     arn The Amazon Resource Name (ARN) specifying the role.   create_date The Amazon Resource Name (ARN) specifying the role.   unique_id The Amazon Resource Name (ARN) specifying the role.    ","id":76,"section":"posts","summary":"Terraform module for describing an IAM role responsible for delegating cross-account access. These types of resources are supported: - \u003ca href=\"https://www.terraform.io/docs/providers/aws/r/iam_role.html\"\u003eIAM Role\u003c/a\u003e","tags":["infraprints"],"title":"terraform-aws-iam-external-role","uri":"/2019/11/infraprints-terraform-aws-iam-external-role/","year":"2019"},{"content":"   README for the Infraprints Community     ","id":77,"section":"posts","summary":"   README for the Infraprints Community     ","tags":["infraprints"],"title":"readme","uri":"/2019/11/infraprints-readme/","year":"2019"},{"content":"   [WIP]     Terraform Resource ID Construct a formatted name for a Terraform resource.\n","id":78,"section":"posts","summary":"Construct a formatted name for a Terraform resource.","tags":["infraprints"],"title":"terraform-id","uri":"/2019/11/infraprints-terraform-id/","year":"2019"},{"content":"   Configures the Gitlab CI environment variables for a Netlify deploy.     Terraform Netlify Gitlab CI/CD Terraform module which creates a site on Netlify with the necessary variables for GitLab CI deployments.\nThese types of resources are supported:\n Netlify Site  Usage module \u0026#34;cloudability\u0026#34; { source = \u0026#34;git::https://gitlab.com/infraprints/modules/netlify/gitlab-cicd.git?ref=master\u0026#34; name = \u0026#34;my-netlify-website\u0026#34; custom_domain = \u0026#34;www.example.com\u0026#34; project = \u0026#34;01234567\u0026#34; } Examples  Basic Example  Notes  Three GitLab CI Environment Variables (NETLIFY_SITE_ID, NETLIFY_NAME, NETLIFY_CUSTOM_DOMAIN)  Inputs    Name Description Type Default Required     name A friendly name for the netlify site. string - yes   custom_domain FQDN built using the zone domain and name. string - yes   project The integer that uniquely identifies the project within the gitlab install. string - yes    Outputs    Name Description     id The unique identifier.   name Name of your site on netlify.   custom_domain A custom domain name, must be configured using a cname in accordance with netlify\u0026rsquo;s docs.    ","id":79,"section":"posts","summary":"Terraform module which creates a site on Netlify with the necessary variables for GitLab CI deployments. These types of resources are supported: bazel BUILD.bazel docs icon outputs README.md scripts srv WORKSPACE \u003ca href=\"https://www.terraform.io/docs/providers/netlify/r/netlify_site.html\"\u003eNetlify Site\u003c/a\u003e","tags":["infraprints"],"title":"terraform-gitlab-netlify-cicd","uri":"/2019/11/infraprints-terraform-gitlab-netlify-cicd/","year":"2019"},{"content":"   The website for Infraprints     Website The work-in-progress stub for the infraprints website.\n","id":80,"section":"posts","summary":"The work-in-progress stub for the infraprints website.","tags":["infraprints"],"title":"website","uri":"/2019/11/infraprints-website/","year":"2019"},{"content":"   Index of all proposals and design documents     Proposals Experimenting with the underlying infrastructure for a GitHub based proposals mechanisms that deploys to a web resource (website/subpage/etc)\nMotivations  Automation to ensure contribution guidelines are both accessible and followed (e.g. first contribution bot, linting) Contribution previews, either by links to markdown rendering or branch previews Support for supplemental resources (e.g. assets/list-of-items.csv) Methods to organize the proposals better (date? slugs? IDs?) Simple layout in code (minimal overhead), with tooling to move assets into the correct locations When working with code, the proposals have some organization mechanism Minimal barriers when trying to write a proposal  Notes Using date-based directory organization helps for filtering based on the latest proposals. If you are looking for the proposals in the most recent year, its fairly easy to complete that query (ls designs/2019/). I find that this falls apart when navigating the history for proposals related to a project. I could enforce directory naming standards, but I feel that makes the repository difficult to approach. If I want all the proposals related to a certain project, I\u0026rsquo;d need to use tools like jq or grep.\nA model that involves namespaces paired with status directories (archived/etc) would help resolve these issues. My concern with this though is the solution becomes over-engineered and closely connected to the concept of directories. I think the \u0026lsquo;Namespaces and Status\u0026rsquo; example really shows how it can become difficult to work with when sticking with tree structure for organization.\nSince each proposal is a self-contained package with metadata, the proposals can be moved freely about the file system. From this, I\u0026rsquo;d say that the best solution is to just use namespace slugs for directory names. Each proposal will have metadata in the form of the DESIGN schema, so the post-processing stage can organize it by that.\nThis repository does a simple example using mkdocs to create a website hosting the proposals. The proposals are put in hardcoded locations to avoid the need for a post-processing step.\nNamespaces and Status \u0026gt; designs/ \u0026gt; project1/ \u0026gt; sub1/ \u0026gt; remove-items-from/ \u0026gt; README.md \u0026gt; assets/ \u0026gt; diagram.png \u0026gt; establish-baseline/ \u0026gt; README.md \u0026gt; supplementary/ \u0026gt; NOTES.md \u0026gt; sub2/ \u0026gt; remove-items-from/ \u0026gt; README.md \u0026gt; project2/ \u0026gt; sandbox-components-by/ \u0026gt; README.md \u0026gt; replace-grpc-with/ \u0026gt; README.md \u0026gt; archived/ \u0026gt; 2019 \u0026gt; project1/ \u0026gt; establish-project1-for/ \u0026gt; README.md \u0026gt; 2018 \u0026gt; project2/ \u0026gt; establish-project2-for/ \u0026gt; README.md Directory scoping \u0026gt; xyz/ \u0026gt; README.md \u0026gt; DESIGN \u0026gt; assets/ \u0026gt; diagram.png \u0026gt; architecture.svg \u0026gt; supplmentary/ \u0026gt; NOTES.md \u0026gt; ESTIMATES.md ","id":81,"section":"posts","summary":"Experimenting with the underlying infrastructure for a GitHub based proposals mechanisms that deploys to a web resource (website/subpage/etc)","tags":["jrbeverly"],"title":"proposals-concept","uri":"/2019/10/jrbeverly-proposals-concept/","year":"2019"},{"content":"   Experimenting with CircleCI Orbs for running types of workflows     Experimenting with CircleCI Orbs Experimenting with CircleCI Orbs for reducing code re-use in templates\nUsages cp Deploy to S3\norbs: awscli: jrbeverly/awscli@0.0.5 version: 2.1 workflows: Deploy to S3: jobs: - awscli/aws-copy-to: bucket: hello-bucket namespace: some/terraform source: some/path sync Deploy to S3\norbs: awscli: jrbeverly/awscli@0.0.5 version: 2.1 workflows: Deploy to S3: jobs: - awscli/aws-sync-to: bucket: hello-bucket namespace: some/terraform ","id":82,"section":"posts","summary":"Experimenting with CircleCI Orbs for reducing code re-use in templates","tags":["jrbeverly"],"title":"exp-circleci-orbs","uri":"/2019/10/jrbeverly-exp-circleci-orbs/","year":"2019"},{"content":"   A Node.js style checker and lint tool for Markdown/CommonMark files.     Docker image for MarkdownLint A tool to check markdown files and flag style issues. To have markdownlint check your markdown files, simply run mdl with the filenames as a parameter:\nmdl README.md Markdownlint can also take a directory, and it will scan all markdown files within the directory (and nested directories):\nmdl docs/ You can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/markdownlint Test interactively docker run -it cardboardci/markdownlint /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/markdownlint aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/markdownlint aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/markdownlint:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/markdownlint:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/markdownlint:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/markdownlint:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":83,"section":"posts","summary":"A tool to check markdown files and flag style issues. To have markdownlint check your markdown files, simply run mdl with the filenames as a parameter: \u003ccode\u003ebash mdl README.md \u003c/code\u003e Markdownlint can also take a directory, and it will scan all markdown files within the directory (and nested directories): \u003ccode\u003ebash mdl docs/ \u003c/code\u003e You can see the cli reference \u003ca href=\"https://github.com/markdownlint/markdownlint\"\u003ehere\u003c/a\u003e.","tags":["cardboardci"],"title":"docker-markdownlint","uri":"/2019/10/cardboardci-docker-markdownlint/","year":"2019"},{"content":"   A base ubuntu image for CI images to work off.     CI Core CI-core is a special Docker image that is configured for running in CI environments. It is Ubuntu, with:\n A docker user A directory workspace Mechanisms for running build tools (json, web requests, etc)  ","id":84,"section":"posts","summary":"CI-core is a special Docker image that is configured for running in CI environments. It is Ubuntu, with: - A docker user - A directory workspace - Mechanisms for running build tools (json, web requests, etc)","tags":["cardboardci"],"title":"docker-ci-core","uri":"/2019/10/cardboardci-docker-ci-core/","year":"2019"},{"content":"   Static analysis of C/C++ code.     Docker image for CppCheck Cppcheck is an analysis tool for C/C++ code. It provides unique code analysis to detect bugs and focuses on detecting undefined behaviour and dangerous coding constructs. The goal is to detect only real errors in the code (i.e. have very few false positives).\nYou can see the source repository here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/awscli Test interactively docker run -it cardboardci/awscli /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/awscli aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/awscli aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/awscli:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/awscli:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/awscli:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/awscli:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":85,"section":"posts","summary":"Cppcheck is an analysis tool for C/C++ code. It provides unique code analysis to detect bugs and focuses on detecting undefined behaviour and dangerous coding constructs. The goal is to detect only real errors in the code (i.e. have very few false positives). You can see the source repository \u003ca href=\"https://github.com/danmar/cppcheck\"\u003ehere\u003c/a\u003e.","tags":["cardboardci"],"title":"docker-cppcheck","uri":"/2019/10/cardboardci-docker-cppcheck/","year":"2019"},{"content":"   A command line client for Dropbox built using the Go SDK.     Docker image for AWS CLI A command line client for Dropbox built using the Go SDK\n Supports basic file operations like ls, cp, mkdir, mv (via the Files API) Supports search Supports file revisions and file restore Chunked uploads for large files, paginated listing for large directories Supports a growing set of Team operations  You can see the source repository here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/awscli Test interactively docker run -it cardboardci/awscli /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/awscli aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/awscli aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/awscli:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/awscli:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/awscli:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/awscli:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":86,"section":"posts","summary":"A command line client for Dropbox built using the Go SDK bazel BUILD.bazel docs icon outputs README.md scripts srv WORKSPACE Supports basic file operations like ls, cp, mkdir, mv (via the Files API) bazel BUILD.bazel docs icon outputs README.md scripts srv WORKSPACE Supports search bazel BUILD.bazel docs icon outputs README.md scripts srv WORKSPACE Supports file revisions and file restore bazel BUILD.bazel docs icon outputs README.md scripts srv WORKSPACE Chunked uploads for large files, paginated listing for large directories bazel BUILD.bazel docs icon outputs README.md scripts srv WORKSPACE Supports a growing set of Team operations You can see the source repository \u003ca href=\"https://github.com/dropbox/dbxcli\"\u003ehere\u003c/a\u003e.","tags":["cardboardci"],"title":"docker-dbxcli","uri":"/2019/10/cardboardci-docker-dbxcli/","year":"2019"},{"content":"   The static code analysis tool you need for your HTML.     Docker image for HTMLHint The static code analysis tool you need for your HTML.\nYou can see the source repository here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/htmlhint Test interactively docker run -it cardboardci/htmlhint /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/htmlhint aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/htmlhint aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/htmlhint:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/htmlhint:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/htmlhint:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/htmlhint:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":87,"section":"posts","summary":"The static code analysis tool you need for your HTML. You can see the source repository \u003ca href=\"https://github.com/htmlhint/HTMLHint\"\u003ehere\u003c/a\u003e.","tags":["cardboardci"],"title":"docker-htmlhint","uri":"/2019/10/cardboardci-docker-htmlhint/","year":"2019"},{"content":"   Luacheck is a static analyzer and a linter for Lua.     Docker image for LuaCheck Luacheck is a static analyzer and a linter for Lua. Luacheck detects various issues such as usage of undefined global variables, unused variables and values, accessing uninitialized variables, unreachable code and more. Most aspects of checking are configurable: there are options for defining custom project-related globals, for selecting set of standard globals (version of Lua standard library), for filtering warnings by type and name of related variable, etc. The options can be used on the command line, put into a config or directly into checked files as Lua comments.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/luacheck Test interactively docker run -it cardboardci/luacheck /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/luacheck aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/luacheck aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/luacheck:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/luacheck:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/luacheck:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/luacheck:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":88,"section":"posts","summary":"Luacheck is a static analyzer and a linter for Lua. Luacheck detects various issues such as usage of undefined global variables, unused variables and values, accessing uninitialized variables, unreachable code and more. Most aspects of checking are configurable: there are options for defining custom project-related globals, for selecting set of standard globals (version of Lua standard library), for filtering warnings by type and name of related variable, etc. The options can be used on the command line, put into a config or directly into checked files as Lua comments. You can see the cli reference \u003ca href=\"https://github.com/mpeterv/luacheck\"\u003ehere\u003c/a\u003e.","tags":["cardboardci"],"title":"docker-luacheck","uri":"/2019/10/cardboardci-docker-luacheck/","year":"2019"},{"content":"   Pylint is a Python static code analysis tool which looks for programming errors.     Docker image for PyLint Pylint is a Python static code analysis tool which looks for programming errors, helps enforcing a coding standard, sniffs for code smells and offers simple refactoring suggestions.\nIt\u0026rsquo;s highly configurable, having special pragmas to control its errors and warnings from within your code, as well as from an extensive configuration file. It is also possible to write your own plugins for adding your own checks or for extending pylint in one way or another.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/pylint Test interactively docker run -it cardboardci/pylint /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/pylint aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/pylint aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/pylint:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/pylint:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/pylint:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/pylint:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":89,"section":"posts","summary":"Pylint is a Python static code analysis tool which looks for programming errors, helps enforcing a coding standard, sniffs for code smells and offers simple refactoring suggestions. It\u0026rsquo;s highly configurable, having special pragmas to control its errors and warnings from within your code, as well as from an extensive configuration file. It is also possible to write your own plugins for adding your own checks or for extending pylint in one way or another. You can see the cli reference \u003ca href=\"https://github.com/PyCQA/pylint/\"\u003ehere\u003c/a\u003e.","tags":["cardboardci"],"title":"docker-pylint","uri":"/2019/10/cardboardci-docker-pylint/","year":"2019"},{"content":"   Turn SVG files into raster images.     Docker image for Render SVGs A utility to render Scalable Vector Graphics (SVG), associated with the GNOME Project. It renders SVG files to Cairo surfaces. Cairo is the 2D, antialiased drawing library that GNOME uses to draw things to the screen or to generate output for printing.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/rsvg Test interactively docker run -it cardboardci/rsvg /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/rsvg aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/rsvg aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/rsvg:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/rsvg:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/rsvg:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/rsvg:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":90,"section":"posts","summary":"A utility to render Scalable Vector Graphics (SVG), associated with the GNOME Project. It renders SVG files to Cairo surfaces. Cairo is the 2D, antialiased drawing library that GNOME uses to draw things to the screen or to generate output for printing. You can see the cli reference \u003ca href=\"https://github.com/GNOME/librsvg\"\u003ehere\u003c/a\u003e.","tags":["cardboardci"],"title":"docker-rsvg","uri":"/2019/10/cardboardci-docker-rsvg/","year":"2019"},{"content":"   A Ruby static code analyzer and formatter, based on the community Ruby style guide.     Docker image for Rubocop RuboCop is a Ruby static code analyzer and code formatter. Out of the box it will enforce many of the guidelines outlined in the community Ruby Style Guide.\nRuboCop is extremely flexible and most aspects of its behavior can be tweaked via various configuration options.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/rubocop Test interactively docker run -it cardboardci/rubocop /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/rubocop aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/rubocop aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/rubocop:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/rubocop:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/rubocop:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/rubocop:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":91,"section":"posts","summary":"RuboCop is a Ruby static code analyzer and code formatter. Out of the box it will enforce many of the guidelines outlined in the community Ruby Style Guide. RuboCop is extremely flexible and most aspects of its behavior can be tweaked via various configuration options. You can see the cli reference \u003ca href=\"https://github.com/rubocop-hq/rubocop\"\u003ehere\u003c/a\u003e.","tags":["cardboardci"],"title":"docker-rubocop","uri":"/2019/10/cardboardci-docker-rubocop/","year":"2019"},{"content":"   A mighty, modern style linter.     Docker image for StyleLint A mighty, modern linter that helps you avoid errors and enforce conventions in your styles.\nIt\u0026rsquo;s mighty because it:\n understands the latest CSS syntax including custom properties and level 4 selectors extracts embedded styles from HTML, markdown and CSS-in-JS object \u0026amp; template literals parses CSS-like syntaxes like SCSS, Sass, Less and SugarSS has over 170 built-in rules to catch errors, apply limits and enforce stylistic conventions supports plugins so you can create your own rules or make use of plugins written by the community automatically fixes some violations (experimental feature) is well tested with over 10000 unit tests supports shareable configs that you can extend or create your own of is unopinionated so you can tailor the linter to your exact needs  You can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/stylelint Test interactively docker run -it cardboardci/stylelint /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/stylelint aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/stylelint aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/stylelint:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/stylelint:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/stylelint:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/stylelint:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":92,"section":"posts","summary":"A mighty, modern linter that helps you avoid errors and enforce conventions in your styles. It\u0026rsquo;s mighty because it: bazel BUILD.bazel docs icon outputs README.md scripts srv WORKSPACE understands the latest CSS syntax including custom properties and level 4 selectors bazel BUILD.bazel docs icon outputs README.md scripts srv WORKSPACE extracts embedded styles from HTML, markdown and CSS-in-JS object \u0026amp; template literals bazel BUILD.bazel docs icon outputs README.md scripts srv WORKSPACE parses CSS-like syntaxes like SCSS, Sass, Less and SugarSS bazel BUILD.bazel docs icon outputs README.md scripts srv WORKSPACE has over 170 built-in rules to catch errors, apply limits and enforce stylistic conventions bazel BUILD.bazel docs icon outputs README.md scripts srv WORKSPACE supports plugins so you can create your own rules or make use of plugins written by the community bazel BUILD.bazel docs icon outputs README.md scripts srv WORKSPACE automatically fixes some violations (experimental feature) bazel BUILD.bazel docs icon outputs README.md scripts srv WORKSPACE is well tested with over 10000 unit tests bazel BUILD.bazel docs icon outputs README.md scripts srv WORKSPACE supports shareable configs that you can extend or create your own of bazel BUILD.bazel docs icon outputs README.md scripts srv WORKSPACE is unopinionated so you can tailor the linter to your exact needs You can see the cli reference \u003ca href=\"https://github.com/stylelint/stylelint\"\u003ehere\u003c/a\u003e.","tags":["cardboardci"],"title":"docker-stylelint","uri":"/2019/10/cardboardci-docker-stylelint/","year":"2019"},{"content":"   README for the CardboardCI Community     CardboardCI A collection of docker images that provide a common core for use in continuous integration.\nThe idea of these images is to balance the following:\n Frequency of updates Standard set of tooling Common Environment  Background Notes I encountered a lot of difficult with trying to automate the updating of those docker images. GitLab does not really have the idea of bots. Instead it would require a personal access token for my account, if I wanted to make commits. I didn\u0026rsquo;t like, as that would grant access to pretty much every one of my repositories. At the time I was self-hosting Gitlab, so I opted to just create additional users that would be my bots.\nHaving a bot user worked pretty well, in the sense that a scheduled Gitlab repository would execute the updates. However, at that time the building of each image was a burden on my local self-hosted CI machine. When I opted to use Gitlab.com instead of self-hosted, I ended up abandoning the idea of automated upgrading.\nLater on when I was doing some more work in the frontend realm, I found that I didn\u0026rsquo;t like working with the available docker images. They fit into a couple categories:\n Very big Dependencies I did not like/want Slow update schedule  That is when I decided to move my docker images into a separate project called CardboardCI. The hope was a set of small images, with automated updates and a standard set of dependencies (jq/zip/tar/etc). These images were based on alpine, until ubuntu released minimal images (~35MB). At that time I started the process of moving them over to ubuntu.\nGitHub Actions When GitHub Actions became available, I noticed that you would be able to use the GITHUB_TOKEN to commit against the repository itself. This meant you could define a workflow that checked for updates, and if existed, create a pull request with those updates.\nIn essence, you would be able to roll your own dependabot. I thought this would be ideal for CardboardCI, and moved it from GitLab to GitHub.\n","id":93,"section":"posts","summary":"A collection of docker images that provide a common core for use in continuous integration. The idea of these images is to balance the following: - Frequency of updates - Standard set of tooling - Common Environment","tags":["cardboardci"],"title":"readme","uri":"/2019/10/cardboardci-readme/","year":"2019"},{"content":"   A collection of example usages of rules_csharp for development purposes     Bazel CSharp Rules Examples Overview This repository provides a set of usages for the bazel csharp rules. The idea behind these examples is to cover edge cases that are encountered during development, and provide a comprehensive test (\u0026amp; prototype) suite.\nWhile working on the bazel csharp rules, I have encountered bugs or small quirks that I would like to encode records of. Some of these are very minor details, so I felt it would work best to have them as an external repository.\nI defined the idea behind each program here.\nSetup If you\u0026rsquo;d like to test the rules in your own repository, you can add the following to your WORKSPACE file to add the external repositories:\nload(\u0026#34;@bazel_tools//tools/build_defs/repo:http.bzl\u0026#34;, \u0026#34;http_archive\u0026#34;) http_archive( name = \u0026#34;d2l_rules_csharp\u0026#34;, strip_prefix = \u0026#34;rules_csharp-0.6\u0026#34;, urls = [\u0026#34;https://github.com/Brightspace/rules_csharp/archive/v0.6.tar.gz\u0026#34;], ) load( \u0026#34;@d2l_rules_csharp//csharp:defs.bzl\u0026#34;, \u0026#34;csharp_register_toolchains\u0026#34;, \u0026#34;csharp_repositories\u0026#34;, ) csharp_repositories() csharp_register_toolchains() Or you can consult the minimal example in usage/.\n","id":94,"section":"posts","summary":"This repository provides a set of usages for the bazel csharp rules. The idea behind these examples is to cover edge cases that are encountered during development, and provide a comprehensive test (\u0026amp; prototype) suite. While working on the bazel csharp rules, I have encountered bugs or small quirks that I would like to encode records of. Some of these are very minor details, so I felt it would work best to have them as an external repository. I defined the idea behind each program \u003ca href=\"docs/criteria.md\"\u003ehere\u003c/a\u003e.","tags":["jrbeverly"],"title":"bazel-csharp-testcases","uri":"/2019/10/jrbeverly-bazel-csharp-testcases/","year":"2019"},{"content":"   Experimenting with GitHub Actions for building machine images with Packer     Packer with GitHub Actions Experimenting with GitHub Actions for building machine images with Packer. Ideally trying to figure out what it takes for building the following on GitHub Actions:\n VirtualBox ISO Hyper-V ISO Docker Image  Docker Image A base case. I want to confirm that I am in fact able to use packer without issue on GitHub Actions.\nHyperV I have had to do some work with Windows containers recently, and have found them to have performance issues for disk intense work. I was curious, if some of the provisioning scripts for the docker container could be used in a HyperV machine. That would allow for some performance experimentation between the two types.\nVirtualBox One of my previous projects involved using Vagrant to provision desktop environments. The problem with those environments is that the provisioning process was not hermetic. Building the same commit a month apart could result in significantly different images, or even failures. This was not ideal, so I wanted to investigate building images (ISOs) and using the Vagrantfile for environment configuration (e.g. ISO = environment, Vagrantfile = how you connect to this environment).\nAt the time I didn\u0026rsquo;t like the available options for CI/CD, so decided to shelve the idea for now. With GitHub Actions, I\u0026rsquo;m curious if I would be able to build these images, then deploy them to something like VagrantCloud.\n","id":95,"section":"posts","summary":"Experimenting with GitHub Actions for building machine images with Packer. Ideally trying to figure out what it takes for building the following on GitHub Actions: - VirtualBox ISO - Hyper-V ISO - Docker Image","tags":["devkitspaces"],"title":"packer-on-github","uri":"/2019/10/devkitspaces-packer-on-github/","year":"2019"},{"content":"   Viewdocs is simple project documentation. It renders Markdown as simple static pages.     Viewdocs Autodoc This was an early concept I was working with for converting metadata files (json/yml) into standard README markdown files using simple bash and templates. Ultimately I did not go in this direction, as I found that I was not making the best use of the customization yielded from using with bash + templates. A simple templating engine provided all the basics that I was ultimately after.\nBelow I have described some of the template concepts I was exploring at the time:\nIcon Acknowledgements I like to use icons from thenounproject.com, which I include an acknowledgements file (icon.json) in projects. I was looking to take this file, and convert it into a readable markdown file. Additionally I was looking at some simple scripts for generating the icons across all projects.\nBriefs I have been exploring having a repository.{json|yml} metadata file in all repositories. This would include all the header information you see on GitHub / GitLab, such as description, icon, social preview, tags, etc. The template would involve taking this data and converting it into a simple README.md that could be included in a list/aggregate site.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by MICHAEL G BROWN from the Noun Project.\n","id":96,"section":"posts","summary":"\u003cp\u003eThis was an early concept I was working with for converting metadata files (json/yml) into standard README markdown files using simple bash and templates. Ultimately I did not go in this direction, as I found that I was not making the best use of the customization yielded from using with bash + templates. A simple templating engine provided all the basics that I was ultimately after.\u003c/p\u003e\n\u003cp\u003eBelow I have described some of the template concepts I was exploring at the time:\u003c/p\u003e\n","tags":["jrbeverly"],"title":"viewdocs-autodoc","uri":"/2019/10/jrbeverly-viewdocs-autodoc/","year":"2019"},{"content":"   Prototyping a concept of an awesome-list based on yaml files.     Awesome Terraform Prototype An experiment using mkdocs and a series of json/yml files to define an awesome list.\nThe markdown files are automatically generated from the yml files that define each element of the list (tags/metadata/etc). These files are then piped into mkdocs, which yields a material theme website for the project.\n","id":97,"section":"posts","summary":"An experiment using \u003ccode\u003emkdocs\u003c/code\u003e and a series of json/yml files to define an awesome list. The markdown files are automatically generated from the yml files that define each element of the list (tags/metadata/etc). These files are then piped into \u003ccode\u003emkdocs\u003c/code\u003e, which yields a material theme website for the project.","tags":["jrbeverly"],"title":"awesome-terraform-prototype","uri":"/2019/10/jrbeverly-awesome-terraform-prototype/","year":"2019"},{"content":"   Experimenting with using docker-slim to shrink docker images     Minimizing Docker Builds Summary A GitlabCI repository designed to experiment with potential avenues for minimizing the size of a build image.\nNotes  Experiment with flags from docker:1.13 for minimizing the process Compress the build-context that is sent to the daemon (compress) Squash the layers using the --squash flag Experiment with the tool docker-slim for shrinking the build image Issues with permissions on the tooling are preventing deployment in Gitlab  ","id":98,"section":"posts","summary":"A GitlabCI repository designed to experiment with potential avenues for minimizing the size of a build image.","tags":["cardboardci"],"title":"slim-docker-with-gitlab","uri":"/2019/10/cardboardci-slim-docker-with-gitlab/","year":"2019"},{"content":"   An experiment for performing SVG icon processing     SVG Icon Processing Summary Experiment with programmatically generating color variants for SVG files using a JSON definition file with the source SVG.\nUsage The variants of each of the files is defined as a dictionary (string:object). The key for the dictionary matches the name of the variant. The object defines a collection of id and properties. These will be merged into the SVG to generate the variant icon. An example definition file is included below.\n{ \u0026#34;name\u0026#34;: \u0026#34;photo\u0026#34;, \u0026#34;variants\u0026#34;: { \u0026#34;pink\u0026#34;: { \u0026#34;outline\u0026#34;: { \u0026#34;stroke\u0026#34;: \u0026#34;#E54E7A\u0026#34; }, \u0026#34;outline_bg\u0026#34;: { \u0026#34;stroke\u0026#34;: \u0026#34;#E54E7A\u0026#34; }, \u0026#34;land\u0026#34;: { \u0026#34;fill\u0026#34;: \u0026#34;#E54E7A\u0026#34; }, \u0026#34;sun\u0026#34;: { \u0026#34;fill\u0026#34;: \u0026#34;#E54E7A\u0026#34; } }, \u0026#34;default\u0026#34;: { \u0026#34;outline\u0026#34;: { \u0026#34;stroke\u0026#34;: \u0026#34;#B35C00\u0026#34; }, \u0026#34;outline_bg\u0026#34;: { \u0026#34;stroke\u0026#34;: \u0026#34;#B35C00\u0026#34; }, \u0026#34;land\u0026#34;: { \u0026#34;fill\u0026#34;: \u0026#34;#3BB300\u0026#34; }, \u0026#34;sun\u0026#34;: { \u0026#34;fill\u0026#34;: \u0026#34;#FFF200\u0026#34; } } } } The above definition will generate two variants (named pink \u0026amp; default). For the pink variant, the list of SVG elements that match the IDs (outline, outline_bg, land, sun) will have the defined properties merged. In this case you would have something like:\n\u0026lt;circle id=\u0026#34;sun\u0026#34; cx=\u0026#34;23.3\u0026#34; cy=\u0026#34;43\u0026#34; r=\u0026#34;4.8\u0026#34;\u0026gt;\u0026lt;/circle\u0026gt; That will add the attribute (fill=#E54E7A) to the SVG element.\n\u0026lt;circle id=\u0026#34;sun\u0026#34; cx=\u0026#34;23.3\u0026#34; cy=\u0026#34;43\u0026#34; r=\u0026#34;4.8\u0026#34; fill=\u0026#34;#E54E7A\u0026#34;/\u0026gt; ","id":99,"section":"posts","summary":"Experiment with programmatically generating color variants for SVG files using a JSON definition file with the source SVG.","tags":["jrbeverly"],"title":"exp-svg-icon-processing","uri":"/2019/10/jrbeverly-exp-svg-icon-processing/","year":"2019"},{"content":"   Experimenting with using the .NET library BullsEye     BullsEye Experimentation Summary Experiment with BullsEye for building command-driven tooling (build-systems).\nUsage Experimenting with using BullsEye in a dotnet project. BullsEye doesn\u0026rsquo;t handle parsing of command line arguments, instead recommends using a tool for parsing them.\nvar app = new CommandLineApplication(throwOnUnexpectedArg: false); var foo = app.Option\u0026lt;string\u0026gt;(\u0026#34;--foo\u0026#34;, \u0026#34;foo\u0026#34;, CommandOptionType.SingleValue); BullsEye can then be used to built a higher level build system for languages (terraform, docker, etc).\nNotes  Creates common build-systems for templates (terraform-module, docker image) Auto-generate the console apps (BullsEye, CommandLineApplication) from a definition Define the system, then generate interfaces (service, cli, client, etc)  ","id":100,"section":"posts","summary":"Experiment with BullsEye for building command-driven tooling (build-systems).","tags":["jrbeverly"],"title":"bullseye-exp","uri":"/2019/10/jrbeverly-bullseye-exp/","year":"2019"},{"content":"   Using Rubocop and Shellcheck to perform linting on a Vagrantfile.     Vagrant Continuous Integration Prototype Pre-built Vagrant Box: -vagrant init ubuntu/trusty64\nThis example vagrant configuration installs and configures Ubuntu Trusty using simple Ruby scripts.\nThe objective is to move as much of the Vagrantfile configuration into external ruby scripts. These scripts could then be split into testable functions and modules. Currently the focus is on providing a consistent installation process using shellcheck and rubocop.\nAiming to have a simple vagrant example in this repository, and then use GitLab CI to perform linting on the provisioning scripts (shell+ruby).\nRequirements The following software must be installed/present on your local machine before you can use Vagrant to build the virtual machine:\n Vagrant VirtualBox  Usage Make sure all the required software (listed above) is installed, then cd to the directory containing this README.md file, and run:\nvagrant up  After a few minutes, Vagrant should tell you the machine was generated successfully.\n","id":101,"section":"posts","summary":"\u003cstrong\u003ePre-built Vagrant Box\u003c/strong\u003e: -\u003ca href=\"https://app.vagrantup.com/ubuntu/boxes/trusty64\"\u003e\u003ccode\u003evagrant init ubuntu/trusty64\u003c/code\u003e\u003c/a\u003e This example vagrant configuration installs and configures Ubuntu Trusty using simple Ruby scripts. The objective is to move as much of the Vagrantfile configuration into external ruby scripts. These scripts could then be split into testable functions and modules. Currently the focus is on providing a consistent installation process using \u003ccode\u003eshellcheck\u003c/code\u003e and \u003ccode\u003erubocop\u003c/code\u003e. Aiming to have a simple vagrant example in this repository, and then use GitLab CI to perform linting on the provisioning scripts (shell+ruby).","tags":["devkitspaces"],"title":"vagrant-cicd","uri":"/2019/10/devkitspaces-vagrant-cicd/","year":"2019"},{"content":"   GitHub Learning Lab Tutorial on \u0026lsquo;Write a Learning Lab course\u0026rsquo;     GitHub Learning Lab - Lab Starter Noticed this when working with GitHub Actions that you can have a automated \u0026lsquo;teacher\u0026rsquo; by using linting + GitHub Bot. Thought this was an interesting idea, and have started this repository to get a better understanding of how this actually works.\nWith GitHub now having its own CI/CD Pipeline, it may be possible to create all sorts of tutorials for setting up applications. The first one that comes to mind is gamedev with something like Godot. Since a written tutorial is more static, this would allow a learner to follow a structure process for writing their own game, while having step by step linting to ensure that nothing went off the guard-rails.\nPotential concern would be how the linting would handle a learner that introduced unexpected aspects to the code itself. For example, if the code is testing the output from the program, what if something like Console.WriteLine(\u0026quot;mydebuggingcode\u0026quot;) is present, would that be a failure? How should that best be handled? Etc.\n","id":102,"section":"posts","summary":"Noticed this when working with GitHub Actions that you can have a automated \u0026lsquo;teacher\u0026rsquo; by using linting + GitHub Bot. Thought this was an interesting idea, and have started this repository to get a better understanding of how this actually works. With GitHub now having its own CI/CD Pipeline, it may be possible to create all sorts of tutorials for setting up applications. The first one that comes to mind is gamedev with something like Godot. Since a written tutorial is more static, this would allow a learner to follow a structure process for writing their own game, while having step by step linting to ensure that nothing went off the guard-rails. Potential concern would be how the linting would handle a learner that introduced unexpected aspects to the code itself. For example, if the code is testing the output from the program, what if something like \u003ccode\u003eConsole.WriteLine(\u0026quot;mydebuggingcode\u0026quot;)\u003c/code\u003e is present, would that be a failure? How should that best be handled? Etc.","tags":["jrbeverly"],"title":"lab-starter","uri":"/2019/09/jrbeverly-lab-starter/","year":"2019"},{"content":"   The GitHub Learning Lab \u0026lsquo;Hello GitHub Actions\u0026rsquo; tutorial     Welcome to \u0026ldquo;Hello World\u0026rdquo; with GitHub Actions This course will walk you through writing your first action and using it with a workflow file.\nReady to get started? Navigate to the first issue.\n","id":103,"section":"posts","summary":"This course will walk you through writing your first action and using it with a workflow file. \u003cstrong\u003eReady to get started? Navigate to the first issue.\u003c/strong\u003e","tags":["jrbeverly"],"title":"hello-github-actions","uri":"/2019/09/jrbeverly-hello-github-actions/","year":"2019"},{"content":"   wkhtmltopdf is a command line tools to render HTML into PDF.     Docker image for WkHtmlToPDF wkhtmltopdf and wkhtmltoimage are open source (LGPLv3) command line tools to render HTML into PDF and various image formats using the Qt WebKit rendering engine. These run entirely \u0026ldquo;headless\u0026rdquo; and do not require a display or display service.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/wkhtmltopdf Test interactively docker run -it cardboardci/wkhtmltopdf /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/wkhtmltopdf aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/wkhtmltopdf aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/wkhtmltopdf:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/wkhtmltopdf:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/wkhtmltopdf:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/wkhtmltopdf:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":104,"section":"posts","summary":"wkhtmltopdf and wkhtmltoimage are open source (LGPLv3) command line tools to render HTML into PDF and various image formats using the Qt WebKit rendering engine. These run entirely \u0026ldquo;headless\u0026rdquo; and do not require a display or display service. You can see the cli reference \u003ca href=\"https://github.com/wkhtmltopdf/wkhtmltopdf\"\u003ehere\u003c/a\u003e.","tags":["cardboardci"],"title":"docker-wkhtmltopdf","uri":"/2019/03/cardboardci-docker-wkhtmltopdf/","year":"2019"},{"content":"   Tools for working with Scalable Vector Graphics (SVG) files     Docker image for SVG Tools SVG Tools are a collection of tools for working with vector graphics.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/svgtools Test interactively docker run -it cardboardci/svgtools /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/svgtools aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/svgtools aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/svgtools:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/svgtools:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/svgtools:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/svgtools:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":105,"section":"posts","summary":"SVG Tools are a collection of tools for working with vector graphics. You can see the cli reference \u003ca href=\"https://github.com/inkscape/inkscape\"\u003ehere\u003c/a\u003e.","tags":["cardboardci"],"title":"docker-svgtools","uri":"/2019/03/cardboardci-docker-svgtools/","year":"2019"},{"content":"   Surge is static web publishing for Front-End Developers, right from the CLI.     Docker image for Surge This is the CLI client for the surge.sh hosted service. Its what gets installed when you run npm install -g surge.\nThis CLI library manages access tokens locally and handles the upload and subsequent reporting when you publish a project using surge.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/surge Test interactively docker run -it cardboardci/surge /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/surge aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/surge aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/surge:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/surge:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/surge:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/surge:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":106,"section":"posts","summary":"This is the CLI client for the surge.sh hosted service. Its what gets installed when you run \u003ccode\u003enpm install -g surge\u003c/code\u003e. This CLI library manages access tokens locally and handles the upload and subsequent reporting when you publish a project using surge. You can see the cli reference \u003ca href=\"https://github.com/sintaxi/surge\"\u003ehere\u003c/a\u003e.","tags":["cardboardci"],"title":"docker-surge","uri":"/2019/03/cardboardci-docker-surge/","year":"2019"},{"content":"   ShellCheck is a static anaylsis tool that automatically finds bugs in your shell scripts.     Docker image for Shellcheck ShellCheck is a GPLv3 tool that gives warnings and suggestions for bash/sh shell scripts:\nThe goals of ShellCheck are:\n To point out and clarify typical beginner\u0026rsquo;s syntax issues that cause a shell to give cryptic error messages. To point out and clarify typical intermediate level semantic problems that cause a shell to behave strangely and counter-intuitively. To point out subtle caveats, corner cases and pitfalls that may cause an advanced user\u0026rsquo;s otherwise working script to fail under future circumstances.  You can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/shellcheck Test interactively docker run -it cardboardci/shellcheck /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/shellcheck aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/shellcheck aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/shellcheck:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/shellcheck:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/shellcheck:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/shellcheck:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":107,"section":"posts","summary":"ShellCheck is a GPLv3 tool that gives warnings and suggestions for bash/sh shell scripts: The goals of ShellCheck are: bazel BUILD.bazel docs icon outputs README.md scripts srv WORKSPACE To point out and clarify typical beginner\u0026rsquo;s syntax issues that cause a shell to give cryptic error messages. bazel BUILD.bazel docs icon outputs README.md scripts srv WORKSPACE To point out and clarify typical intermediate level semantic problems that cause a shell to behave strangely and counter-intuitively. bazel BUILD.bazel docs icon outputs README.md scripts srv WORKSPACE To point out subtle caveats, corner cases and pitfalls that may cause an advanced user\u0026rsquo;s otherwise working script to fail under future circumstances. You can see the cli reference \u003ca href=\"https://github.com/koalaman/shellcheck\"\u003ehere\u003c/a\u003e.","tags":["cardboardci"],"title":"docker-shellcheck","uri":"/2019/03/cardboardci-docker-shellcheck/","year":"2019"},{"content":"   PSScriptAnalyzer is a static code checker for Windows PowerShell modules and scripts.     Docker image for PSScriptAnalyzer PSScriptAnalyzer is a static code checker for Windows PowerShell modules and scripts. PSScriptAnalyzer checks the quality of Windows PowerShell code by running a set of rules. The rules are based on PowerShell best practices identified by PowerShell Team and the community. It generates DiagnosticResults (errors and warnings) to inform users about potential code defects and suggests possible solutions for improvements.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/psscriptanalyzer Test interactively docker run -it cardboardci/psscriptanalyzer pwsh Test interactively with Bash docker run -it cardboardci/psscriptanalyzer bash Emit version table docker run -it cardboardci/psscriptanalyzer pwsh -Command \u0026#39;$PSVersionTable\u0026#39; Emit versions of installed modules docker run -it cardboardci/psscriptanalyzer pwsh -Command \u0026#39;Get-InstalledModule\u0026#39; Run format invocation docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/psscriptanalyzer pwsh -Command \u0026#39;Invoke-Formatter -ScriptDefinition (Get-Content -Path \u0026#39;File.ps1\u0026#39; -Raw)\u0026#39; Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/psscriptanalyzer:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/psscriptanalyzer:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/psscriptanalyzer:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/psscriptanalyzer:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":108,"section":"posts","summary":"PSScriptAnalyzer is a static code checker for Windows PowerShell modules and scripts. PSScriptAnalyzer checks the quality of Windows PowerShell code by running a set of rules. The rules are based on PowerShell best practices identified by PowerShell Team and the community. It generates DiagnosticResults (errors and warnings) to inform users about potential code defects and suggests possible solutions for improvements. You can see the cli reference \u003ca href=\"https://github.com/PowerShell/PSScriptAnalyzer\"\u003ehere\u003c/a\u003e.","tags":["cardboardci"],"title":"docker-psscriptanalyzer","uri":"/2019/03/cardboardci-docker-psscriptanalyzer/","year":"2019"},{"content":"   Command lines tools for manipulating pdfs.     Docker image for PdfTools Scientific articles are typically locked away in PDF format, a format designed primarily for printing but not so great for searching or indexing. The new pdftools package allows for extracting text and metadata from pdf files in R. From the extracted plain-text one could find articles discussing a particular drug or species name, without having to rely on publishers providing metadata, or pay-walled search engines.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/pdftools Test interactively docker run -it cardboardci/pdftools /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/pdftools aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/pdftools aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/pdftools:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/pdftools:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/pdftools:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/pdftools:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":109,"section":"posts","summary":"Scientific articles are typically locked away in PDF format, a format designed primarily for printing but not so great for searching or indexing. The new pdftools package allows for extracting text and metadata from pdf files in R. From the extracted plain-text one could find articles discussing a particular drug or species name, without having to rely on publishers providing metadata, or pay-walled search engines. You can see the cli reference \u003ca href=\"https://github.com/ropensci/pdftools\"\u003ehere\u003c/a\u003e.","tags":["cardboardci"],"title":"docker-pdftools","uri":"/2019/03/cardboardci-docker-pdftools/","year":"2019"},{"content":"   Convert PDF to HTML without losing text or format.     Docker image for Pdf2HtmlEX pdf2htmlEX renders PDF files in HTML, utilizing modern Web technologies. Academic papers with lots of formulas and figures? Magazines with complicated layouts? No problem!\nFeatures:\n Native HTML text with precise font and location. Flexible output: all-in-one HTML or on demand page loading (needs JavaScript). Moderate file size, sometimes even smaller than PDF. Supporting links, outlines (bookmarks), printing, SVG background, Type 3 fonts and more.  You can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/pdf2htmlex Test interactively docker run -it cardboardci/pdf2htmlex /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/pdf2htmlex aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/pdf2htmlex aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/pdf2htmlex:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/pdf2htmlex:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/pdf2htmlex:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/pdf2htmlex:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":110,"section":"posts","summary":"pdf2htmlEX renders PDF files in HTML, utilizing modern Web technologies. Academic papers with lots of formulas and figures? Magazines with complicated layouts? No problem! Features: bazel BUILD.bazel docs icon outputs README.md scripts srv WORKSPACE Native HTML text with precise font and location. bazel BUILD.bazel docs icon outputs README.md scripts srv WORKSPACE Flexible output: all-in-one HTML or on demand page loading (needs JavaScript). bazel BUILD.bazel docs icon outputs README.md scripts srv WORKSPACE Moderate file size, sometimes even smaller than PDF. bazel BUILD.bazel docs icon outputs README.md scripts srv WORKSPACE Supporting links, outlines (bookmarks), printing, SVG background, Type 3 fonts and more. You can see the cli reference \u003ca href=\"https://github.com/coolwanglu/pdf2htmlEX\"\u003ehere\u003c/a\u003e.","tags":["cardboardci"],"title":"docker-pdf2htmlex","uri":"/2019/03/cardboardci-docker-pdf2htmlex/","year":"2019"},{"content":"   Netlify builds, deploys and hosts your netlify services.     Docker image for Netlify The Netlify CLI facilitates the deployment of websites to Netlify, to improve the site building experience.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/netlify Test interactively docker run -it cardboardci/netlify /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/netlify aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/netlify aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/netlify:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/netlify:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/netlify:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/netlify:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":111,"section":"posts","summary":"The Netlify CLI facilitates the deployment of websites to Netlify, to improve the site building experience. You can see the cli reference \u003ca href=\"https://github.com/netlify/cli\"\u003ehere\u003c/a\u003e.","tags":["cardboardci"],"title":"docker-netlify","uri":"/2019/03/cardboardci-docker-netlify/","year":"2019"},{"content":"   LaTeX is a system for computer typesetting of documents.     Docker image for LaTeX LaTeX is a high-quality typesetting system; it includes features designed for the production of technical and scientific documentation. LaTeX is the de facto standard for the communication and publication of scientific documents. LaTeX is available as free software.\nYou can see the LaTeX reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/latex Test interactively docker run -it cardboardci/latex /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/latex aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/latex aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/latex:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/latex:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/latex:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/latex:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":112,"section":"posts","summary":"LaTeX is a high-quality typesetting system; it includes features designed for the production of technical and scientific documentation. LaTeX is the de facto standard for the communication and publication of scientific documents. LaTeX is available as free software. You can see the LaTeX reference \u003ca href=\"https://www.tug.org/begin.html\"\u003ehere\u003c/a\u003e.","tags":["cardboardci"],"title":"docker-latex","uri":"/2019/03/cardboardci-docker-latex/","year":"2019"},{"content":"   Hugo is an open-source static site generator.     Docker image for Hugo Hugo is one of the most popular open-source static site generators. With its amazing speed and flexibility, Hugo makes building websites fun again.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/hugo Test interactively docker run -it cardboardci/hugo /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/hugo aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/hugo aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/hugo:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/hugo:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/hugo:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/hugo:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":113,"section":"posts","summary":"Hugo is one of the most popular open-source static site generators. With its amazing speed and flexibility, Hugo makes building websites fun again. You can see the cli reference \u003ca href=\"https://github.com/gohugoio/hugo/\"\u003ehere\u003c/a\u003e.","tags":["cardboardci"],"title":"docker-hugo","uri":"/2019/03/cardboardci-docker-hugo/","year":"2019"},{"content":"   Lab wraps Git or Hub, making it simple to clone, fork, and interact with repositories on GitLab.     Docker image for GitLabCLI What is GitLabCLI ?\n It\u0026rsquo;s a cross platform GitLab command line tool to quickly \u0026amp; naturally perform frequent tasks on GitLab project. It does not force you to hand craft json or use other unnatural ways (for example ids, concatenating of strings) like other CLI\u0026rsquo;s to interact with GitLab. It does not have any dependencies. It\u0026rsquo;s self contained .NET core application - you don\u0026rsquo;t need to have .NET installed for it to work.  You can see the source repository here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/awscli Test interactively docker run -it cardboardci/awscli /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/awscli aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/awscli aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/awscli:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/awscli:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/awscli:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/awscli:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":114,"section":"posts","summary":"What is GitLabCLI ? bazel BUILD.bazel docs icon outputs README.md scripts srv WORKSPACE It\u0026rsquo;s a cross platform GitLab command line tool to quickly \u0026amp; naturally perform frequent tasks on GitLab project. bazel BUILD.bazel docs icon outputs README.md scripts srv WORKSPACE It does not force you to hand craft json or use other unnatural ways (for example ids, concatenating of strings) like other CLI\u0026rsquo;s to interact with GitLab. bazel BUILD.bazel docs icon outputs README.md scripts srv WORKSPACE It does not have any dependencies. bazel BUILD.bazel docs icon outputs README.md scripts srv WORKSPACE It\u0026rsquo;s self contained .NET core application - you don\u0026rsquo;t need to have .NET installed for it to work. You can see the source repository \u003ca href=\"https://github.com/nmklotas/GitLabCLI\"\u003ehere\u003c/a\u003e.","tags":["cardboardci"],"title":"docker-gitlab","uri":"/2019/03/cardboardci-docker-gitlab/","year":"2019"},{"content":"   A unified tool to deploy Docker images to Amazon Elastic Container Registry (ECR).     Docker image for AWS CLI \u0026amp; Docker The AWS Command Line Interface (CLI) is a unified tool to manage your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts. This container includes docker, allowing deployments to Amazon Elastic Container Registry (ECR), a fully-managed Docker container registry.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/awscli Test interactively docker run -it cardboardci/awscli /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/awscli aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/awscli aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/awscli:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/awscli:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/awscli:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/awscli:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":115,"section":"posts","summary":"The AWS Command Line Interface (CLI) is a unified tool to manage your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts. This container includes docker, allowing deployments to Amazon Elastic Container Registry (ECR), a fully-managed Docker container registry. You can see the cli reference \u003ca href=\"https://docs.aws.amazon.com/cli/latest/reference/ecr/index.html\"\u003ehere\u003c/a\u003e.","tags":["cardboardci"],"title":"docker-ecr","uri":"/2019/03/cardboardci-docker-ecr/","year":"2019"},{"content":"   Bats is most useful when testing software written in Bash, but you can use it to test any UNIX program.     Docker image for Bats (Bash Automated Testing System) Bats is a TAP-compliant testing framework for Bash. It provides a simple way to verify that the UNIX programs you write behave as expected.\nA Bats test file is a Bash script with special syntax for defining test cases. Under the hood, each test case is just a function with a description.\n#!/usr/bin/env bats  @test \u0026#34;addition using bc\u0026#34; { result=\u0026#34;$(echo 2+2 | bc)\u0026#34; [ \u0026#34;$result\u0026#34; -eq 4 ] } @test \u0026#34;addition using dc\u0026#34; { result=\u0026#34;$(echo 2 2+p | dc)\u0026#34; [ \u0026#34;$result\u0026#34; -eq 4 ] } You can see the source repository here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/awscli Test interactively docker run -it cardboardci/awscli /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/awscli aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/awscli aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/awscli:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/awscli:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/awscli:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/awscli:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":116,"section":"posts","summary":"Bats is a TAP-compliant testing framework for Bash. It provides a simple way to verify that the UNIX programs you write behave as expected. A Bats test file is a Bash script with special syntax for defining test cases. Under the hood, each test case is just a function with a description. ```bash","tags":["cardboardci"],"title":"docker-bats","uri":"/2019/03/cardboardci-docker-bats/","year":"2019"},{"content":"   The AWS Command Line Interface (CLI) is a unified tool to manage your AWS services.     Docker image for AWS CLI The AWS Command Line Interface (CLI) is a unified tool to manage your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/awscli Test interactively docker run -it cardboardci/awscli /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/awscli aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/awscli aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/awscli:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/awscli:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/awscli:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/awscli:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":117,"section":"posts","summary":"The AWS Command Line Interface (CLI) is a unified tool to manage your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts. You can see the cli reference \u003ca href=\"https://docs.aws.amazon.com/cli/latest/reference/\"\u003ehere\u003c/a\u003e.","tags":["cardboardci"],"title":"docker-awscli","uri":"/2019/03/cardboardci-docker-awscli/","year":"2019"},{"content":"   A command-line tool that makes git easier to use with GitHub.     Docker image for GitHubCLI hub is an extension to command-line git that helps you do everyday GitHub tasks without ever leaving the terminal. hub can be safely aliased as git so you can type $ git \u0026lt;command\u0026gt;in the shell and get all the usual hub features.\nYou can see the source repository here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/awscli Test interactively docker run -it cardboardci/awscli /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/awscli aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/awscli aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/awscli:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/awscli:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/awscli:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/awscli:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":118,"section":"posts","summary":"\u003ccode\u003ehub\u003c/code\u003e is an extension to command-line git that helps you do everyday GitHub tasks without ever leaving the terminal. \u003ccode\u003ehub\u003c/code\u003e can be safely aliased as git so you can type \u003ccode\u003e$ git \u0026lt;command\u0026gt;\u003c/code\u003ein the shell and get all the usual hub features. You can see the source repository \u003ca href=\"https://github.com/dropbox/dbxcli\"\u003ehere\u003c/a\u003e.","tags":["cardboardci"],"title":"docker-github","uri":"/2019/03/cardboardci-docker-github/","year":"2019"},{"content":"   Dockerfile linter, validate inline bash, written in Haskell.     Docker image for HadoLint A smarter Dockerfile linter that helps you build best practice Docker images. The linter is parsing the Dockerfile into an AST and performs rules on top of the AST. It is standing on the shoulders of ShellCheck to lint the Bash code inside RUN instructions.\nYou can see the source repository here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/awscli Test interactively docker run -it cardboardci/awscli /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/awscli aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/awscli aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/awscli:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/awscli:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/awscli:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/awscli:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":119,"section":"posts","summary":"A smarter Dockerfile linter that helps you build best practice Docker images. The linter is parsing the Dockerfile into an AST and performs rules on top of the AST. It is standing on the shoulders of ShellCheck to lint the Bash code inside RUN instructions. You can see the source repository \u003ca href=\"https://github.com/hadolint/hadolint\"\u003ehere\u003c/a\u003e.","tags":["cardboardci"],"title":"docker-hadolint","uri":"/2019/03/cardboardci-docker-hadolint/","year":"2019"},{"content":"   The AWS Command Line Interface (CLI) is a unified tool to manage your AWS services.     Docker AWSCLI A docker image based on Windows containing the awscli.\n","id":120,"section":"posts","summary":"A docker image based on Windows containing the awscli.","tags":["cardboardci"],"title":"docker-win-awscli","uri":"/2019/02/cardboardci-docker-win-awscli/","year":"2019"},{"content":"   A simple CI/CD pipeline making use of CoreRT to build native tooling.     .NET Core Native Compilation Experiments A simple CI/CD pipeline making use of CoreRT to build linux and windows copies of a \u0026ldquo;Hello World\u0026rdquo; console application.\nNotes  CoreRT currently does not support compilation cross-compilation (as it is not supported yet). AOT for a linux binary requires non-dotnet dependencies (that need to be installed to be base image)  At present it is likely best to use Windows docker images for building the dotnet native jobs.\n","id":121,"section":"posts","summary":"A simple CI/CD pipeline making use of CoreRT to build linux and windows copies of a \u0026ldquo;Hello World\u0026rdquo; console application.","tags":["jrbeverly"],"title":"dotnet-native-corert","uri":"/2019/02/jrbeverly-dotnet-native-corert/","year":"2019"},{"content":"   A simple repository for testing terraform pipelines.     Prototype CodePipeline Terraform Repository A repository for terraform execution in a Codepipeline task. This repository is part of an original experiment\nI wanted to have an terraform executor that met the following requirements:\n Use official terraform docker image (hashicorp/terraform:light) No external dependencies or custom images (e.g. terragrunt, astro, etc) Customizable execution process with minimal overhead Support in-repository modules No credential management (AWS Codepipeline execution) Multiple AWS environments within a single repository No single state file, state file per component (controlled by terraform.tf file) State files map to location in repository Potential for custom IAM role per component (as opposed to single access permission)  This was a quick prototype to see if I would be able to get something rough running, with the shell executor being just the bare essentials that I need.\nIssues with the final result There are a couple of issues I noted when setting this up, and the eventual improvements made to the later executors. I have listed them below:\n CodePipeline requires cloudwatch to provide notifications on failure CodeBuild log for the terraform plan/show outputs is not very pretty CodePipeline is limited to a single branch, impacting the idea of \u0026lsquo;preview\u0026rsquo; builds Shell based executors are simple, but require maintenance YAML Executors (see GitLab / GitHub Actions / CircleCI) have isolated executor for each component Deployments to 2 or more accounts requires a \u0026lsquo;GitFlow\u0026rsquo; style approach, which has a lot of overhead Restricting permissions on the component level requires a fair bit of extra work, and isn\u0026rsquo;t really sound Control options are handled by files (e.g. ORDER, IGNORE, APPLY_ONLY), which doesn\u0026rsquo;t lend itself to customization well  It is nice to have everything in AWS, with serverless architecture but I find the limitations are a big issue. If I was starting from scratch, I would be interested in the idea of deploying a GitLab + GitLabCI architecture in AWS, then restricting IAM access by executors (that would be provisioned by AWS Lambda).\nThe big problems I have with this style of terraform executor is the edge cases. I\u0026rsquo;ll mention three that have bugged me the most:\nDeployments into another AWS ACcount Allowing another team (or individual) to have a component (e.g. teraform deployment) that exists within an account I own. I would want to restrict access for that role, and keep knowledge of that terraform within my infrastructure repository. Also wouldn\u0026rsquo;t want to require my approval on every PR, but at the same time not allow free reign within the AWS account.\nIt is an option to just expose a role in an account, then allow the terraform+executor to be handled by the team in another repo/executor. This can mean back and forth a bit, as the IAM permissions for that role need to be fine tuned\nExisting infrastructure / failed deployments This can happen when you are not using uniqely named resources (e.g. my-s3-bucket-x2r1), where terraform will fail when it encounters an existing resource. Failures can also happen when working with a non-terraform managed resource (e.g. IAM policy usages). When using a codepipeline executor, everything is in a single log file so it can be difficult to know:\n Who failed? What resource had failed? Where is that in the repository?  Cross-cutting concerns Deployments with codepipeline executors do not work well for cross-cutting concerns. If I want to deploy something into every single AWS account (such as an IAM role or S3 bucket), it just doesn\u0026rsquo;t work well. I don\u0026rsquo;t get the advantages of parallel execution, and a failure midway cannot be reverted easily. Permissions are also way too open for my taste, in that the single executor is running in every single account within any real guard-rails.\nAt least with an external ID per account role, with isolated execution (in say GitLabCI / CircleCI) I have more confidence that I won\u0026rsquo;t see misconfiguration in an account.\n","id":122,"section":"posts","summary":"A repository for terraform execution in a Codepipeline task. This repository is part of an original experiment I wanted to have an terraform executor that met the following requirements: - Use official terraform docker image (\u003ccode\u003ehashicorp/terraform:light\u003c/code\u003e) - No external dependencies or custom images (e.g. terragrunt, astro, etc) - Customizable execution process with minimal overhead - Support in-repository modules - No credential management (AWS Codepipeline execution) - Multiple AWS environments within a single repository - No single state file, state file per component (controlled by \u003ccode\u003eterraform.tf\u003c/code\u003e file) - State files map to location in repository - Potential for custom IAM role per component (as opposed to single access permission) This was a quick prototype to see if I would be able to get something rough running, with the shell executor being just the bare essentials that I need.","tags":["infraprints"],"title":"simple-terraform","uri":"/2019/01/infraprints-simple-terraform/","year":"2019"},{"content":"   TFLint is a Terraform linter for detecting errors that can not be detected by terraform plan.     Docker image for TfLint TFLint is a Terraform linter focused on possible errors, best practices, etc.\nYou can see the cli reference here.\nUsage You can run awscli to manage your AWS services.\naws iam list-users aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \u0026#34;*\u0026#34; --include \u0026#34;*.jpg\u0026#34; aws sts assume-role --role-arn arn:aws:iam::123456789012:role/xaccounts3access --role-session-name s3-access-example Pull latest image docker pull cardboardci/tflint Test interactively docker run -it cardboardci/tflint /bin/bash Run basic AWS command docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace cardboardci/tflint aws s3 cp file.txt s3://bucket/file.txt Run AWS CLI with custom profile docker run -it -v \u0026#34;$(pwd)\u0026#34;:/workspace -v \u0026#34;~/.aws/\u0026#34;:/cardboardci/.aws/ cardboardci/tflint aws s3 cp file.txt s3://bucket/file.txt Continuous Integration Services For each of the following services, you can see an example of this image in that environment:\n CircleCI GitHub Actions GitLabCI JenkinsFile TravisCI Codeship  Tagging Strategy Every new release of the image includes three tags: version, date and latest. These tags can be described as such:\n latest: The most-recently released version of an image. (cardboardci/tflint:latest) \u0026lt;version\u0026gt;: The most-recently released version of an image for that version of the tool. (cardboardci/tflint:1.0.0) \u0026lt;version-date\u0026gt;: The version of the tool released on a specific date (cardboarci/awscli:1.0.0-20190101)  We recommend using the digest for the docker image, or pinning to the version-date tag. If you are unsure how to get the digest, you can retrieve it for any image with the following command:\ndocker pull cardboardci/tflint:latest docker inspect --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; cardboardci/tflint:latest Fundamentals All images in the CardboardCI namespace are built from cardboardci/ci-core. This image ensures that the base environment for every image is always up to date. The common base image provides dependencies that are often used building and deploying software.\nBy having a common base, it means that each image is able to focus on providing the optimal tooling for each development workflow.\n","id":123,"section":"posts","summary":"TFLint is a Terraform linter focused on possible errors, best practices, etc. You can see the cli reference \u003ca href=\"https://github.com/terraform-linters/tflint\"\u003ehere\u003c/a\u003e.","tags":["cardboardci"],"title":"docker-tflint","uri":"/2018/12/cardboardci-docker-tflint/","year":"2018"},{"content":"   Experimenting with a build-harness style of makefiles     Makefile Experiments Summary Experimenting with using makefiles as a build harness type structure. The idea is to package makefile using GitHub, that can then be downloaded when running.\nConceptual Usage As the structure is simply an experiment, no targets are actually implemented. At the top of the Makefile, you can include the makefile using the following:\n-include $(shell curl -sSL -o .build-system \u0026#34;https://.../makefile\u0026#34;; echo .build-system) This will download a Makefile called .build-system and include it at run-time. Once it has been successfully downloaded, you can run make help for a lsit of available targets.\nLimitations Although the targets for a library are included, that doesn\u0026rsquo;t mean that any dependencies are included. For example, if a makefile library relies on non-standard binaries (e.g. jq, awscli, docker), then they would need to be installed on the local environment.\nI have found this doesn\u0026rsquo;t scale that well, especially for dependencies that are frequently updated (pre-1.0.0). It does have a bit of advantage in scrapping together quick and dirty build systems when working with Dev-Ops style work (docker, terraform, ansible, packer). As I have found it pretty nice to get useful commands out of the box (and readily available with make help).\nInspiration These experiments are inspired by cloudposse/build-harness.\n","id":124,"section":"posts","summary":"Experimenting with using makefiles as a build harness type structure. The idea is to package \u003ccode\u003emakefile\u003c/code\u003e using GitHub, that can then be downloaded when running.","tags":["jrbeverly"],"title":"make-exp","uri":"/2018/12/jrbeverly-make-exp/","year":"2018"},{"content":"   Terraform modules that define \u0026lsquo;Repositories - jrbeverly\u0026rsquo;     Infrastructure Summary The specification of jrbeverlylabs as a set of terraform modules.\nUsage To run this you need to execute:\nterraform init terraform plan terraform apply Notes This was a simple experiment making use of the gitlab provider of terraform. The idea was to see if it would assist in the process of maintaining jrbeverlylabs between gitlab.com and my internal gitlab instance.\n","id":125,"section":"posts","summary":"The specification of \u003ccode\u003ejrbeverlylabs\u003c/code\u003e as a set of terraform modules.","tags":["jrbeverly"],"title":"infrastructure-labs","uri":"/2018/11/jrbeverly-infrastructure-labs/","year":"2018"},{"content":"   Strongly-typed attributes for the management and organization of tests.     XUnit.Metadata Summary Strongly-typed attributes for the management and organization of tests. As opposed to using strings throughout the code, [Trait(\u0026quot;Category\u0026quot;, \u0026quot;Unit\u0026quot;)], you can use strongly-typed attributes for organizing tests.\nGetting Started With xUnit v2 you can markup tests with traits, particullary of interest is the category key. Using traits you can sort tests or control execution of tests from the command line. You can install in your project using the Nuget Manager Console:\nInstall-Package xUnit.Metadata Or from the .NET CLI with the following command:\ndotnet add package xUnit.Metadata Usage You can use the attributes just as you would a TraitAttribute from xUnit v2.\n[Fact] [Unit] [Nighty] public void ExceptionWhenDivideByZero() { var value = 10; Assert.Throws\u0026lt;ArgumentException\u0026gt;(() =\u0026gt; _adder.Divide(value, 0)); } Or as a class attribute:\n[Unit] public sealed class AdderTests { [Theory] [Nighty] [InlineData(3, 1, 2)] [InlineData(4, 2, 2)] [InlineData(1, 1, 0)] [InlineData(0, -2, 2)] public void AddSet(int expected, int a, int b) { Assert.Equal(expected, _adder.Add(a, b)); } [Fact] [Weekly] public void ExceptionWhenDivideByZero() { var value = 10; Assert.Throws\u0026lt;ArgumentException\u0026gt;(() =\u0026gt; _adder.Divide(value, 0)); } } Running from Command Line When using the .NET CLI, you can use dotnet test to execute tests from the command line. You can selectively execute tests based on filtering condition through --filter. You can do this with the following:\ndotnet test --filter \u0026#34;Category = Unit\u0026#34; ProjectA.csproj Running the above: dotnet test --filter Category=Unit will run tests which are annotated with the strongly typed attribute [Unit] or [Trait(\u0026quot;Category\u0026quot;, \u0026quot;Unit\u0026quot;)].\nBuild You can build the project using the scripts available in the build/ directory. The scripts are used in the project pipeline, and can be run locally or in a docker container. You can do this with the following:\nsh build/build.sh Or you can use the docker based script, docker-build.sh if you do not have the necessary commands installed locally on your machine. You can do this with the following:\nsh build/docker-build.sh Packaging You can package the library into a nuget package using the package.sh script. You can do this with the following:\nsh build/package.sh Tests All scripts in the repository use xUnit for testing. You can view the tests in the tests/ directory. An example testing project that makes use of the attributes is available under sample/. However, this project is more designed as a prototyping ground for new attributes. To run the tests for the project, you can do so with the following:\nsh build/test.sh Or you can use the docker version of the script, docker-test.sh if you do not have the necessary commands installed locally on your machine. You can do this with the following:\nsh build/docker-test.sh Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Ana Paula Tello from the Noun Project.\n","id":126,"section":"posts","summary":"Strongly-typed attributes for the management and organization of tests. As opposed to using strings throughout the code, \u003ccode\u003e[Trait(\u0026quot;Category\u0026quot;, \u0026quot;Unit\u0026quot;)]\u003c/code\u003e, you can use strongly-typed attributes for organizing tests.","tags":["jrbeverly"],"title":"xunit-metadata","uri":"/2018/11/jrbeverly-xunit-metadata/","year":"2018"},{"content":"   A meta-repository for facilitating development of the many-repository XPlatformer project.     XPlatformer-Workspace Summary A meta-repository for facilitating development of the many-repository XPlatformer project.\nGetting Started XPlatformer-Workspace is a git-submodules oriented approach for dealing with the multiple repositories of the XPlatformer project (XPlatformer / XGameLib / XSamples). The project provides all the repositories in one, using vagrant-desktop to provide an X11 development environment. You can clone all the repositories using the --recursive directive of git as such:\ngit clone --recursive git@gitlab:.../XPlatformer-Workspace.git Or if the repository has already been cloned, you can use:\ngit submodule init git submodule update Usage The Workspace architecture aims to provide a way of provisioning a complete environment for working with multiple repositories. The desired architecture of the meta-project is specified below:\n* Repository * bin * lib * build * build-all.sh * ... * Repositories * [r] XPlatformer (git@gitlab/XPlatformer.git) * [r] XGameLib (git@gitlab/XGameLib.git) * ... * Environments * [r] vagrant-desktop-x11 (git@gitlab/vagrant-desktop-x11.git) * ... * README.md The XPlatformer-Workspace is a modified version of the proposed architecture where the environments and repositories directories are merged. This is done to simplify the working environment.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Maxi Koichi from the Noun Project.\n","id":127,"section":"posts","summary":"A meta-repository for facilitating development of the many-repository XPlatformer project.","tags":["xplatformer"],"title":"xplatformer-workspace","uri":"/2018/11/xplatformer-xplatformer-workspace/","year":"2018"},{"content":"   Wifi Web provides an autorun USB for connecting to wireless access points for devices that do not have access to a camera     Wifi Web Summary Wifi Web provides an autorun USB for connecting to wireless access points for devices that do not have access to a camera. It opens an HTML page that provides easy access to the Wifi connection details.\nIf you have a camera-enabled device, you can scan Wifi connection details using a QR Code (or any barcode type).\nInstallation You can install Wifi Web onto a USB stick by unzipping the most recent build. You can use the autorun.sh or autorun.bat file to open the HTML page. If autorun is possible in the environment (unlikely), you will have the opporunity to open the HTML page when the USB device is connected.\nConfiguration You need to manually edit the connection details, you can do so with the js/connections.js file. The file is of the format:\nexports = { connections: [ { name: \u0026#34;My Wifi 2.4-Ghz\u0026#34;, password: \u0026#34;Passw0rd!\u0026#34; }, { name: \u0026#34;My Wifi 5-Ghz\u0026#34;, password: \u0026#34;L337Pass\u0026#34; } ] }; You can see an example of the file format in js/default.js.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by novita dian from the Noun Project.\n","id":128,"section":"posts","summary":"Wifi Web provides an autorun USB for connecting to wireless access points for devices that do not have access to a camera. It opens an HTML page that provides easy access to the Wifi connection details. If you have a camera-enabled device, you can scan Wifi connection details using a QR Code (or any barcode type).","tags":["jrbeverly"],"title":"wifi-web","uri":"/2018/11/jrbeverly-wifi-web/","year":"2018"},{"content":"   stack-opengl is a variant of stack-net written in OpenGL.     stack-opengl Summary stack-opengl is a variant of stack-net written in OpenGL. It uses extremely simple shaders and OpenGL programming to create a block stacking application.\nGetting Started The project uses premake4 as the cross-platform build system. You will need to build the external dependencies of the project, by running a root level build. You can then build the project itself. You can do so as such:\npremake4 gmake make cd src/ premake4 gmake make ./Stack Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Christina Witt George from the Noun Project.\n","id":129,"section":"posts","summary":"stack-opengl is a variant of \u003ccode\u003estack-net\u003c/code\u003e written in OpenGL. It uses extremely simple shaders and OpenGL programming to create a block stacking application.","tags":["jrbeverly"],"title":"stack-opengl","uri":"/2018/11/jrbeverly-stack-opengl/","year":"2018"},{"content":"   A block blueprinter, built using a visual graph style approach to graphics.     Stack-NET Summary A block blueprinter, built using a visual graph style approach to graphics.\nGetting Started The project is based on the old approach to C# projects. The project should be opened in Visual Studio, built, then run.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Christina Witt George from the Noun Project.\n","id":130,"section":"posts","summary":"A block blueprinter, built using a visual graph style approach to graphics.     Stack-NET Summary A block blueprinter, built using a visual graph style approach to graphics.\nGetting Started The project is based on the old approach to C# projects. The project should be opened in Visual Studio, built, then run.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project.","tags":["jrbeverly"],"title":"stack-net","uri":"/2018/11/jrbeverly-stack-net/","year":"2018"},{"content":"   A Raytracer that receives a scene defined in lua, and produces an image output.     RayTracer Summary A Raytracer that receives a scene defined in lua, and produces an image output.\nGetting Started Compilation follows the standard process defined by the UWaterloo CS488 sample projects.\nWe use premake4 as our cross-platform build system. First you will need to build all the static libraries that the projects depend on. To build the libraries, open up a terminal, and cd to the top level of the CS488 project directory and then run the following:\npremake4 gmake make This will build the following static libraries, and place them in the top level lib folder of your cs488 project directory.\n libcs488-framework.a libglfw3.a libimgui.a  NOTE: As the following is only the relevant Raytracer files, the build system/dependencies and premake4.lua definition files are all missing. The files in this project are provided as-is.\nDependencies  OpenGL 3.2+ GLFW  http://www.glfw.org/   Lua  http://www.lua.org/   Premake4  https://github.com/premake/premake-4.x/wiki http://premake.github.io/download.html   GLM  http://glm.g-truc.net/0.9.7/index.html   AntTweakBar  http://anttweakbar.sourceforge.net/doc/    Notes Objectives were completed as defined by the assignment.\nThe sample.lua scene that is defined is based on the simple.lua, and uses a compositions of items from the sample lua files.\nFeatures:\n Uses a multi-threaded design to increase performance (number of threads can be specified at compile-time) Mirror reflections was the supported offical feature that was added to the project  Standing Issues:\n Currently there is an issue with the Mesh models. As it stands they only partially render (certain values do) This likely has to do with an issue with the algorithm that was being used Originally used an angle based algorithm (described here) but it did not render the objects  Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Arthur Schmitt from the Noun Project.\n","id":131,"section":"posts","summary":"A Raytracer that receives a scene defined in lua, and produces an image output.     RayTracer Summary A Raytracer that receives a scene defined in lua, and produces an image output.\nGetting Started Compilation follows the standard process defined by the UWaterloo CS488 sample projects.\nWe use premake4 as our cross-platform build system. First you will need to build all the static libraries that the projects depend on.","tags":["jrbeverly"],"title":"raytracer","uri":"/2018/11/jrbeverly-raytracer/","year":"2018"},{"content":"   This is a one page user profile for Jonathan Beverly (jrbeverly - i.e. me).     jrbeverly.profile Summary This is a one page user profile for Jonathan Beverly (jrbeverly - i.e. me), linking to multiple online identities, relevant external sites, and popular social networking websites. Not all of them are included, but most of the relevant ones are.\nBuild Process The process of minimizing the web resources is handled using the command line utility of Minify which is available here. The process is used manually as opposed to leveraging a specific build system, is to experiment with more granular controls for website compilation.\nWhy not minify with ___? The static pages are minified using Minify CLI for the simple reason of tinkering. I wanted to be able to fiddle with various ways of optimizing a static HTML project, and a one page user profile project seemed to be the perfect fit.\nDeployment The static pages can be downloaded from the pipeline artifacts, which can then be run on any web server. For example, using it with Gitlab Pages can be done by creating the project jrbeverly.gitlab.io. The .gitlab-ci.yml of the jrbeverly.gitlab.io project can download the artifacts, and deploy the artifacts to gitlab.io.\nRead more about user/group Pages and project Pages.\nTemplate The webpage is based on the template designed by mRova available here.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Daniel Gamage from the Noun Project.\n","id":132,"section":"posts","summary":"This is a one page user profile for Jonathan Beverly (jrbeverly - i.e. me), linking to multiple online identities, relevant external sites, and popular social networking websites. Not all of them are included, but most of the relevant ones are.","tags":["jrbeverly"],"title":"profile","uri":"/2018/11/jrbeverly-profile/","year":"2018"},{"content":"   Collections data from a specified list of gitlab projects, then converts them into static HTML briefs.     jrbeverly portfolio Summary Collections data from a specified list of gitlab projects, then converts them into static HTML briefs.\nGetting Started The project is designed to git clone a series of repository, then collect information from each of them. This includes the project icon, name, license, path, etc.\nFrom this information, the project will then for each repository create:\n A brief - A simple static page featuring the README.md of the project rendered in a viewdocs format A redirect link, allowing one to navigate to jrbeverly.me/ref/reponame to be redirected to the primary git repo  Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Kiryl Sytsko from the Noun Project.\n","id":133,"section":"posts","summary":"Collections data from a specified list of gitlab projects, then converts them into static HTML briefs.     jrbeverly portfolio Summary Collections data from a specified list of gitlab projects, then converts them into static HTML briefs.\nGetting Started The project is designed to git clone a series of repository, then collect information from each of them. This includes the project icon, name, license, path, etc.","tags":["jrbeverly"],"title":"exp-portfolio","uri":"/2018/11/jrbeverly-exp-portfolio/","year":"2018"},{"content":"   This build configuration installs the vagrant-desktop environments onto minimal ubuntu images that can be used with Virtualbox.     Packer Desktop Summary This build configuration installs the vagrant-desktop environments onto minimal ubuntu images that can be used with Virtualbox.\nGetting Started Make sure all the required software (listed above) is installed, then you can build the images as follows:\n# cd x11/ # cd opengl/ # cd base/ packer build ubuntu.json After a few minutes, Packer should tell you the image was generated successfully.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Arthur Schmitt from the Noun Project.\n","id":134,"section":"posts","summary":"This build configuration installs the vagrant-desktop environments onto minimal ubuntu images that can be used with Virtualbox.     Packer Desktop Summary This build configuration installs the vagrant-desktop environments onto minimal ubuntu images that can be used with Virtualbox.\nGetting Started Make sure all the required software (listed above) is installed, then you can build the images as follows:\n# cd x11/ # cd opengl/ # cd base/ packer build ubuntu.","tags":["devkitspaces"],"title":"packer-desktop","uri":"/2018/11/devkitspaces-packer-desktop/","year":"2018"},{"content":"   office-depot is a container based software development stack.     office-depot Summary office-depot is a container based software development stack.\nGetting Started Getting started is as simple as using docker-compose. You can do so as such:\ndocker-compose up --env-file=office-depot.env -d Updating and Upgrading If you wish to upgrade the container stack, you need to run the following commands:\ndocker-compose stop docker-compose rm -v docker-compose pull You can then start the docker environment.\ndocker-compose up --env-file=office-depot.env -d Cleaning After upgrading, you can be left with unused images or containers. To remove all unused containers, volumes, networks and images (both dangling and unreferenced), you can use the following command:\ndocker system prune Running this after upgrade will ensure that any dangling images are cleaned up free up valuable disk space.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Marco Olgio from the Noun Project.\n","id":135,"section":"posts","summary":"office-depot is a container based software development stack.     office-depot Summary office-depot is a container based software development stack.\nGetting Started Getting started is as simple as using docker-compose. You can do so as such:\ndocker-compose up --env-file=office-depot.env -d Updating and Upgrading If you wish to upgrade the container stack, you need to run the following commands:\ndocker-compose stop docker-compose rm -v docker-compose pull You can then start the docker environment.","tags":["jrbeverly"],"title":"office-depot","uri":"/2018/11/jrbeverly-office-depot/","year":"2018"},{"content":"   A lightweight bash script that allows easy mirroring of projects to external git hosts.     Mirroring Summary A lightweight bash script that allows easy mirroring of projects to external git hosts.\nGetting started Simply fork this repository, as it has all the scripts necessary for performing mirrors. You can then add your repositories into the assets/ directory. You will want to store them as such:\n\u0026gt; repoA/\r\u0026gt; bitbucket.config\r\u0026gt; github.config\r\u0026gt; gitlab.config\r\u0026gt; someService.config\r\u0026gt; REPO\r\u0026gt; repoB/\r\u0026gt; ...\r\u0026gt; repoC/\r\u0026gt; ...\r Each repositroy is its own directory. The REPO file is expected, and will contain only one variable, SOURCE. The source variable is used for retrieving the repository from the original source location. For each site, you can then create a \u0026lt;site\u0026gt;.config file that contains the following variables:\n|Name|Description| ||| |MIRROR_NAME|The name of the host site| |MIRROR_SSH|The ssh url of the git repository.| |MIRROR_HTTP|The http url of the git repository.|\nYou can see an example github.config file presented below:\nMIRROR_NAME=\u0026#34;github\u0026#34; MIRROR_SSH=\u0026#34;git@github.com:jrbeverly/XPlatformer.git\u0026#34; MIRROR_HTTP=\u0026#34;https://github.com/jrbeverly/XPlatformer.git\u0026#34; ","id":136,"section":"posts","summary":"A lightweight bash script that allows easy mirroring of projects to external git hosts.     Mirroring Summary A lightweight bash script that allows easy mirroring of projects to external git hosts.\nGetting started Simply fork this repository, as it has all the scripts necessary for performing mirrors. You can then add your repositories into the assets/ directory. You will want to store them as such:","tags":["jrbeverly"],"title":"mirroring","uri":"/2018/11/jrbeverly-mirroring/","year":"2018"},{"content":"   A simple experiment prototyping a concept for strongly typed language terms.     Localization.NET Concept Summary A simple experiment prototyping a concept for strongly typed language terms.\nNote: The generated component is not built with this. This is a usage prototype only (no generator is included).\nGetting Started The idea that Localization.NET is attempting to conceptualize is one where an interface is used as the primary mechanism for declaring language terms. Attributes can be used to include more contextual information (usage, type, namespace). The Roslyn compiler can then use this information to generate the underlying code to facilitate the Language terms.\nWith the underlying code generated on-the-fly, a dependency injection framework (for example: Ninject, TinyIoC) can then be used for setting the ILanguageTerms _languageTerms field.\npublic class MyGreeter { private ILanguageTerms _languageTerms; public void SayHello(string name) { Console.WriteLine(_languageTerms.Hello(name)); } [LanguageTerms(\u0026#34;myapp.greeter\u0026#34;)] public interface ILanguageTerms { [LangTerm(\u0026#34;hello\u0026#34;)] LanguageTerm Hello(string name); } } The language resources could be defined in any format, below is an example using json (mygreeter.en-CA.json):\n{ \u0026#34;myapp\u0026#34;: { \u0026#34;greeter\u0026#34;: { \u0026#34;hello\u0026#34;: \u0026#34;Hello {0}! How are you?\u0026#34; } } } Implementing new language terms is a simple process of adding the term to ILanguageTerms, with an invariant set:\npublic class MyGreeter { // .... [LanguageTerms(\u0026#34;myapp.greeter\u0026#34;)] public interface ILanguageTerms { [LangTerm(\u0026#34;no_name\u0026#34;)] [Invariant(\u0026#34;Sorry! Didn\u0026#39;t catch that name?\u0026#34;)] LanguageTerm Message { get; } } } The [Invariant] attribute can later be added into the globalaization pipeline by an automated system (powered by Roslyn).\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Percy Batalier from the Noun Project.\n","id":137,"section":"posts","summary":"A simple experiment prototyping a concept for strongly typed language terms. \u003cstrong\u003eNote: The generated component is not built with this. This is a usage prototype only (no generator is included).\u003c/strong\u003e","tags":["jrbeverly"],"title":"localization-net","uri":"/2018/11/jrbeverly-localization-net/","year":"2018"},{"content":"   A style guide for issue management, release versioning, Git Flow and repository documentation.     Issues.Style Summary A style guide for issue management, release versioning, Git Flow and repository documentation.\nDescription In order to speed up the initialization process of a new gitlab project, Issues.Style provides a set of common labels and issues that might be used when setting up a new project. The project provides methods for quickly setting up the project, specifically providing the following:\n Labels - Grouped by color, according to broad themes Setup Issues - Initialization labels, including licensing, documentation, CI and metadata.  Labels The project labels can be viewed on Gitlab in the Issues \u0026gt; Labels section, or you can view them here in labels.md.\nGetting Started First, make sure you have valid GitLab account tokens for both source and destination GitLab installations. They are used to access GitLab resources without authentication. GitLab private tokens are availble in \u0026ldquo;Profile Settings -\u0026gt; Account\u0026rdquo;.\ngitlab-copy is a simple tool for copying issues/labels/milestones/notes from one GitLab project to another, possibly running on different GitLab instances.\nUsage To copy an template issues or labels, you can use the batch copy utility gitlab-copy. You will need to create a YAML configuration file to be used by the gitlab utility gitlab-copy. You can view a sample template for copying over labels here. The configuration file will specify source and targets, along with the access tokens for each of the gitlab instances. The gitlab.yml will be of the form:\nfrom: url: https://gitlab.com token: ${TOKEN} project: jrbeverly/Issues.Style labelsOnly: true to: url: https://gitlab.com token: ${TOKEN} project: jrbeverly/new_project The above will copy just the labels from the Issue.Styles project. Using gitlab-copy, you can then copy the labels from the Issues.Style to another project. You can do this with the following command:\ngitlab-copy -y gitlab.yml If you would like to perform a smoke run, you can omit the -y flag to not execute. You can do that as such:\ngitlab-copy -y gitlab.yml Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Desbenoit from the Noun Project.\n","id":138,"section":"posts","summary":"A style guide for issue management, release versioning, Git Flow and repository documentation.","tags":["jrbeverly"],"title":"issues-style","uri":"/2018/11/jrbeverly-issues-style/","year":"2018"},{"content":"   A collection of scalable vector graphics (SVG) that define project and group icons.     jrbeverly.icons Summary A collection of scalable vector graphics (SVG) that define project and group icons.\nBuild You can build the icons using the tool rsvg-convert. To build with rsvg-convert, you can do the following:\nrsvg-convert -f svg icon.svg \u0026gt; output.svg rsvg-convert -f png icon.svg \u0026gt; output.png It is recommend to use the build scripts available in build/ or in the local source directory. These scripts are used in the build pipeline, ensuring that all arguments and attributes are set for compilation of the icons. These should be run from the root of the project directory.\nsh build/compile.sh Or if running in an environment with rsvg-convert installed (such as a docker container), you can do the following:\nsh build/build.sh Docker Environment You can start a docker container with rsvg-convert installed to experiment with building the icons. To build with rsvg-convert, you can do the following:\nsh build/run.sh Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Margot Nadot from the Noun Project.\n","id":139,"section":"posts","summary":"A collection of scalable vector graphics (SVG) that define project and group icons.","tags":["jrbeverly"],"title":"project-icons","uri":"/2018/11/jrbeverly-project-icons/","year":"2018"},{"content":"   A collection of GitLab CI configuration files that are used by my projects.     gitlab-ci.yml A collection of GitLab CI configuration files that are used by my projects. Stored here as the process of docker projects are polished and standardized.\nGetting Started Each of the dockerfiles is presented with a simple .gitlab-ci.yml file that uses one of my docker images. The resources referenced by the definition are not included in this project. You can start by copying the .gitlab-cy.yml, then replacing the relevant bits.\nstages: - build build: stage: build image: jrbeverly/minify:baseimage script: - minify -o index-min.html index.html only: - master artifacts: paths: - public/ expire_in: 1 hour Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Icons8 from the Noun Project.\n","id":140,"section":"posts","summary":"A collection of GitLab CI configuration files that are used by my projects. Stored here as the process of docker projects are polished and standardized.","tags":["jrbeverly"],"title":"gitlab-ci-yml","uri":"/2018/11/jrbeverly-gitlab-ci-yml/","year":"2018"},{"content":"   A simple experiment prototyping a concept for strongly typed ORMs.     Entity.NET Concept Summary A simple experiment prototyping a concept for strongly typed ORMs.\nGetting Started The idea that Entity.NET is attempting to conceptualize is one where strongly-typed objects are used with an ORM system. The primary objective is to use strongly typed identifiers (Keys.Customer) that restricts the usages of an applications ORM. The concept is from the following scenario:\nvar cust = new Models.Customer() { Name = \u0026#34;John Doe\u0026#34; }; var entity = Repository.Add(cust); //entity.Key =\u0026gt; underlying int (10) Repository.Update(entity.Key, new Models.Customer() { Name = \u0026#34;David Smith\u0026#34; }); //...  var existing = (Keys.Customer)10; var updatedCust = Repository.Find(entity.Key); Console.WriteLine(updatedCust.Name); //David Smith The code would be generated from the base definition of an entity below. Roslyn would generate the strongly-typed code using Attributes on the models to control the code outputs.\n[Entity(typeof(int))] class Customer { public string Name { get; set; } public string Address { get; set; } } Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Maxi Koichi from the Noun Project.\n","id":141,"section":"posts","summary":"A simple experiment prototyping a concept for strongly typed ORMs.","tags":["jrbeverly"],"title":"entity-net","uri":"/2018/11/jrbeverly-entity-net/","year":"2018"},{"content":"   [DEPRECATED] A super small image with wkhtmltopdf installed.     Dockerized WKHtmlToPDF Summary A super small image with wkhtmltopdf installed. The project icon is from cre.ativo mustard, HK from the Noun Project\nNOTE: This image is marked EOL, and use is discouraged.\nUsage You can use this image locally with docker run, calling wkhtmltopdf:\ndocker run -v $(pwd):/media/ jrbeverly/wkhtmltopdf:privileged wkhtmltopdf http://google.com google.pdf Gitlab You can setup a build job using .gitlab-ci.yml:\nbuild: image: jrbeverly/wkhtmltopdf:baseimage script: - wkhtmltopdf http://google.com google.pdf artifacts: paths: - google.pdf Components Metadata Arguments Metadata build arguments used with the Label Schema Convention.\n   Variable Value Description     BUILD_DATE see metadata.variable The Date/Time the image was built.   VERSION see metadata.variable Release identifier for the contents of the image.   VCS_REF see metadata.variable Identifier for the version of the source code from which this image was built.    Build Arguments Build arguments used in the image.\n   Variable Value Description     USER see Makefile.options Sets the user to use when running the image.   DUID see user.variable The user id of the docker user.   DGID see user.variable The group id of the docker user\u0026rsquo;s group.    Volumes No volumes are exposed by the docker container. However, while running the image with limited permissions (baseimage), it is necessary to ensure that the docker user has permission to access mounted volumes. You will need to ensure that the docker user can read/write to the mounted volumes. (see User / Group Identifiers)\nThe working directory of the image is /media/.\nBuild Process To build the docker image, use the included Makefile. It is recommended to use the makefile to ensure all build arguments are provided.\nmake VERSION=\u0026lt;version\u0026gt; build You can view the build/README.md for more on using the Makefile to build the image.\nLabels The docker image follows the Label Schema Convention. Label Schema is a community project to provide a shared namespace for use by multiple tools, specifically org.label-schema. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.label-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/wkhtmltopdf:\u0026lt;TAG\u0026gt; Label Extension The label namespace org.doc-schema is an extension of org.label-schema. The namespace stores internal variables often used when interacting with the image. These variables will often be application versions or exposed internal variables. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/wkhtmltopdf:\u0026lt;TAG\u0026gt; User and Group Mapping All processes within the baseimage docker container will be run as the docker user, a non-root user. The docker user is created on build with the user id DUID and a member of a group with group id DGID.\nAny permissions on the host operating system (OS) associated with either the user (DUID) or group (DGID) will be associated with the docker user. The values of DUID and DGID are visible in the Build Arguments, and can be accessed by the commands:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.user\u0026#34; }}\u0026#39; jrbeverly/wkhtmltopdf:baseimage docker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.group\u0026#34; }}\u0026#39; jrbeverly/wkhtmltopdf:baseimage The notation of the build variables is short form for docker user id (DUID) and docker group id (DGID).\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Creative Commons By 3.0.\nThe project icon is by cre.ativo mustard from the Noun Project.\n","id":142,"section":"posts","summary":"A super small image with \u003ca href=\"https://wkhtmltopdf.org/\"\u003ewkhtmltopdf\u003c/a\u003e installed. The project icon is from \u003ca href=\"docs/icon/icon.json\"\u003ecre.ativo mustard, HK from the Noun Project\u003c/a\u003e \u003cstrong\u003eNOTE: This image is marked EOL, and use is discouraged.\u003c/strong\u003e","tags":["cardboardci"],"title":"ci-wkhtmltopdf","uri":"/2018/11/cardboardci-ci-wkhtmltopdf/","year":"2018"},{"content":"   [DEPRECATED] A super small Alpine image with OptiPNG installed.     Dockerized Optipng Summary A super small Alpine image with OptiPNG installed. The project icon is from cre.ativo mustard, HK from the Noun Project\nNOTE: This image is marked EOL, and use is discouraged.\nUsage You can use this image locally with docker run, calling as such:\ndocker run -v $(pwd):/media/ jrbeverly/optipng:baseimage optipng test.png Gitlab You can setup a build job using .gitlab-ci.yml:\ncompile_pdf: image: jrbeverly/optipng:baseimage script: - optipng test.png artifacts: paths: - test.png Components Metadata Arguments Metadata build arguments used with the Label Schema Convention.\n   Variable Value Description     BUILD_DATE see metadata.variable The Date/Time the image was built.   VERSION see metadata.variable Release identifier for the contents of the image.   VCS_REF see metadata.variable Identifier for the version of the source code from which this image was built.    Build Arguments Build arguments used in the image.\n   Variable Value Description     USER see Makefile.options Sets the user to use when running the image.   DUID see user.variable The user id of the docker user.   DGID see user.variable The group id of the docker user\u0026rsquo;s group.    Volumes No volumes are exposed by the docker container. However, while running the image with limited permissions (baseimage), it is necessary to ensure that the docker user has permission to access mounted volumes. You will need to ensure that the docker user can read/write to the mounted volumes. (see User / Group Identifiers)\nThe working directory of the image is /media/.\nBuild Process To build the docker image, use the included Makefile. It is recommended to use the makefile to ensure all build arguments are provided.\nmake VERSION=\u0026lt;version\u0026gt; build You can view the build/README.md for more on using the Makefile to build the image.\nLabels The docker image follows the Label Schema Convention. Label Schema is a community project to provide a shared namespace for use by multiple tools, specifically org.label-schema. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.label-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/optipng Label Extension The label namespace org.doc-schema is an extension of org.label-schema. The namespace stores internal variables often used when interacting with the image. These variables will often be application versions or exposed internal variables. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/optipng User and Group Mapping All processes within the baseimage docker container will be run as the docker user, a non-root user. The docker user is created on build with the user id DUID and a member of a group with group id DGID.\nAny permissions on the host operating system (OS) associated with either the user (DUID) or group (DGID) will be associated with the docker user. The values of DUID and DGID are visible in the Build Arguments, and can be accessed by the commands:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.user\u0026#34; }}\u0026#39; jrbeverly/optipng:baseimage docker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.group\u0026#34; }}\u0026#39; jrbeverly/optipng:baseimage The notation of the build variables is short form for docker user id (DUID) and docker group id (DGID).\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Creative Commons By 3.0.\nThe project icon is by cre.ativo mustard from the Noun Project.\n","id":143,"section":"posts","summary":"A super small Alpine image with OptiPNG installed. The project icon is from \u003ca href=\"docs/icon/icon.json\"\u003ecre.ativo mustard, HK from the Noun Project\u003c/a\u003e \u003cstrong\u003eNOTE: This image is marked EOL, and use is discouraged.\u003c/strong\u003e","tags":["cardboardci"],"title":"ci-optipng","uri":"/2018/11/cardboardci-ci-optipng/","year":"2018"},{"content":"   [DEPRECATED] A super small image with Minify.     Dockerized Minify Summary A super small image with Minify. The project icon is from cre.ativo mustard, HK from the Noun Project\nNOTE: This image is marked EOL, and use is discouraged.\nUsage You can use this image locally with docker run, calling minify as such:\ndocker run -v /media/:/media/ jrbeverly/minify:privileged minify -o index-min.html index.html Gitlab You can setup a build job using .gitlab-ci.yml:\ncompile: image: jrbeverly/minify:baseimage script: - minify -o index-min.html index.html artifacts: paths: - index.html Components Metadata Arguments Metadata build arguments used with the Label Schema Convention.\n   Variable Value Description     BUILD_DATE see metadata.variable The Date/Time the image was built.   VERSION see metadata.variable Release identifier for the contents of the image.   VCS_REF see metadata.variable Identifier for the version of the source code from which this image was built.    Build Arguments Build arguments used in the image.\n   Variable Value Description     USER see Makefile.options Sets the user to use when running the image.   DUID see user.variable The user id of the docker user.   DGID see user.variable The group id of the docker user\u0026rsquo;s group.    Volumes No volumes are exposed by the docker container. However, while running the image with limited permissions (baseimage), it is necessary to ensure that the docker user has permission to access mounted volumes. You will need to ensure that the docker user can read/write to the mounted volumes. (see User / Group Identifiers)\nThe working directory of the image is /media/.\nBuild Process To build the docker image, use the included Makefile. It is recommended to use the makefile to ensure all build arguments are provided.\nmake VERSION=\u0026lt;version\u0026gt; build You can view the build/README.md for more on using the Makefile to build the image.\nLabels The docker image follows the Label Schema Convention. Label Schema is a community project to provide a shared namespace for use by multiple tools, specifically org.label-schema. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.label-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/minify Label Extension The label namespace org.doc-schema is an extension of org.label-schema. The namespace stores internal variables often used when interacting with the image. These variables will often be application versions or exposed internal variables. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/minify User and Group Mapping All processes within the baseimage docker container will be run as the docker user, a non-root user. The docker user is created on build with the user id DUID and a member of a group with group id DGID.\nAny permissions on the host operating system (OS) associated with either the user (DUID) or group (DGID) will be associated with the docker user. The values of DUID and DGID are visible in the Build Arguments, and can be accessed by the commands:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.user\u0026#34; }}\u0026#39; jrbeverly/minify:baseimage docker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.group\u0026#34; }}\u0026#39; jrbeverly/minify:baseimage The notation of the build variables is short form for docker user id (DUID) and docker group id (DGID).\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Creative Commons By 3.0.\nThe project icon is by cre.ativo mustard from the Noun Project.\n","id":144,"section":"posts","summary":"A super small image with Minify. The project icon is from \u003ca href=\"docs/icon/icon.json\"\u003ecre.ativo mustard, HK from the Noun Project\u003c/a\u003e \u003cstrong\u003eNOTE: This image is marked EOL, and use is discouraged.\u003c/strong\u003e","tags":["cardboardci"],"title":"ci-minify","uri":"/2018/11/cardboardci-ci-minify/","year":"2018"},{"content":"   A multi-client, multi-server environment that relies on a binder to facilitate an RPC system.     DistributedRPC Summary A multi-client, multi-server environment that relies on a binder to facilitate an RPC system.\nGetting Started To make (\u0026ldquo;compile and link\u0026rdquo;) all components of the project, you can quickly get started with\nmake exec Or if you are doing quick debugging\nmake exec \u0026amp;\u0026amp; ./binder You can also invidiaully build each component with make \u0026lt;component\u0026gt;.\nNotes The project is over-commented to explain each line of code. This is for the purposes of explaining how the overall project connects together.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by marcela contreras from the Noun Project.\n","id":145,"section":"posts","summary":"A multi-client, multi-server environment that relies on a binder to facilitate an RPC system.","tags":["jrbeverly"],"title":"distributedrpc","uri":"/2018/11/jrbeverly-distributedrpc/","year":"2018"},{"content":"   A proof of concept for generation of strongly typed paths using the Roslyn Framework.     ContentBundler Summary A proof of concept for generation of strongly typed paths using the Roslyn Framework.\nGetting Started ContentBundler is provided as an command line application, originally adapted from an XNA Content Compiler. The new version greatly simplifies the code requirements, leveraging Roslyn for the code generation. An example is available from test/assets, which will generate the result [PatchQ.cs]\n./ContentBundler.exe --archive patch-Q.zip --file PatchQ.cs --class PatchQ --namespace PlatformerGame.Assets This will then output a series of static classes.\nnamespace PlatformerGame.Assets { using System; public static class PatchQ { public static class Textures { public readonly static string Tex0 = \u0026#34;textures/tex0.png\u0026#34;; public readonly static string Tex01 = \u0026#34;textures/tex01.png\u0026#34;; public readonly static string Tex02 = \u0026#34;textures/tex02.png\u0026#34;; public readonly static string Tex03 = \u0026#34;textures/tex03.png\u0026#34;; public readonly static string Tex04 = \u0026#34;textures/tex04.png\u0026#34;; public static class Backgrounds { public readonly static string Background = \u0026#34;textures/backgrounds/background.png\u0026#34;; } } public static class Images { public readonly static string Icon = \u0026#34;images/icon.png\u0026#34;; public readonly static string Logo = \u0026#34;images/logo.png\u0026#34;; } public static class Sounds { public readonly static string Day01 = \u0026#34;sounds/day01.wav\u0026#34;; public readonly static string Day02 = \u0026#34;sounds/day02.wav\u0026#34;; public readonly static string Day03 = \u0026#34;sounds/day03.wav\u0026#34;; public readonly static string Day04 = \u0026#34;sounds/day04.wav\u0026#34;; public readonly static string Night01 = \u0026#34;sounds/night01.wav\u0026#34;; public readonly static string Night02 = \u0026#34;sounds/night02.wav\u0026#34;; public readonly static string Night03 = \u0026#34;sounds/night03.wav\u0026#34;; public readonly static string Night04 = \u0026#34;sounds/night04.wav\u0026#34;; public readonly static string Night05 = \u0026#34;sounds/night05.wav\u0026#34;; } } } Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by jeanbaptiste sautelet from the Noun Project.\n","id":146,"section":"posts","summary":"A proof of concept for generation of strongly typed paths using the Roslyn Framework.     ContentBundler Summary A proof of concept for generation of strongly typed paths using the Roslyn Framework.\nGetting Started ContentBundler is provided as an command line application, originally adapted from an XNA Content Compiler. The new version greatly simplifies the code requirements, leveraging Roslyn for the code generation. An example is available from test/assets, which will generate the result [PatchQ.","tags":["jrbeverly"],"title":"contentbundler","uri":"/2018/11/jrbeverly-contentbundler/","year":"2018"},{"content":"   A set of chocolatey packages for setting up a Windows PC.     Boxstarter.Workspace Summary A set of chocolatey packages for setting up a Windows PC.\nGetting Started The packages are not available on chocolatey.org, so they will need to be manually built. You can pack them up using the following:\ncd src/baseenv choco pack This will create a package.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Five by Five from the Noun Project.\n","id":147,"section":"posts","summary":"A set of chocolatey packages for setting up a Windows PC.     Boxstarter.Workspace Summary A set of chocolatey packages for setting up a Windows PC.\nGetting Started The packages are not available on chocolatey.org, so they will need to be manually built. You can pack them up using the following:\ncd src/baseenv choco pack This will create a package.\nAcknowledgements The project icon is retrieved from the Noun Project.","tags":["devkitspaces"],"title":"boxstarter-workspace","uri":"/2018/11/devkitspaces-boxstarter-workspace/","year":"2018"},{"content":"   Mobile Flashcards.     UdaciCards Summary For the UdaciCards project, you will build a mobile application (Android or iOS - or both) that allows users to study collections of flashcards. The app will allow users to create different categories of flashcards called \u0026ldquo;decks\u0026rdquo;, add flashcards to those decks, then take quizzes on those decks.\nInstallation You can install the project dependencies using:\ncd src/udacicards/ yarn The you can start the application:\nyarn start To troubleshoot the issues of the application, you can review React Native - Getting Started.\nSupported OS Application was tested using Android Emulators.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Tim Holman from the Noun Project.\n","id":148,"section":"posts","summary":"For the UdaciCards project, you will build a mobile application (Android or iOS - or both) that allows users to study collections of flashcards. The app will allow users to create different categories of flashcards called \u0026ldquo;decks\u0026rdquo;, add flashcards to those decks, then take quizzes on those decks.","tags":["jrbeverly"],"title":"udacicards","uri":"/2018/04/jrbeverly-udacicards/","year":"2018"},{"content":"   A Prisoners' Dilemma A.I. bot performing an \u0026lsquo;Olive Branch\u0026rsquo; strategy focusing on attempting to cooperate whenever possible.     JollyBot Summary Prisoners' Dilemma A.I. bot performing an \u0026lsquo;Olive Branch\u0026rsquo; strategy focusing on attempting to cooperate whenever possible. The bot attempts to establish cooperation, even in cases where the opposing agent may appear hostile (e.g. always defect).\nDescription The iterated prisoners dilemma is a classic two-person game which consists of a number of rounds. In each round, each person can either defect by taking $1 from a (common) pile, or cooperate by giving $2 from the same pile to the other person. A purely rational agent will optimise over his expected long-term payoffs, possibly by averaging over his expectations of his opponents type (or strategy).\nJollyBot works by always attempting to cooperate with the opposing agent, attempting to establish mutual cooperation between the agents whenever reasonable. Even in the case of a more aggressive agent, JollyBot tries to remain jolly. The bot was developed as part of an assignment in an Artificial Intelligence course, with the goal of ultimately participating in a tournament against other submitted agents. JollyBot faced off against the other agents, emerging victorious by being a really swell fella.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Matt Brooks from the Noun Project.\n","id":149,"section":"posts","summary":"A Prisoners' Dilemma A.I. bot performing an \u0026lsquo;Olive Branch\u0026rsquo; strategy focusing on attempting to cooperate whenever possible.     JollyBot Summary Prisoners' Dilemma A.I. bot performing an \u0026lsquo;Olive Branch\u0026rsquo; strategy focusing on attempting to cooperate whenever possible. The bot attempts to establish cooperation, even in cases where the opposing agent may appear hostile (e.g. always defect).\nDescription The iterated prisoners dilemma is a classic two-person game which consists of a number of rounds.","tags":["jrbeverly"],"title":"jollybot","uri":"/2018/04/jrbeverly-jollybot/","year":"2018"},{"content":"   Udacity Nanodegree React Project.     Readable Udacity Nanodegree React Project\nInstalling Server  cd src/api npm install node start  Installing Client  cd src/client npm install npm start  API Server Information about the API server and how to use it can be found in its README file.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Srikanta from the Noun Project.\n","id":150,"section":"posts","summary":"Udacity Nanodegree React Project","tags":["jrbeverly"],"title":"readable","uri":"/2018/01/jrbeverly-readable/","year":"2018"},{"content":"   A digital bookshelf app that allows you to select and categorize books you have read, are currently reading, or want to read.     MyReads Project Summary A digital bookshelf app that allows you to select and categorize books you have read, are currently reading, or want to read.\nDescription MyReads is a digital bookshelf app that allows you to select and categorize books you have read, are currently reading, or want to read. It is built as a project for Udacity React Nanodegree.\nGetting Started The backend API uses a fixed set of cached search results and is limited to a particular set of search terms, which can be found in SEARCH_TERMS.md. That list of terms are the only terms that will work with the backend, so don\u0026rsquo;t be surprised if your searches for Basket Weaving or Bubble Wrap don\u0026rsquo;t come back with any results.\nBuild To start development with the project, you will need to run the following command:\nnpm install This will install all node modules that the project relies on. You can then start the web server using the following command;\nnpm start You can then open localhost:3000 to view the MyReads in the browser. The page will automatically reload if you make changes to the code.\nDependencies The project relies on the installation on Node.js.\nReact This project was bootstrapped with Create React App. You can find more information on how to perform common tasks here.\nBooks Web Service The MyReads application makes use of the Udacity React Books API. The service provides methods for searching a catalog of books based on a predefined list of terms. These terms are available at docs/SEARCH_TERMS.md.\nYou can read more about the web service at docs/API.md.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Marjon Siero from the Noun Project.\n","id":151,"section":"posts","summary":"A digital bookshelf app that allows you to select and categorize books you have read, are currently reading, or want to read.","tags":["jrbeverly"],"title":"myreads","uri":"/2017/11/jrbeverly-myreads/","year":"2017"},{"content":"   This repository provides a base Docker Desktop environment, sandboxed on your local computer.     Vagrant Docker Desktop Summary Provide a method of reproducible graphical development environments based on Linux. This repository provides a base Docker Desktop environment, sandboxed on your local computer.\nGetting Started You can use this locally with vagrant up, calling as such:\nvagrant --name=mydesktop up However It is recommended to use the script create.sh for the first run to ensure all necessary arguments are provided. The provided arguments creates a settings.yaml, storing the settings for the machine. You can create the machine by calling:\nsh create.sh -n mydesktop -d ubuntu If you want more information about the script create.sh, you can do so by calling:\nsh create.sh -h Parameters The parameters are used in the calling of vagrant up, primarily as vagrant [OPTIONS] up. After provisioning the environment, a settings file (setting.yaml) is created, which stores the provided parameters.\n   Name Type Description     name string Name of the provisioned desktop environment   desktop filename The name of the desktop provisioning script. These scripts are present in packaging/environments.    The vagrant environment is based on the bento/ubuntu images. If the timezone is not set, the provision script will attempt to auto-detect the timezone using tzupdate.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Maxi Koichi from the Noun Project.\n","id":152,"section":"posts","summary":"Provide a method of reproducible graphical development environments based on Linux. This repository provides a base Docker Desktop environment, sandboxed on your local computer.","tags":["cardboardci"],"title":"vagrant-desktop-docker","uri":"/2017/09/cardboardci-vagrant-desktop-docker/","year":"2017"},{"content":"   This repository provides a base Homelab Desktop environment, sandboxed on your local computer.     Vagrant Homelab Desktop Summary Provide a method of reproducible graphical development environments based on Linux. This repository provides a base Homelab Desktop environment, sandboxed on your local computer.\nGetting Started You can use this locally with vagrant up, calling as such:\nvagrant --name=mydesktop up However It is recommended to use the script create.sh for the first run to ensure all necessary arguments are provided. The provided arguments creates a settings.yaml, storing the settings for the machine. You can create the machine by calling:\nsh create.sh -n mydesktop -d ubuntu If you want more information about the script create.sh, you can do so by calling:\nsh create.sh -h Parameters The parameters are used in the calling of vagrant up, primarily as vagrant [OPTIONS] up. After provisioning the environment, a settings file (setting.yaml) is created, which stores the provided parameters.\n   Name Type Description     name string Name of the provisioned desktop environment   desktop filename The name of the desktop provisioning script. These scripts are present in packaging/environments.    The vagrant environment is based on the bento/ubuntu images. If the timezone is not set, the provision script will attempt to auto-detect the timezone using tzupdate.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Maxi Koichi from the Noun Project.\n","id":153,"section":"posts","summary":"Provide a method of reproducible graphical development environments based on Linux. This repository provides a base Homelab Desktop environment, sandboxed on your local computer.","tags":["devkitspaces"],"title":"vagrant-desktop-homelab","uri":"/2017/09/devkitspaces-vagrant-desktop-homelab/","year":"2017"},{"content":"   GitHooks provides a multi-hook framework for Git Hooks.     Githooks Summary GitHooks provides a multi-hook framework for Git Hooks, along with a collection of scripts for the purposes of encouraging a commit policy, altering the project environment depending on the state of the repository, and implementing continuous integration workflows. The framework allows multi-script execution, you can use GitHooks to automate or optimize virtually any aspect of your development workflow.\nGetting Started Git Hooks are event-based scripts you can place in a hooks directory to trigger actions at certain points in gits execution. When you run certain git commands, the software will run the associated script within the git repository. GitHooks extends on this by enabling the installation of any arbitrary number of hooks for a command.\nInstallation You can install hooks in a git repository by two methods: unzipping then deleting, or manually copying in the scripts. Either option you will need to use the command line to setup your git repository, however it is recommended to use the unzip method as it is less likely to encounter mistakes (forgetting to set execution bit, missing a script, etc).\nUnzip githooks You can quickly install a set of hooks into the git repository by unzipping githooks.zip into the .git/hooks/ directory. To install all hooks and sub-hooks, you can do so with the following:\nunzip githooks.zip -d .git/hooks/ chmod +x .git/hooks/* You can then delete any hooks from the .git/hooks/ directory that you do not wish to use.\nManual Copying To manually install a git hook, you will need to start by copying the hook into the .git/hooks/ directory. The git hook, also known as the entrypoint hook, is responsible for executing scripts in a sub-directory. If you wish to install the commit-msg hook, you can do the following:\ncp commit-msg .git/hooks/commit-msg mkdir -p .git/hooks/commit-msg.d/ chmod +x .git/hooks/commit-msg After copying in the entrypoint hook, you will be able to copy hooks into a sub-directory named after the hook (e.g. commit-msg.d/). These hooks will be run by the entrypoint hook, commit-msg. To add a hook, you can do so with the following:\ncp 001-my-githook.sh .git/hooks/commit-msg.d/ This will install the hook 001-my-githook.sh into the commit-msg.d/ directory. When the entrypoint commit-msg is executed, it will call any scripts in the commit-msg.d/ directory. The entrypoint hook can be of the form of any supported git hook (applypatch-msg, commit-msg, post-update, pre-applypatch, pre-commit, etc).\nAcknowledgements The project icon is by Mario Gallego Adn from the Noun Project.\n","id":154,"section":"posts","summary":"GitHooks provides a multi-hook framework for Git Hooks, along with a collection of scripts for the purposes of encouraging a commit policy, altering the project environment depending on the state of the repository, and implementing continuous integration workflows. The framework allows multi-script execution, you can use GitHooks to automate or optimize virtually any aspect of your development workflow.","tags":["jrbeverly"],"title":"githooks","uri":"/2017/09/jrbeverly-githooks/","year":"2017"},{"content":"   [DEPRECATED] A super small image with basic development libraries installed.     Docker Alpine Base Summary A super small image with basic development libraries installed. The project icon is from cre.ativo mustard, HK from the Noun Project\nNOTE: This image is marked EOL, and use is discouraged.\nUsage You can use this image locally with docker run, calling sh to enter the container:\ndocker run -it -v /media/:/media/ jrbeverly/alpine:edge sh Gitlab You can setup a build job using .gitlab-ci.yml:\ncompile: image: jrbeverly/alpine:edge script: - echo \u0026#34;Hello world\u0026#34; Components Metadata Arguments Metadata build arguments used with the Label Schema Convention.\n   Variable Value Description     BUILD_DATE see metadata.variable The Date/Time the image was built.   VERSION see metadata.variable Release identifier for the contents of the image.   VCS_REF see metadata.variable Identifier for the version of the source code from which this image was built.    Build Arguments Build arguments used in the image.\n   Variable Value Description     USER see Makefile.options Sets the user to use when running the image.   DUID see user.variable The user id of the docker user.   DGID see user.variable The group id of the docker user\u0026rsquo;s group.    Volumes No volumes are exposed by the docker container. However, while running the image with limited permissions (baseimage), it is necessary to ensure that the docker user has permission to access mounted volumes. You will need to ensure that the docker user can read/write to the mounted volumes.\nThe working directory of the image is /media/.\nBuild Process To build the docker image, use the included Makefile. It is recommended to use the makefile to ensure all build arguments are provided.\nmake VERSION=\u0026lt;version\u0026gt; build You can view the build/README.md for more on using the Makefile to build the image.\nLabels The docker image follows the Label Schema Convention. Label Schema is a community project to provide a shared namespace for use by multiple tools, specifically org.label-schema. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.label-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/alpine Label Extension The label namespace org.doc-schema is an extension of org.label-schema. The namespace stores internal variables often used when interacting with the image. These variables will often be application versions or exposed internal variables. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/alpine Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Creative Commons By 3.0.\nThe project icon is by cre.ativo mustard from the Noun Project.\n","id":155,"section":"posts","summary":"[DEPRECATED] A super small image with basic development libraries installed.     Docker Alpine Base Summary A super small image with basic development libraries installed. The project icon is from cre.ativo mustard, HK from the Noun Project\nNOTE: This image is marked EOL, and use is discouraged.\nUsage You can use this image locally with docker run, calling sh to enter the container:\ndocker run -it -v /media/:/media/ jrbeverly/alpine:edge sh Gitlab You can setup a build job using .","tags":["cardboardci"],"title":"ci-alpine","uri":"/2017/05/cardboardci-ci-alpine/","year":"2017"},{"content":"   [DEPRECATED] A super small image with pdf2htmlEX installed.     Dockerized pdf2htmlEX Summary A super small image with pdf2htmlEX installed. The project icon is from cre.ativo mustard, HK from the Noun Project\nNOTE: This image is marked EOL, and use is discouraged.\nUsage You can use this image locally with docker run, calling pdf2htmlEX:\ndocker run -v /media/:/media/ jrbeverly/pdf2htmlEX:privileged pdf2htmlEX report.pdf Gitlab You can setup a build job using .gitlab-ci.yml:\ncompile: image: jrbeverly/pdf2htmlEX:baseimage script: - pdf2htmlEX report.pdf artifacts: paths: - report.pdf Components Metadata Arguments Metadata build arguments used with the Label Schema Convention.\n   Variable Value Description     BUILD_DATE see metadata.variable The Date/Time the image was built.   VERSION see metadata.variable Release identifier for the contents of the image.   VCS_REF see metadata.variable Identifier for the version of the source code from which this image was built.    Build Arguments Build arguments used in the image.\n   Variable Value Description     USER see Makefile.options Sets the user to use when running the image.   DUID see user.variable The user id of the docker user.   DGID see user.variable The group id of the docker user\u0026rsquo;s group.    Volumes No volumes are exposed by the docker container. However, while running the image with limited permissions (baseimage), it is necessary to ensure that the docker user has permission to access mounted volumes. You will need to ensure that the docker user can read/write to the mounted volumes. (see User / Group Identifiers)\nThe working directory of the image is /media/.\nBuild Process To build the docker image, use the included Makefile. It is recommended to use the makefile to ensure all build arguments are provided.\nmake VERSION=\u0026lt;version\u0026gt; build You can view the build/README.md for more on using the Makefile to build the image.\nLabels The docker image follows the Label Schema Convention. Label Schema is a community project to provide a shared namespace for use by multiple tools, specifically org.label-schema. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.label-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/pdf2htmlEX Label Extension The label namespace org.doc-schema is an extension of org.label-schema. The namespace stores internal variables often used when interacting with the image. These variables will often be application versions or exposed internal variables. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/pdf2htmlEX User and Group Mapping All processes within the baseimage docker container will be run as the docker user, a non-root user. The docker user is created on build with the user id DUID and a member of a group with group id DGID.\nAny permissions on the host operating system (OS) associated with either the user (DUID) or group (DGID) will be associated with the docker user. The values of DUID and DGID are visible in the Build Arguments, and can be accessed by the commands:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.user\u0026#34; }}\u0026#39; jrbeverly/pdf2htmlEX:baseimage docker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.group\u0026#34; }}\u0026#39; jrbeverly/pdf2htmlEX:baseimage The notation of the build variables is short form for docker user id (DUID) and docker group id (DGID).\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Creative Commons By 3.0.\nThe project icon is by cre.ativo mustard from the Noun Project.\n","id":156,"section":"posts","summary":"A super small image with pdf2htmlEX installed. The project icon is from \u003ca href=\"docs/icon/icon.json\"\u003ecre.ativo mustard, HK from the Noun Project\u003c/a\u003e \u003cstrong\u003eNOTE: This image is marked EOL, and use is discouraged.\u003c/strong\u003e","tags":["cardboardci"],"title":"ci-pdf2htmlex","uri":"/2017/05/cardboardci-ci-pdf2htmlex/","year":"2017"},{"content":"   [DEPRECATED] A super small image with glibc installed, to allow binaries compiled against glibc to work.     Dockerized GLibC Summary A super small image with glibc installed, to allow binaries compiled against glibc to work. The project icon is from cre.ativo mustard, HK from the Noun Project\nNOTE: This image is marked EOL, and use is discouraged.\nUsage You can use this image locally with docker run, calling sh to enter the container:\ndocker run -v /media/:/media/ jrbeverly/glibc:privileged echo \u0026#34;hello\u0026#34; Gitlab You can setup a build job using .gitlab-ci.yml:\ncompile: image: jrbeverly/glibc:baseimage script: - echo \u0026#34;hello\u0026#34; Components Metadata Arguments Metadata build arguments used with the Label Schema Convention.\n   Variable Value Description     BUILD_DATE see metadata.variable The Date/Time the image was built.   VERSION see metadata.variable Release identifier for the contents of the image.   VCS_REF see metadata.variable Identifier for the version of the source code from which this image was built.    Build Arguments Build arguments used in the image.\n   Variable Value Description     USER see Makefile.options Sets the user to use when running the image.   DUID see user.variable The user id of the docker user.   DGID see user.variable The group id of the docker user\u0026rsquo;s group.    Volumes No volumes are exposed by the docker container. However, while running the image with limited permissions (baseimage), it is necessary to ensure that the docker user has permission to access mounted volumes. You will need to ensure that the docker user can read/write to the mounted volumes. (see User / Group Identifiers)\nThe working directory of the image is /media/.\nBuild Process To build the docker image, use the included Makefile. It is recommended to use the makefile to ensure all build arguments are provided.\nmake VERSION=\u0026lt;version\u0026gt; build You can view the build/README.md for more on using the Makefile to build the image.\nLabels The docker image follows the Label Schema Convention. Label Schema is a community project to provide a shared namespace for use by multiple tools, specifically org.label-schema. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.label-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/glibc Label Extension The label namespace org.doc-schema is an extension of org.label-schema. The namespace stores internal variables often used when interacting with the image. These variables will often be application versions or exposed internal variables. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/glibc User and Group Mapping All processes within the baseimage docker container will be run as the docker user, a non-root user. The docker user is created on build with the user id DUID and a member of a group with group id DGID.\nAny permissions on the host operating system (OS) associated with either the user (DUID) or group (DGID) will be associated with the docker user. The values of DUID and DGID are visible in the Build Arguments, and can be accessed by the commands:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.user\u0026#34; }}\u0026#39; jrbeverly/glibc:baseimage docker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.group\u0026#34; }}\u0026#39; jrbeverly/glibc:baseimage The notation of the build variables is short form for docker user id (DUID) and docker group id (DGID).\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Creative Commons By 3.0.\nThe project icon is by cre.ativo mustard from the Noun Project.\n","id":157,"section":"posts","summary":"A super small image with glibc installed, to allow binaries compiled against glibc to work. The project icon is from \u003ca href=\"docs/icon/icon.json\"\u003ecre.ativo mustard, HK from the Noun Project\u003c/a\u003e \u003cstrong\u003eNOTE: This image is marked EOL, and use is discouraged.\u003c/strong\u003e","tags":["cardboardci"],"title":"ci-glibc","uri":"/2017/05/cardboardci-ci-glibc/","year":"2017"},{"content":"   A super small image with X Window System development libraries installed.     Dockerized X Window System Summary A super small image with X Window System development libraries installed. The project icon is from cre.ativo mustard, HK from the Noun Project\nNOTE: This image is marked EOL, and use is discouraged.\nUsage You can use this image locally with docker run, calling g++ to build X Window System applications:\ndocker run -v $(pwd):/media/ jrbeverly/xwindow:privileged g++ myxapp.cpp -o xapp Gitlab You can setup a build job using .gitlab-ci.yml:\nbuild: image: jrbeverly/xwindow:baseimage script: - g++ myxapp.cpp -o xapp artifacts: paths: - xapp Components Metadata Arguments Metadata build arguments used with the Label Schema Convention.\n   Variable Value Description     BUILD_DATE see metadata.variable The Date/Time the image was built.   VERSION see metadata.variable Release identifier for the contents of the image.   VCS_REF see metadata.variable Identifier for the version of the source code from which this image was built.    Build Arguments Build arguments used in the image.\n   Variable Value Description     USER see Makefile.options Sets the user to use when running the image.   DUID see user.variable The user id of the docker user.   DGID see user.variable The group id of the docker user\u0026rsquo;s group.    Volumes No volumes are exposed by the docker container. However, while running the image with limited permissions (baseimage), it is necessary to ensure that the docker user has permission to access mounted volumes. You will need to ensure that the docker user can read/write to the mounted volumes. (see User / Group Identifiers)\nThe working directory of the image is /media/.\nBuild Process To build the docker image, use the included Makefile. It is recommended to use the makefile to ensure all build arguments are provided.\nmake VERSION=\u0026lt;version\u0026gt; build You can view the build/README.md for more on using the Makefile to build the image.\nLabels The docker image follows the Label Schema Convention. Label Schema is a community project to provide a shared namespace for use by multiple tools, specifically org.label-schema. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.label-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/xwindow:\u0026lt;TAG\u0026gt; Label Extension The label namespace org.doc-schema is an extension of org.label-schema. The namespace stores internal variables often used when interacting with the image. These variables will often be application versions or exposed internal variables. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/xwindow:\u0026lt;TAG\u0026gt; User and Group Mapping All processes within the baseimage docker container will be run as the docker user, a non-root user. The docker user is created on build with the user id DUID and a member of a group with group id DGID.\nAny permissions on the host operating system (OS) associated with either the user (DUID) or group (DGID) will be associated with the docker user. The values of DUID and DGID are visible in the Build Arguments, and can be accessed by the commands:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.user\u0026#34; }}\u0026#39; jrbeverly/xwindow:baseimage docker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.group\u0026#34; }}\u0026#39; jrbeverly/xwindow:baseimage The notation of the build variables is short form for docker user id (DUID) and docker group id (DGID).\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Creative Commons By 3.0.\nThe project icon is by cre.ativo mustard from the Noun Project.\n","id":158,"section":"posts","summary":"A super small image with \u003ca href=\"https://www.x.org/wiki/\"\u003eX Window System\u003c/a\u003e development libraries installed. The project icon is from \u003ca href=\"docs/icon/icon.json\"\u003ecre.ativo mustard, HK from the Noun Project\u003c/a\u003e \u003cstrong\u003eNOTE: This image is marked EOL, and use is discouraged.\u003c/strong\u003e","tags":["xplatformer"],"title":"docker-xwindow","uri":"/2017/05/xplatformer-docker-xwindow/","year":"2017"},{"content":"   [DEPRECATED] Comprehensive TeX document production system for use as a continuous integration image.     Dockerized LaTeX Summary Comprehensive TeX document production system for use as a continuous integration image. The project icon is from cre.ativo mustard, HK from the Noun Project\nNOTE: This image is marked EOL, and use is discouraged.\nUsage You can use this image locally with docker run, calling latexmk as such:\ndocker run -v /media:/media jrbeverly/latex:full latexmk -pdf manual.tex Gitlab You can setup a build job using .gitlab-ci.yml:\ncompile_pdf: image: jrbeverly/latex:full script: - latexmk -pdf manual.tex artifacts: paths: - manual.pdf Components Metadata Arguments Metadata build arguments used with the Label Schema Convention.\n   Variable Value Description     BUILD_DATE see metadata.variable The Date/Time the image was built.   VERSION see metadata.variable Release identifier for the contents of the image.   VCS_REF see metadata.variable Identifier for the version of the source code from which this image was built.    Build Arguments Build arguments used in the image.\n   Variable Value Description     USER see Makefile.options Sets the user to use when running the image.   DUID see user.variable The user id of the docker user.   DGID see user.variable The group id of the docker user\u0026rsquo;s group.    Volumes No volumes are exposed by the docker container. However, while running the image with limited permissions (baseimage), it is necessary to ensure that the docker user has permission to access mounted volumes. You will need to ensure that the docker user can read/write to the mounted volumes. (see User / Group Identifiers)\nThe working directory of the image is /media/.\nBuild Process To build the docker image, use the included Makefile. It is recommended to use the makefile to ensure all build arguments are provided.\nmake VERSION=\u0026lt;version\u0026gt; build You can view the build/README.md for more on using the Makefile to build the image.\nLabels The docker image follows the Label Schema Convention. Label Schema is a community project to provide a shared namespace for use by multiple tools, specifically org.label-schema. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.label-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/latex Label Extension The label namespace org.doc-schema is an extension of org.label-schema. The namespace stores internal variables often used when interacting with the image. These variables will often be application versions or exposed internal variables. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/latex User and Group Mapping All processes within the baseimage docker container will be run as the docker user, a non-root user. The docker user is created on build with the user id DUID and a member of a group with group id DGID.\nAny permissions on the host operating system (OS) associated with either the user (DUID) or group (DGID) will be associated with the docker user. The values of DUID and DGID are visible in the Build Arguments, and can be accessed by the commands:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.user\u0026#34; }}\u0026#39; jrbeverly/latex:baseimage docker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.group\u0026#34; }}\u0026#39; jrbeverly/latex:baseimage The notation of the build variables is short form for docker user id (DUID) and docker group id (DGID).\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Creative Commons By 3.0.\nThe project icon is by cre.ativo mustard from the Noun Project.\n","id":159,"section":"posts","summary":"Comprehensive TeX document production system for use as a continuous integration image. The project icon is from \u003ca href=\"docs/icon/icon.json\"\u003ecre.ativo mustard, HK from the Noun Project\u003c/a\u003e \u003cstrong\u003eNOTE: This image is marked EOL, and use is discouraged.\u003c/strong\u003e","tags":["cardboardci"],"title":"ci-latex","uri":"/2017/05/cardboardci-ci-latex/","year":"2017"},{"content":"   [DEPRECATED] A boilerplate template for specifying a docker image using the makefile build approach.     Boilerplate for Docker Repository Summary A boilerplate template for specifying a docker image using the makefile build approach. The project icon is from cre.ativo mustard, HK from the Noun Project\nThis model of creating docker image has been deprecated.\nComponents Metadata Arguments Metadata build arguments used with the Label Schema Convention.\n   Variable Value Description     BUILD_DATE see metadata.variable The Date/Time the image was built.   VERSION see metadata.variable Release identifier for the contents of the image.   VCS_REF see metadata.variable Identifier for the version of the source code from which this image was built.    Build Arguments Build arguments used in the image.\n   Variable Value Description     USER see Makefile.options Sets the user to use when running the image.   DUID see user.variable The user id of the docker user.   DGID see user.variable The group id of the docker user\u0026rsquo;s group.    Volumes No volumes are exposed by the docker container. However, while running the image with limited permissions (baseimage), it is necessary to ensure that the docker user has permission to access mounted volumes. You will need to ensure that the docker user can read/write to the mounted volumes. (see User / Group Identifiers)\nThe working directory of the image is /media/.\nBuild Process To build the docker image, use the included Makefile. It is recommended to use the makefile to ensure all build arguments are provided.\nmake VERSION=\u0026lt;version\u0026gt; build You can view the build/README.md for more on using the Makefile to build the image.\nLabels The docker image follows the Label Schema Convention. Label Schema is a community project to provide a shared namespace for use by multiple tools, specifically org.label-schema. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.label-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; [IMAGE] Label Extension The label namespace org.doc-schema is an extension of org.label-schema. The namespace stores internal variables often used when interacting with the image. These variables will often be application versions or exposed internal variables. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; [IMAGE] User and Group Mapping All processes within the baseimage docker container will be run as the docker user, a non-root user. The docker user is created on build with the user id DUID and a member of a group with group id DGID.\nAny permissions on the host operating system (OS) associated with either the user (DUID) or group (DGID) will be associated with the docker user. The values of DUID and DGID are visible in the Build Arguments, and can be accessed by the commands:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.user\u0026#34; }}\u0026#39; [IMAGE] docker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.group\u0026#34; }}\u0026#39; [IMAGE] The notation of the build variables is short form for docker user id (DUID) and docker group id (DGID).\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Creative Commons By 3.0.\nThe project icon is by cre.ativo mustard from the Noun Project.\n","id":160,"section":"posts","summary":"[DEPRECATED] A boilerplate template for specifying a docker image using the makefile build approach.     Boilerplate for Docker Repository Summary A boilerplate template for specifying a docker image using the makefile build approach. The project icon is from cre.ativo mustard, HK from the Noun Project\nThis model of creating docker image has been deprecated.\nComponents Metadata Arguments Metadata build arguments used with the Label Schema Convention.","tags":["cardboardci"],"title":"ci-boilerplate","uri":"/2017/05/cardboardci-ci-boilerplate/","year":"2017"},{"content":"   [DEPRECATED] A super small image to act as a baseimage for continuous integration images.     Docker Baseimage Summary A super small image to act as a baseimage for continuous integration images. The project icon is from cre.ativo mustard, HK from the Noun Project\nNOTE: This image is marked EOL, and use is discouraged.\nUsage You can use this image locally with docker run, calling sh to enter the container:\ndocker run -it -v /media/:/media/ jrbeverly/baseimage:baseimage echo \u0026#34;hello\u0026#34; Gitlab You can setup a build job using .gitlab-ci.yml:\ncompile: image: jrbeverly/baseimage:alpine script: - echo \u0026#34;hello\u0026#34; Components Metadata Arguments Metadata build arguments used with the Label Schema Convention.\n   Variable Value Description     BUILD_DATE see metadata.variable The Date/Time the image was built.   VERSION see metadata.variable Release identifier for the contents of the image.   VCS_REF see metadata.variable Identifier for the version of the source code from which this image was built.    Build Arguments Build arguments used in the image.\n   Variable Value Description     USER see Makefile.options Sets the user to use when running the image.   DUID see user.variable The user id of the docker user.   DGID see user.variable The group id of the docker user\u0026rsquo;s group.    Volumes No volumes are exposed by the docker container. However, while running the image with limited permissions (baseimage), it is necessary to ensure that the docker user has permission to access mounted volumes. You will need to ensure that the docker user can read/write to the mounted volumes.\nThe working directory of the image is /media/.\nBuild Process To build the docker image, use the included Makefile. It is recommended to use the makefile to ensure all build arguments are provided.\nmake VERSION=\u0026lt;version\u0026gt; build You can view the build/README.md for more on using the Makefile to build the image.\nLabels The docker image follows the Label Schema Convention. Label Schema is a community project to provide a shared namespace for use by multiple tools, specifically org.label-schema. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.label-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/baseimage Label Extension The label namespace org.doc-schema is an extension of org.label-schema. The namespace stores internal variables often used when interacting with the image. These variables will often be application versions or exposed internal variables. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/baseimage Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Creative Commons By 3.0.\nThe project icon is by cre.ativo mustard from the Noun Project.\n","id":161,"section":"posts","summary":"[DEPRECATED] A super small image to act as a baseimage for continuous integration images.     Docker Baseimage Summary A super small image to act as a baseimage for continuous integration images. The project icon is from cre.ativo mustard, HK from the Noun Project\nNOTE: This image is marked EOL, and use is discouraged.\nUsage You can use this image locally with docker run, calling sh to enter the container:","tags":["cardboardci"],"title":"ci-baseimage","uri":"/2017/05/cardboardci-ci-baseimage/","year":"2017"},{"content":"   [DEPRECATED] A super small image with librsvg installed.     Dockerized RSvg Summary A super small image with librsvg installed. The project icon is from cre.ativo mustard, HK from the Noun Project\nNOTE: This image is marked EOL, and use is discouraged.\nUsage You can use this image locally with docker run, calling rsvg-convert to rasterize an image:\ndocker run -v $(pwd):/media/ jrbeverly/rsvg:privileged rsvg-convert test.svg -o test.png Gitlab You can setup a build job using .gitlab-ci.yml:\nbuild: image: jrbeverly/rsvg:baseimage script: - rsvg-convert test.svg -o test.png artifacts: paths: - test.png Components Metadata Arguments Metadata build arguments used with the Label Schema Convention.\n   Variable Value Description     BUILD_DATE see metadata.variable The Date/Time the image was built.   VERSION see metadata.variable Release identifier for the contents of the image.   VCS_REF see metadata.variable Identifier for the version of the source code from which this image was built.    Build Arguments Build arguments used in the image.\n   Variable Value Description     USER see Makefile.options Sets the user to use when running the image.   DUID see user.variable The user id of the docker user.   DGID see user.variable The group id of the docker user\u0026rsquo;s group.    Volumes No volumes are exposed by the docker container. However, while running the image with limited permissions (baseimage), it is necessary to ensure that the docker user has permission to access mounted volumes. You will need to ensure that the docker user can read/write to the mounted volumes. (see User / Group Identifiers)\nThe working directory of the image is /media/.\nBuild Process To build the docker image, use the included Makefile. It is recommended to use the makefile to ensure all build arguments are provided.\nmake VERSION=\u0026lt;version\u0026gt; build You can view the build/README.md for more on using the Makefile to build the image.\nLabels The docker image follows the Label Schema Convention. Label Schema is a community project to provide a shared namespace for use by multiple tools, specifically org.label-schema. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.label-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/rsvg:\u0026lt;TAG\u0026gt; Label Extension The label namespace org.doc-schema is an extension of org.label-schema. The namespace stores internal variables often used when interacting with the image. These variables will often be application versions or exposed internal variables. The values in the namespace can be accessed by the following command:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.\u0026lt;LABEL\u0026gt;\u0026#34; }}\u0026#39; jrbeverly/rsvg:\u0026lt;TAG\u0026gt; User and Group Mapping All processes within the baseimage docker container will be run as the docker user, a non-root user. The docker user is created on build with the user id DUID and a member of a group with group id DGID.\nAny permissions on the host operating system (OS) associated with either the user (DUID) or group (DGID) will be associated with the docker user. The values of DUID and DGID are visible in the Build Arguments, and can be accessed by the commands:\ndocker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.user\u0026#34; }}\u0026#39; jrbeverly/rsvg:baseimage docker inspect -f \u0026#39;{{ index .Config.Labels \u0026#34;org.doc-schema.group\u0026#34; }}\u0026#39; jrbeverly/rsvg:baseimage The notation of the build variables is short form for docker user id (DUID) and docker group id (DGID).\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Creative Commons By 3.0.\nThe project icon is by cre.ativo mustard from the Noun Project.\n","id":162,"section":"posts","summary":"A super small image with \u003ca href=\"http://manpages.ubuntu.com/manpages/zesty/man1/rsvg-convert.1.html\"\u003elibrsvg\u003c/a\u003e installed. The project icon is from \u003ca href=\"docs/icon/icon.json\"\u003ecre.ativo mustard, HK from the Noun Project\u003c/a\u003e \u003cstrong\u003eNOTE: This image is marked EOL, and use is discouraged.\u003c/strong\u003e","tags":["cardboardci"],"title":"ci-rsvg","uri":"/2017/05/cardboardci-ci-rsvg/","year":"2017"},{"content":"   A script for setting up a Windows PC using BoxStarter and Chocolatey.     Automated Windows Setup A script for setting up a Windows PC using BoxStarter and Chocolatey.\nGetting Started There are a few options for launching a BoxStarter script check out the offical documentation for all the various methods.\n\u0026gt; START http://boxstarter.org/package/nr/url?... Environments    Environment Description     basic A very simple box   standard A common environment that can be used on multiple dev machines   fullwin A .NET Developmenet environment with SQL Database tooling   dotnet A .NET Development environment   nodejs A rough outline of a nodejs environment    Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Ryzhkov Anton from the Noun Project.\n","id":163,"section":"posts","summary":"A script for setting up a Windows PC using BoxStarter and Chocolatey.","tags":["devkitspaces"],"title":"boxstarter-scripts","uri":"/2017/05/devkitspaces-boxstarter-scripts/","year":"2017"},{"content":"   A collection of templates and utility scripts used in my homelab.     Homelab Summary A collection of templates and utility scripts used in my homelab. Most of these are just snippets or experiments.\nGetting Started As most of the scripts are self-contained, you can clone the repository:\ngit clone git://homelab/homelab And copy the relevant scripts into /usr/bin/ (or others) as necessary. You can also skip that, and just copy the contents of a file, then paste it into a fresh nano instance.\nDocker-compose Getting started is as simple as using docker-compose. You can do so as such:\ndocker-compose up -d If you wish to upgrade the container stack, you need to run the following commands:\ndocker-compose stop docker-compose rm -v docker-compose pull You can then start the docker environment.\ndocker-compose up -d Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Timothy Dilich from the Noun Project.\n","id":164,"section":"posts","summary":"A collection of templates and utility scripts used in my homelab.     Homelab Summary A collection of templates and utility scripts used in my homelab. Most of these are just snippets or experiments.\nGetting Started As most of the scripts are self-contained, you can clone the repository:\ngit clone git://homelab/homelab And copy the relevant scripts into /usr/bin/ (or others) as necessary. You can also skip that, and just copy the contents of a file, then paste it into a fresh nano instance.","tags":["jrbeverly"],"title":"homelab","uri":"/2017/05/jrbeverly-homelab/","year":"2017"},{"content":"   [DEPRECATED] A base OpenGL Desktop environment, sandboxed on your local computer.     Vagrant OpenGL Desktop Summary Provide a method of reproducible graphical development environments based on Linux. This repository provides a base OpenGL Desktop environment, sandboxed on your local computer. The repository is meant for experimenting with OpenGL related programming in a virtual machine.\nGetting Started You can use this locally with vagrant up, calling as such:\nvagrant --name=mydesktop up However It is recommended to use the script create.sh for the first run to ensure all necessary arguments are provided. The provided arguments creates a settings.yaml, storing the settings for the machine. You can create the machine by calling:\nsh create.sh -n mydesktop -d ubuntu If you want more information about the script create.sh, you can do so by calling:\nsh create.sh -h Parameters The parameters are used in the calling of vagrant up, primarily as vagrant [OPTIONS] up. After provisioning the environment, a settings file (setting.yaml) is created, which stores the provided parameters.\n   Name Type Description     name string Name of the provisioned desktop environment   desktop filename The name of the desktop provisioning script. These scripts are present in packaging/environments.    The vagrant environment is based on the bento/ubuntu images. If the timezone is not set, the provision script will attempt to auto-detect the timezone using tzupdate.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Maxi Koichi from the Noun Project.\n","id":165,"section":"posts","summary":"Provide a method of reproducible graphical development environments based on Linux. This repository provides a base OpenGL Desktop environment, sandboxed on your local computer. The repository is meant for experimenting with OpenGL related programming in a virtual machine.","tags":["blockycraft"],"title":"vagrant-desktop-opengl","uri":"/2017/05/blockycraft-vagrant-desktop-opengl/","year":"2017"},{"content":"   This repository provides a X11 Linux Desktop environment for the development of the XPlatformer project.     Vagrant X11 Desktop Summary Provide a method of reproducible graphical development environments based on Linux. This repository provides a X11 Linux Desktop environment for the development of the XPlatformer project.\nGetting Started You can use this locally with vagrant up, calling as such:\nvagrant --name=mydesktop up However It is recommended to use the script create.sh for the first run to ensure all necessary arguments are provided. The provided arguments creates a settings.yaml, storing the settings for the machine. You can create the machine by calling:\nsh create.sh -n mydesktop -d ubuntu If you want more information about the script create.sh, you can do so by calling:\nsh create.sh -h Parameters The parameters are used in the calling of vagrant up, primarily as vagrant [OPTIONS] up. After provisioning the environment, a settings file (setting.yaml) is created, which stores the provided parameters.\n   Name Type Description     name string Name of the provisioned desktop environment   desktop filename The name of the desktop provisioning script. These scripts are present in packaging/environments.    The vagrant environment is based on the bento/ubuntu images. If the timezone is not set, the provision script will attempt to auto-detect the timezone using tzupdate.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Maxi Koichi from the Noun Project.\n","id":166,"section":"posts","summary":"Provide a method of reproducible graphical development environments based on Linux. This repository provides a X11 Linux Desktop environment for the development of the XPlatformer project.","tags":["xplatformer"],"title":"vagrant-desktop-x11","uri":"/2017/05/xplatformer-vagrant-desktop-x11/","year":"2017"},{"content":"   This repository provides a base Linux Desktop environment, sandboxed on your local computer.     Vagrant Linux Desktop Summary Provide a method of reproducible graphical development environments based on Linux. This repository provides a base Linux Desktop environment, sandboxed on your local computer.\nGetting Started You can use this locally with vagrant up, calling as such:\nvagrant --name=mydesktop --file=desktop.yaml up However It is recommended to use the script create.sh for the first run to ensure all necessary arguments are provided. The provided arguments requires a settings.yaml, storing the settings for the machine. You can see an example of one in tools/simple.yaml. You can create the machine by calling:\nsh create.sh -n mydesktop -d ubuntu If you want more information about the script create.sh, you can do so by calling:\nsh create.sh -h Parameters The parameters are used in the calling of vagrant up, primarily as vagrant [OPTIONS] up. After provisioning the environment, a settings file (setting.yaml) is created, which stores the provided parameters.\n   Name Type Description     name string Name of the provisioned desktop environment   desktop filename The name of the desktop provisioning script. These scripts are present in packaging/environments.    The vagrant environment is based on the bento/ubuntu images. If the timezone is not set, the provision script will attempt to auto-detect the timezone using tzupdate.\nSettings The following are arguments to the settings.yaml file:\n   Name Type Description     name string Name of the provisioned desktop environment   box vagrant-box The name of the underlying vagrant box   path dirname The path to the .vagrant directory   desktop string The name of the desktop provisioning script   logs dirname The directory to dump logs files   synced_folders (host: directory, guest: directory)[] A collection of syneced folders.    An example yaml is included below:\nname: lab box: ubuntu/trusty64 path: \u0026#34;.\u0026#34; desktop: ubuntu-minimal logs: \u0026#34;log_dir\u0026#34; synced_folders: - host: \u0026#34;../\u0026#34; guest: \u0026#34;/media/vagrant\u0026#34; Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Maxi Koichi from the Noun Project.\n","id":167,"section":"posts","summary":"Provide a method of reproducible graphical development environments based on Linux. This repository provides a base Linux Desktop environment, sandboxed on your local computer.","tags":["devkitspaces"],"title":"vagrant-desktop","uri":"/2017/05/devkitspaces-vagrant-desktop/","year":"2017"},{"content":"   Blockycraft is an interactive graphics demo to create a Minecraft inspired demo.     Blockycraft Manual Summary Blockycraft is an interactive graphics demo to create a Minecraft inspired demo. Blockycraft is focused on a series of graphics techniques create the graphical aethestics of a Minecraft world. This involves the usage of Perlin Noise for a dynamic world, lighting and shadows based on the positions of blocks in the world, and the aesthetic of the block world. The Blockycraft Manual explains the Blockycraft project in terms of interactive, compilation and technical components.\nBlockycraft Blockycraft is an interactive demo that uses standard first person controls to navigate through a block world. Blockycraft is a interactive graphics demo to create a Minecraft inspired demo which revolves around breaking and placing blocks. The game world is composed of rough cubes arranged in a fixed grid pattern and representing different materials, such as dirt, stone, and snow. The techniques used in the demo can be toggled using keyboard commands. The Blockycraft project involves an OpenGL C++ interaction demo that can be viewed at Blockycraft.\nDevelopment If you are developing on Windows, it is recommended to install the IDE Texmaker, the universal LaTeX editor. This will ensure that the necessary TexLive packages are installed (or can be installed). The IDE Texmaker is the default IDE used in development of the project.\nBuilding You can build the image using latexmk. To build with pdf2htmlEX, you can either install pdf2htmlEX in your environment, or make use of the docker image. To start the docker image, run the following:\nsh start.sh To build with latexmk, you can do the following:\nlatexmk -pdf manual.tex It is recommend to use the build scripts available in build/. These scripts are used in the build pipeline, ensuring that all arguments and attributes are set for compilation of the TeX project. The output pdf manual.pdf is available at the project root, while other build files are available in the output/ directory.\nsh build/compile.sh GitLab CI This project\u0026rsquo;s manual is built by GitLab CI, following the steps defined in .gitlab-ci.yml. The build scripts are available in build/, which are used to compile the TeX file.\nAcknowledgements The project icon is retrieved from kenney.nl. The original source material has been altered for the purposes of the project. The icon is used under the terms of the CC0 1.0 Universal.\nThe project icon uses assets by Kenney from kenney.nl/.\n","id":168,"section":"posts","summary":"Blockycraft is an interactive graphics demo to create a Minecraft inspired demo. Blockycraft is focused on a series of graphics techniques create the graphical aethestics of a Minecraft world. This involves the usage of Perlin Noise for a dynamic world, lighting and shadows based on the positions of blocks in the world, and the aesthetic of the block world. The Blockycraft Manual explains the Blockycraft project in terms of interactive, compilation and technical components.","tags":["blockycraft"],"title":"manual-classic","uri":"/2017/05/blockycraft-manual-classic/","year":"2017"},{"content":"   XSamples provides a collection of XWindows starter code that works with the XGameLib library.     XSamples Summary XSamples provides a collection of XWindows starter code that works with the XGameLib library. The XGameLib library powers the XPlatformer project. All the examples make use of the XLib API (XOrg) and focus on code that was developed in the XPlatformer project (or planned to be used). The point of the examples is to demonstrate how to use the XGameLib library.\nGetting Started To make (\u0026ldquo;compile and link\u0026rdquo;) an example, use the included makefile with the name of cpp file passed as a variable. For example, to make null.cpp:\nmake NAME=\u0026#34;null\u0026#34; Then, to run:\n./null Or you can even do it one step:\nmake run NAME=\u0026#34;null\u0026#34; For a full list of the examples, see docs/examples.md.\nAcknowledgements The project icon is retrieved from kenney.nl. The original source material has been altered for the purposes of the project. The icon is used under the terms of the CC0 1.0 Universal.\nThe project icon uses assets by Kenney from kenney.nl/.\n","id":169,"section":"posts","summary":"XSamples provides a collection of XWindows starter code that works with the XGameLib library.     XSamples Summary XSamples provides a collection of XWindows starter code that works with the XGameLib library. The XGameLib library powers the XPlatformer project. All the examples make use of the XLib API (XOrg) and focus on code that was developed in the XPlatformer project (or planned to be used). The point of the examples is to demonstrate how to use the XGameLib library.","tags":["xplatformer"],"title":"xsamples","uri":"/2017/05/xplatformer-xsamples/","year":"2017"},{"content":"   A framework for a 2D Platformer built using the X Window System.     XGameLib Summary XGameLib is a simple video game library used in the development of a classic side-scrolling arcade game, using the XLib API. The point of the game is to control a character through a terrain to meet an objective. The project makes use of the XLib API (XOrg) and focus on code that was developed to accomplish tasks for the assignment task.\nComponents    Component Filename Description     Spritesheet Spritesheet.h A uniform sheet of sprites that can be drawn individually.   Logger Logger.h Contains standard logging functionality and stored notifications.   KeyboardState KeyboardState.h Represents the state of keystrokes recorded by a keyboard input device.   MouseState MouseState.h Represents the state of a mouse input device, including mouse cursor position and buttons pressed.   Displayable Displayable.h Displayable is the base class for an object that can be updated/drawn to the screen.     Compile Instructions To make (\u0026ldquo;compile and link\u0026rdquo;) an example, use the included makefile with the name of cpp file passed as a variable.\nmake build Or you can even do it without specifying:\nmake Acknowledgements The project icon is retrieved from kenney.nl. The original source material has been altered for the purposes of the project. The icon is used under the terms of the CC0 1.0 Universal.\nThe project icon uses assets by Kenney from kenney.nl/.\n","id":170,"section":"posts","summary":"XGameLib is a simple video game library used in the development of a classic side-scrolling arcade game, using the XLib API. The point of the game is to control a character through a terrain to meet an objective. The project makes use of the XLib API (XOrg) and focus on code that was developed to accomplish tasks for the assignment task.","tags":["xplatformer"],"title":"xgamelib","uri":"/2017/05/xplatformer-xgamelib/","year":"2017"},{"content":"   XPlatformer is a simple video game reminiscent of the classic side-scrolling arcade game.     XPlatformer Summary XPlatformer is a simple video game reminiscent of the classic side-scrolling arcade game, using the XLib API. The point of the game is to control a character through a terrain to meet an objective. The project makes use of the XLib API (XOrg) and focus on code that was developed to accomplish tasks for the assignment task.\nGetting Started To make (\u0026ldquo;compile and link\u0026rdquo;) an example, you can use bazel to build and run the project:\nbazel build //:xplatformer bazel run //:xplatformer Usage You can read more about how to interact with XPlatformer in the usage document.\nAcknowledgements The project icon is retrieved from kenney.nl. The original source material has been altered for the purposes of the project. The icon is used under the terms of the CC0 1.0 Universal.\nThe project icon uses assets by Kenney from kenney.nl/.\nResources All art assets were acquired from http://opengameart.org/ in particular from http://opengameart.org/users/kenney. Majority of art assets come from a particular package known as \u0026ldquo;Platformer Art Deluxe\u0026rdquo; available at http://opengameart.org/content/platformer-art-deluxe. If you would like to know more about these art assets, look into http://open.commonly.cc/ or the \u0026ldquo;Open Bundle\u0026rdquo; [See http://www.kenney.nl/]. The art assets are available with the Creative Commons License (CC0)\n","id":171,"section":"posts","summary":"XPlatformer is a simple video game reminiscent of the classic side-scrolling arcade game, using the XLib API. The point of the game is to control a character through a terrain to meet an objective. The project makes use of the XLib API (XOrg) and focus on code that was developed to accomplish tasks for the assignment task. \u003cimg src=\"./docs/screenshots/xplatformer.png\" alt=\"xplatformer\" title=\"XPlatformer\"\u003e","tags":["xplatformer"],"title":"xplatformer","uri":"/2017/05/xplatformer-xplatformer/","year":"2017"},{"content":"   Friending is an imaginary online dating, friendship, and social networking mobile application that features user-created questionnaires.     Friending Summary Friending is an online dating, friendship, and social networking mobile application that features user-created questionnaires. Friending has two primary features: joining groups to find people similar to you or signing up for events happening in your local area. Friending is a prototype built with the Proto.io application prototyping tool.\nThe Friending prototype was built as part of a requirements specification project. The project focused on the development of a user manual around a fictional matchmaking application called Friending. The application centered around user-created questionnaires that could be used to find relationship matches. The user manual was designed with the goal of deceiving the reader into believing that the application existed. A project vision document and set of user requirements guided the development of scenarios and use cases for the application. The fully-interactive high-fidelity prototype created for the requirements specification project is provided in this repository, available as a standalone HTML page.\nManual The Friending user manual provides info and tips to help you understand the mobile application. The requirements specification project involved the creation of a user manual for the fictional mobile application Friending. The Friending prototype is the actualization of a user vision and set of requirements to construct a matchmaking application. The vision and requirements were used to develop the expected behaviour of the prototype, although not all requirements were actualized into the interactive prototype. The prototype merely needed to present a faithful representation of the original vision.\nProto.io Proto.io is a web service to create fully-interactive high-fidelity prototypes that look and work exactly like your app should. No coding required. Using Proto.io you can quickly design and prototype design ideas. As the user manual was built around the fictional mobile application Friending, mockups were necessary to explain the application to the reader. Proto.io was selected for developing the mockups as they produced realistic mockups for a mobile application. Interaction was added to produce a fully-interactive prototype, which was useful in validating scenarios and use cases from the user manual. The visual design of the prototype was guided by the project vision document.\nGenerated The code provided in this repository is generated by Proto.io using the Export to HTML feature. Proto.io allows you to export your prototype so that it can be runnable as a standalone HTML page. When exported a zip package containing all necessary files to run and view the prototype is created. The code provided in this repository contains the project data files, and all the assets used in the project. As each exported prototype uses randomly generated file names for assets, this can mean that the versioning of assets is not always consistent, nor legible. The interaction code for a prototype is controlled by minimized javascript (scripts/ \u0026amp; libraries/), so it too is not legible.\nThe exported index.html file does not contain the rendered HTML pages of the prototype. The prototype pages are rendered by the Proto.io rendering engine (Javascript) which means that you should not modify any exported HTML code as this may break the presentation and functionality of the prototype. If you open the index.html file, then you will also see the respective prototypes device skin. The exported code has been altered slightly to switch the frame.html and index.html to make use of the device skin wrapper of the prototype.\nAcknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Stefan Hartmann from the Noun Project.\n","id":172,"section":"posts","summary":"Friending is an online dating, friendship, and social networking mobile application that features user-created questionnaires. Friending has two primary features: joining groups to find people similar to you or signing up for events happening in your local area. Friending is a prototype built with the Proto.io application prototyping tool. The Friending prototype was built as part of a requirements specification project. The project focused on the development of a user manual around a fictional matchmaking application called Friending. The application centered around user-created questionnaires that could be used to find relationship matches. The user manual was designed with the goal of \u003cem\u003edeceiving\u003c/em\u003e the reader into believing that the application existed. A project vision document and set of user requirements guided the development of scenarios and use cases for the application. The fully-interactive high-fidelity prototype created for the requirements specification project is provided in this repository, available as a standalone HTML page.","tags":["thefriending"],"title":"friending","uri":"/2017/05/thefriending-friending/","year":"2017"},{"content":"   The Friending user manual provided as a web resource generated from the user manual.     Friending User Guide Summary The Friending user manual provided as a web resource generated from the user manual. The user guide provides info and tips to help you understand the mobile application as a web resource, instead of the standard PDF representation of the user manual. The method used to convert the user manual can be viewed in the build/ directory.\nFriending Friending is an online dating, friendship, and social networking mobile application that features user-created questionnaires. Friending has two primary features: joining groups to find people similar to you or signing up for events happening in your local area. Friending is a prototype built with the Proto.io application prototyping tool.\nThe Friending prototype was built as part of a requirements specification project. The project focused on the development of a user manual around a fictional matchmaking application called Friending. The application centered around user-created questionnaires that could be used to find relationship matches. The user manual was designed with the goal of deceiving the reader into believing that the application existed. A project vision document and set of user requirements guided the development of scenarios and use cases for the application. The user manual created for the requirements specification project is provided in this repository, available as a standalone HTML page.\nBuild You can build the standalone HTML page using pdf2htmlEX. To build with pdf2htmlEX, you can either install pdf2htmlEX in your environment, or make use of the docker image. To start the docker image, run the following:\nsh start.sh You can then use pdf2htmlEX as such:\npdf2htmlEX --zoom 1.5 --embed cfijo --dest-dir public/ index.pdf It is recommend to use the build scripts available in build/. These scripts are used in the build pipeline, ensuring that all arguments and attributes are set for compilation of the HTML page. The scripts handle the process of downloading the user manual from source, allowing it to be converted from PDF to HTML without losing text or format.\nsh compile.sh Build Parameters    Variable Default Description      ARTIFACT_URL see .gitlab-ci.yml The URL hosting the friending user manual.     Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Stefan Hartmann from the Noun Project.\n","id":173,"section":"posts","summary":"The Friending user manual provided as a web resource generated from the user manual. The user guide provides info and tips to help you understand the mobile application as a web resource, instead of the standard PDF representation of the user manual. The method used to convert the user manual can be viewed in the \u003ccode\u003ebuild/\u003c/code\u003e directory.","tags":["thefriending"],"title":"user-guide","uri":"/2017/05/thefriending-user-guide/","year":"2017"},{"content":"   The Friending user manual provides info and tips to help you understand the mobile application.     Friending Manual Summary The Friending user manual provides info and tips to help you understand the mobile application. The project is the actualization of a user vision and set of requirements to construct a matchmaking application. The User Manual encodes these requirements as scenarios and use cases, while the Friending prototype presents a high-fidelity vision of the matchmaking application.\nFriending Friending is an online dating, friendship, and social networking mobile application that features user-created questionnaires. Friending has two primary features: joining groups to find people similar to you or signing up for events happening in your local area. Friending is a prototype built with the Proto.io application prototyping tool.\nThe Friending prototype was built as part of a requirements specification project. The project focused on the development of a user manual around a fictional matchmaking application called Friending. The application centered around user-created questionnaires that could be used to find relationship matches. The user manual was designed with the goal of deceiving the reader into believing that the application existed. A project vision document and set of user requirements guided the development of scenarios and use cases for the application. The user manual created for the requirements specification project is provided in this repository, available as a PDF.\nDevelopment If you are developing on Windows, it is recommended to install the IDE Texmaker, the universal LaTeX editor. This will ensure that the necessary TexLive packages are installed (or can be installed). The IDE Texmaker is the default IDE used in development of the project.\nBuilding You can build the image using latexmk. To build with pdf2htmlEX, you can either install pdf2htmlEX in your environment, or make use of the docker image. To start the docker image, run the following:\nsh start.sh To build with latexmk, you can do the following:\nlatexmk -pdf manual.tex It is recommend to use the build scripts available in build/. These scripts are used in the build pipeline, ensuring that all arguments and attributes are set for compilation of the TeX project. The output pdf manual.pdf is available at the project root, while other build files are available in the output/ directory.\nsh build/compile.sh Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by Stefan Hartmann from the Noun Project.\n","id":174,"section":"posts","summary":"The Friending user manual provides info and tips to help you understand the mobile application. The project is the actualization of a user vision and set of requirements to construct a matchmaking application. The User Manual encodes these requirements as scenarios and use cases, while the Friending prototype presents a high-fidelity vision of the matchmaking application.","tags":["thefriending"],"title":"manual","uri":"/2017/05/thefriending-manual/","year":"2017"},{"content":"   Jotto is a logic-oriented word game played with two players.     Jotto Summary Jotto is a logic-oriented word game played with two players. Each player picks a secret word of five letters (that is in the dictionary), and the object of the game is to correctly guess the other player\u0026rsquo;s word first. Players take turns guessing and giving the number of Jots, or the number of letters that are in both the guessed word and the secret word.\nThe Jotto application is built with a single player, playing against a computer. The objective of the game is to correctly guess the secret word before the maximum number of guesses. The user interface provides feedback about the success of each guess, and the progress being made by the player. Each guess must be validated that it is present in the dictionary, of the proper length and contains repeated characters. After each guess the player will be provided feedback about the guess, such as the number of exact character matches and the number of partial character matches.\nDevelopment If you are developing on Windows, it is recommended to install the IDE IntelliJ IDEA. This will work with the existing jotto.iml present in the jotto/ directory. The output of the build process is available in the CI artifacts browser. The artifacts have an expiration period to ensure that old build artifacts are properly cleaned up.\nBuilding You can build the image using mvn or scripts in the build/ directory. To build with maven, you can either install maven in your environment, or make use of the docker image. To start the docker image, run the following:\nsh build/start.sh To build with mvn, you can do the following:\nmvn compile It is recommend to use the build scripts available in build/. These scripts are used in the build pipeline, ensuring that all arguments and attributes are set for compilation of the project. The output application jotto.jar is available at the project root, while other build files are available in the target/ directory.\nsh build/compile.sh Acknowledgements The project icon is retrieved from the Noun Project. The original source material has been altered for the purposes of the project. The icon is used under the terms of the Public Domain.\nThe project icon is by dnlhtz from the Noun Project.\n","id":175,"section":"posts","summary":"Jotto is a logic-oriented word game played with two players. Each player picks a secret word of five letters (that is in the dictionary), and the object of the game is to correctly guess the other player\u0026rsquo;s word first. Players take turns guessing and giving the number of Jots, or the number of letters that are in both the guessed word and the secret word. The Jotto application is built with a single player, playing against a computer. The objective of the game is to correctly guess the secret word before the maximum number of guesses. The user interface provides feedback about the success of each guess, and the progress being made by the player. Each guess must be validated that it is present in the dictionary, of the proper length and contains repeated characters. After each guess the player will be provided feedback about the guess, such as the number of exact character matches and the number of partial character matches.","tags":["jrbeverly"],"title":"jotto","uri":"/2017/05/jrbeverly-jotto/","year":"2017"}],"tags":[{"title":"blockycraft","uri":"/tags/blockycraft/"},{"title":"cardboardci","uri":"/tags/cardboardci/"},{"title":"devkitspaces","uri":"/tags/devkitspaces/"},{"title":"friending","uri":"/tags/friending/"},{"title":"index","uri":"/tags/index/"},{"title":"infraprints","uri":"/tags/infraprints/"},{"title":"jrbeverly","uri":"/tags/jrbeverly/"},{"title":"thefriending","uri":"/tags/thefriending/"},{"title":"wisevault","uri":"/tags/wisevault/"},{"title":"xplatformer","uri":"/tags/xplatformer/"}]}